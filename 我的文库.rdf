<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="#item_1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
    </bib:Article>
    <bib:Article rdf:about="http://arxiv.org/abs/1912.13200">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1912.13200 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Hanting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yunhe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Chunjing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Boxin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Chao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Chang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_2"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>AdderNet: Do We Really Need Multiplications in Deep Learning?</dc:title>
        <dcterms:abstract>Compared with cheap addition operation, multiplication operation is of much higher computation complexity. The widely-used convolutions in deep neural networks are exactly cross-correlation to measure the similarity between input feature and convolution ﬁlters, which involves massive multiplications between ﬂoat values. In this paper, we present adder networks (AdderNets) to trade these massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs. In AdderNets, we take the 1-norm distance between ﬁlters and input feature as the output response. The inﬂuence of this new similarity measure on the optimization of neural network have been thoroughly analyzed. To achieve a better performance, we develop a special back-propagation approach for AdderNets by investigating the full-precision gradient. We then propose an adaptive learning rate strategy to enhance the training procedure of AdderNets according to the magnitude of each neuron’s gradient. As a result, the proposed AdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50 on the ImageNet dataset without any multiplication in convolution layer.</dcterms:abstract>
        <dc:date>2020-01-08</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AdderNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1912.13200</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-19 05:01:25</dcterms:dateSubmitted>
        <dc:description>arXiv: 1912.13200</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_2">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2/2020 - AdderNet Do We Really Need Multiplications in Dee.pdf"/>
        <dc:title>2020 - AdderNet Do We Really Need Multiplications in Dee.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_4">
        <z:itemType>newspaperArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Newspaper></bib:Newspaper>
        </dcterms:isPartOf>
    </bib:Article>
    <z:Attachment rdf:about="#item_11">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/11/李萌专利.PDF"/>
        <dc:title>李萌专利.PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_22">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/22/李萌专利.PDF"/>
        <dc:title>李萌专利.PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1904.08189">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1904.08189 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duan</foaf:surname>
                        <foaf:givenName>Kaiwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Song</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Lingxi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Honggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Qingming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_33"/>
        <link:link rdf:resource="#item_96"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CenterNet: Keypoint Triplets for Object Detection</dc:title>
        <dcterms:abstract>In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efﬁcient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/ Duankaiwen/CenterNet.</dcterms:abstract>
        <dc:date>2019-04-18</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CenterNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.08189</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-20 06:06:54</dcterms:dateSubmitted>
        <dc:description>arXiv: 1904.08189</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_33">
        <rdf:value>Comment: 10 pages (including 2 pages of References), 7 figures, 5 tables</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_96">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Duan_2019_CenterNet.pdf"/>
        <dc:title>Duan_2019_CenterNet.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://zhuanlan.zhihu.com/p/85194783">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>知乎专栏</dc:title></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_43"/>
        <dc:title>Centernet相关---尤其有关heatmap相关解释</dc:title>
        <dcterms:abstract>本人现在工作在阿里云数据智能事业部工农业研发，对工作应聘感兴趣的欢迎私信。 前言最近anchor free的工作层出不穷，趁着工作闲暇时间，精读了论文以及相关代码。其中以CenterNet最为简洁，整体思路最为清晰，代…</dcterms:abstract>
        <z:language>zh</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://zhuanlan.zhihu.com/p/85194783</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-21 03:22:01</dcterms:dateSubmitted>
        <dc:description>Library Catalog: zhuanlan.zhihu.com</dc:description>
    </bib:Document>
    <z:Attachment rdf:about="#item_43">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/43/85194783.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://zhuanlan.zhihu.com/p/85194783</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-21 03:22:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.google.com/search?q=%E9%98%BF%E6%96%AF%E8%92%82%E8%8A%AC&amp;oq=%E9%98%BF%E6%96%AF%E8%92%82%E8%8A%AC&amp;aqs=chrome..69i57j69i60.1112j0j7&amp;sourceid=chrome&amp;ie=UTF-8">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>阿斯蒂芬 - Google 搜索</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.google.com/search?q=%E9%98%BF%E6%96%AF%E8%92%82%E8%8A%AC&amp;oq=%E9%98%BF%E6%96%AF%E8%92%82%E8%8A%AC&amp;aqs=chrome..69i57j69i60.1112j0j7&amp;sourceid=chrome&amp;ie=UTF-8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-21 03:22:14</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_60">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/60/李萌专利.PDF"/>
        <dc:title>李萌专利.PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_62">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/62/1904.07850.pdf"/>
        <dc:title>1904.07850.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_68">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/68/csp.pdf"/>
        <dc:title>csp.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_70">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/70/One-Snapshot Face Anti-spoofing Using a Light.pdf"/>
        <dc:title>One-Snapshot Face Anti-spoofing Using a Light.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s00418-019-01831-2">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0948-6143,%201432-119X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taatjes</foaf:surname>
                        <foaf:givenName>Douglas J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_99"/>
        <dc:title>In focus in HCB</dc:title>
        <dc:date>12/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s00418-019-01831-2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-24 05:31:02</dcterms:dateSubmitted>
        <bib:pages>391-395</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0948-6143,%201432-119X">
        <prism:volume>152</prism:volume>
        <dc:title>Histochemistry and Cell Biology</dc:title>
        <dc:identifier>DOI 10.1007/s00418-019-01831-2</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Histochem Cell Biol</dcterms:alternative>
        <dc:identifier>ISSN 0948-6143, 1432-119X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_99">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Taatjes_2019_In focus in HCB.pdf"/>
        <dc:title>Taatjes_2019_In focus in HCB.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_PRX9ZSEU/1&quot;&gt;Most popular articles published in HCB in 2017&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_PRX9ZSEU/1&quot;&gt;Alterations in placental ROS-associated components with obesity&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_PRX9ZSEU/3&quot;&gt;FoxN1 impacts on thymic cortex-medulla differentiation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_PRX9ZSEU/3&quot;&gt;Association between adipocytes and mast cells in superficial fascia&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_PRX9ZSEU/3&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="#item_75">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>G D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Castan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Loiseau</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fossard</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nelayah</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alloyeau</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bichara</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Amara</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_94"/>
        <dc:title>A deep learning approach for determining the chiral indices of carbon nanotubes from high-resolution transmission electron microscopy images</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>20</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_94">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Forster_A deep learning approach for determining the chiral indices of carbon nanotubes.pdf"/>
        <dc:title>Forster_A deep learning approach for determining the chiral indices of carbon nanotubes.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.nature.com/articles/s41598-017-13565-z">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2045-2322"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Modarres</foaf:surname>
                        <foaf:givenName>Mohammad Hadi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aversa</foaf:surname>
                        <foaf:givenName>Rossella</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cozzini</foaf:surname>
                        <foaf:givenName>Stefano</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ciancio</foaf:surname>
                        <foaf:givenName>Regina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leto</foaf:surname>
                        <foaf:givenName>Angelo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brandino</foaf:surname>
                        <foaf:givenName>Giuseppe Piero</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_100"/>
        <dc:title>Neural Network for Nanoscience Scanning Electron Microscope Image Recognition</dc:title>
        <dc:date>12/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.nature.com/articles/s41598-017-13565-z</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-24 05:34:59</dcterms:dateSubmitted>
        <bib:pages>13282</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2045-2322">
        <prism:volume>7</prism:volume>
        <dc:title>Scientific Reports</dc:title>
        <dc:identifier>DOI 10.1038/s41598-017-13565-z</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Sci Rep</dcterms:alternative>
        <dc:identifier>ISSN 2045-2322</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_100">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Modarres_2017_Neural Network for Nanoscience Scanning Electron Microscope Image Recognition.pdf"/>
        <dc:title>Modarres_2017_Neural Network for Nanoscience Scanning Electron Microscope Image Recognition.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/2&quot;&gt;Results and Discussion&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/3&quot;&gt;Defining the training set. &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/3&quot;&gt;Achievements on Transfer Learning. &lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/3&quot;&gt;Inception-v3 feature extraction. &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/4&quot;&gt;Comparison with other models. &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/5&quot;&gt;Achievements on image classification. &lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/5&quot;&gt;Image classification of subcategories. &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/7&quot;&gt;Images within multiple categories. &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/7&quot;&gt;Statistics on image classification. &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/9&quot;&gt;Massive image processing. &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/9&quot;&gt;A nanoscience task: mutually coherent alignment of nanowires. &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/10&quot;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/11&quot;&gt;Methods and Tools&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/11&quot;&gt;Data availability. &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/12&quot;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/3&quot;&gt;Figure 1 Categories chosen for SEM images.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/6&quot;&gt;Figure 2 A summary of the image classification outcomes for the most scattered categories.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/7&quot;&gt;Figure 3 An example of images within two categories.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/7&quot;&gt;Figure 4 A histogram of the score of the top ranked category.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/8&quot;&gt;Figure 5 The normalized distribution of scores for each of the groups.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/8&quot;&gt;Figure 6 Probability that the top ranked score is assigned to the correct (green) or incorrect (red) category.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/9&quot;&gt;Figure 7 Speedup of our Spark implementation as a function of the number of cores on the cluster node, using a sample of 1,000 (blue) and 5,000 (orange) images.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/10&quot;&gt;Figure 8 Comparison of alignment score from the image classification obtained by a trained neural network (blue bars) and from the OrientationJ plugin (orange bars) on representative nanowires test images.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/4&quot;&gt;Table 1 SEM training set.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/4&quot;&gt;Table 2 Confusion matrix with the prediction (expressed in percentage) for each category obtained when testing the feature extraction task.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/5&quot;&gt;Table 3 Results after 5,000 iterations when retraining the last layers using different models.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C2PHHRJM/5&quot;&gt;Table 4 The breakdown of the second test set.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1603.07285">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1603.07285 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dumoulin</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Visin</foaf:surname>
                        <foaf:givenName>Francesco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_95"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A guide to convolution arithmetic for deep learning</dc:title>
        <dcterms:abstract>We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.</dcterms:abstract>
        <dc:date>2018-01-11</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1603.07285</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-04-30 14:22:23</dcterms:dateSubmitted>
        <dc:description>arXiv: 1603.07285</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_95">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Dumoulin_2018_A guide to convolution arithmetic for deep learning.pdf"/>
        <dc:title>Dumoulin_2018_A guide to convolution arithmetic for deep learning.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/5&quot;&gt;1 Introduction&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/6&quot;&gt;1.1 Discrete convolutions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/10&quot;&gt;1.2 Pooling&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/12&quot;&gt;2 Convolution arithmetic&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/12&quot;&gt;2.1 No zero padding, unit strides&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/13&quot;&gt;2.2 Zero padding, unit strides&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/13&quot;&gt;2.2.1 Half (same) padding&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/13&quot;&gt;2.2.2 Full padding&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/15&quot;&gt;2.3 No zero padding, non-unit strides&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/15&quot;&gt;2.4 Zero padding, non-unit strides&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/18&quot;&gt;3 Pooling arithmetic&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/19&quot;&gt;4 Transposed convolution arithmetic&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/20&quot;&gt;4.1 Convolution as a matrix operation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/20&quot;&gt;4.2 Transposed convolution&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/21&quot;&gt;4.3 No zero padding, unit strides, transposed&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/22&quot;&gt;4.4 Zero padding, unit strides, transposed&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/22&quot;&gt;4.4.1 Half (same) padding, transposed&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/22&quot;&gt;4.4.2 Full padding, transposed&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/24&quot;&gt;4.5 No zero padding, non-unit strides, transposed&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/24&quot;&gt;4.6 Zero padding, non-unit strides, transposed&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/28&quot;&gt;5 Miscellaneous convolutions&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8QCHH2X2/28&quot;&gt;5.1 Dilated convolutions&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Document rdf:about="https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
            <z:Website>
               <dc:title>Unix &amp; Linux Stack Exchange</dc:title>
            </z:Website>
        </dcterms:isPartOf>
        <dcterms:isReferencedBy rdf:resource="#item_82"/>
        <link:link rdf:resource="#item_83"/>
        <dc:title>bash - How to correctly add a path to PATH?</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-02 06:18:51</dcterms:dateSubmitted>
        <dc:description>Library Catalog: unix.stackexchange.com</dc:description>
    </bib:Document>
    <bib:Memo rdf:about="#item_82">
        <rdf:value>&lt;blockquote&gt;You don't need export if the variable is already in the environment:&lt;/blockquote&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_83">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/83/how-to-correctly-add-a-path-to-path.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-02 06:19:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2004.08955">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2004.08955 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Chongruo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zhongyue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zhi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Haibin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Yue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mueller</foaf:surname>
                        <foaf:givenName>Jonas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manmatha</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Mu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smola</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_102"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>ResNeSt: Split-Attention Networks</dc:title>
        <dcterms:abstract>While image classiﬁcation models have recently continued to advance, most downstream applications such as object detection and semantic segmentation still employ ResNet variants as the backbone network due to their simple and modular structure. We present a modular Split-Attention block that enables attention across feature-map groups. By stacking these Split-Attention blocks ResNet-style, we obtain a new ResNet variant which we call ResNeSt. Our network preserves the overall ResNet structure to be used in downstream tasks straightforwardly without introducing additional computational costs.</dcterms:abstract>
        <dc:date>2020-04-19</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>ResNeSt</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.08955</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-04 10:32:55</dcterms:dateSubmitted>
        <dc:description>arXiv: 2004.08955</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_102">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Zhang_2020_ResNeSt.pdf"/>
        <dc:title>Zhang_2020_ResNeSt.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_BL9BYXUF/1&quot;&gt;ResNeSt: Split-Attention Networks&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72814-803-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72814-803-8</dc:identifier>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2019.00975</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seoul, Korea (South)</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Ze</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Shaohui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Han</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Liwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Stephen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_101"/>
        <dc:title>RepPoints: Point Set Representation for Object Detection</dc:title>
        <dcterms:abstract>Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the ﬁnal predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present RepPoints (representative points), a new ﬁner representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically signiﬁcant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 AP50 on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at https://github.com/microsoft/RepPoints.</dcterms:abstract>
        <dc:date>10/2019</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>RepPoints</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9009032/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-05 14:20:03</dcterms:dateSubmitted>
        <bib:pages>9656-9665</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_101">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Yang_2019_RepPoints.pdf"/>
        <dc:title>Yang_2019_RepPoints.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_88">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/88/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.pdf"/>
        <dc:title>Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-05 15:10:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1911.09070">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1911.09070 [cs, eess]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Mingxing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pang</foaf:surname>
                        <foaf:givenName>Ruoming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_91"/>
        <link:link rdf:resource="#item_98"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Image and Video Processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>EfficientDet: Scalable and Efficient Object Detection</dc:title>
        <dcterms:abstract>Model efﬁciency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efﬁciency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfﬁcientNet backbones, we have developed a new family of object detectors, called EfﬁcientDet, which consistently achieve much better efﬁciency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfﬁcientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs1, being 4x – 9x smaller and using 13x – 42x fewer FLOPs than previous detectors. Code is available at https://github.com/ google/automl/tree/master/efficientdet.</dcterms:abstract>
        <dc:date>2020-04-03</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>EfficientDet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.09070</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-06 02:19:31</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.09070</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_91">
       <rdf:value>&lt;p&gt;Comment: CVPR 2020&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_98">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Tan_2020_EfficientDet.pdf"/>
        <dc:title>Tan_2020_EfficientDet.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_92">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1904.08189 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duan</foaf:surname>
                        <foaf:givenName>Kaiwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Song</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Lingxi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Honggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Qingming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_93"/>
        <link:link rdf:resource="#item_97"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CenterNet: Keypoint Triplets for Object Detection</dc:title>
        <dcterms:abstract>In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efﬁcient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/ Duankaiwen/CenterNet.</dcterms:abstract>
        <dc:date>2019-04-18</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CenterNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.08189</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-06 09:36:30</dcterms:dateSubmitted>
        <dc:description>arXiv: 1904.08189</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_93">
        <rdf:value>Comment: 10 pages (including 2 pages of References), 7 figures, 5 tables</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_97">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Duan_2019_CenterNet2.pdf"/>
        <dc:title>Duan_2019_CenterNet.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2004.01177">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2004.01177 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koltun</foaf:surname>
                        <foaf:givenName>Vladlen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krähenbühl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_105"/>
        <link:link rdf:resource="#item_103"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Tracking Objects as Points</dc:title>
        <dcterms:abstract>Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. In this paper, we present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That’s it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves 67.3% MOTA on the MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves 28.3% AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS.</dcterms:abstract>
        <dc:date>2020-04-02</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.01177</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-06 12:06:35</dcterms:dateSubmitted>
        <dc:description>arXiv: 2004.01177</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_105">
        <rdf:value>Comment: Code available at https://github.com/xingyizhou/CenterTrack</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_103">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/103/Zhou 等。 - 2020 - Tracking Objects as Points.pdf"/>
        <dc:title>Zhou 等。 - 2020 - Tracking Objects as Points.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2004.01177v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-06 12:06:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_106">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/106/630.pdf"/>
        <dc:title>630.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://cs231n.stanford.edu/reports/2017/pdfs/630.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-06 12:18:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-0457-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.572</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Venkateswara</foaf:surname>
                        <foaf:givenName>Hemanth</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eusebio</foaf:surname>
                        <foaf:givenName>Jose</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chakraborty</foaf:surname>
                        <foaf:givenName>Shayok</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Panchanathan</foaf:surname>
                        <foaf:givenName>Sethuraman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_65"/>
        <dc:title>Deep Hashing Network for Unsupervised Domain Adaptation</dc:title>
        <dcterms:abstract>In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efﬁciency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the ﬁrst research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efﬁcient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8100055/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-07 00:45:34</dcterms:dateSubmitted>
        <bib:pages>5385-5394</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_65">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/65/Venkateswara 等。 - 2017 - Deep Hashing Network for Unsupervised Domain Adapt.pdf"/>
        <dc:title>Venkateswara 等。 - 2017 - Deep Hashing Network for Unsupervised Domain Adapt.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_109">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>Object Tracking</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yifu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chunyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xinggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>Wenjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_108"/>
        <dc:title>A Simple Baseline for Multi-Object Tracking</dc:title>
        <dcterms:abstract>There has been remarkable progress on object detection and re-identiﬁcation in recent years which are the core components for multiobject tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identiﬁcation branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problem. It remarkably outperforms the state-of-the-arts on the public datasets at 30 fps. We hope this baseline could inspire and help evaluate new ideas in this ﬁeld. The code and the pre-trained models are available at https://github.com/ifzhang/FairMOT.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>17</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_108">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/108/Zhang 等。 - A Simple Baseline for Multi-Object Tracking.pdf"/>
        <dc:title>Zhang 等。 - A Simple Baseline for Multi-Object Tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2004.01888.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-07 00:53:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://github.com/zotero/zotero">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>GitHub</dc:title></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_111"/>
        <dc:title>zotero/zotero</dc:title>
        <dcterms:abstract>Zotero is a free, easy-to-use tool to help you collect, organize, cite, and share your research sources. - zotero/zotero</dcterms:abstract>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://github.com/zotero/zotero</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-08 09:54:02</dcterms:dateSubmitted>
        <dc:description>Library Catalog: github.com</dc:description>
    </bib:Document>
    <z:Attachment rdf:about="#item_111">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/111/zh-CN.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/zotero/zotero/tree/master/chrome/locale/zh-CN</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-08 09:54:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://arxiv.org/abs/1901.08043">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_113"/>
        <dc:title>[1901.08043] Bottom-up Object Detection by Grouping Extreme and Center Points</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1901.08043</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-12 07:54:29</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_113">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/113/1901.html"/>
        <dc:title>[1901.08043] Bottom-up Object Detection by Grouping Extreme and Center Points</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1901.08043</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-12 07:54:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1901.08043">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1901.08043 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuo</foaf:surname>
                        <foaf:givenName>Jiacheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krähenbühl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_114"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Bottom-up Object Detection by Grouping Extreme and Center Points</dc:title>
        <dcterms:abstract>With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the ﬁve keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classiﬁcation or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.7% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.</dcterms:abstract>
        <dc:date>2019-04-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1901.08043</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-12 07:57:19</dcterms:dateSubmitted>
        <dc:description>arXiv: 1901.08043</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_114">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/114/Zhou 等。 - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf"/>
        <dc:title>Zhou 等。 - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1901.08043.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-12 07:57:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1603.00831">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1603.00831 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Milan</foaf:surname>
                        <foaf:givenName>Anton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leal-Taixe</foaf:surname>
                        <foaf:givenName>Laura</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reid</foaf:surname>
                        <foaf:givenName>Ian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schindler</foaf:surname>
                        <foaf:givenName>Konrad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_119"/>
        <link:link rdf:resource="#item_117"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>MOT16: A Benchmark for Multi-Object Tracking</dc:title>
        <dcterms:abstract>Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods [28]. The ﬁrst release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a signiﬁcant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest.</dcterms:abstract>
        <dc:date>2016-05-03</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>MOT16</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1603.00831</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 01:11:17</dcterms:dateSubmitted>
        <dc:description>arXiv: 1603.00831</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_119">
        <rdf:value>Comment: arXiv admin note: substantial text overlap with arXiv:1504.01942</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_117">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/117/Milan 等。 - 2016 - MOT16 A Benchmark for Multi-Object Tracking.pdf"/>
        <dc:title>Milan 等。 - 2016 - MOT16 A Benchmark for Multi-Object Tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1603.00831v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 01:11:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1504.01942">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1504.01942 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leal-Taixé</foaf:surname>
                        <foaf:givenName>Laura</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Milan</foaf:surname>
                        <foaf:givenName>Anton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reid</foaf:surname>
                        <foaf:givenName>Ian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schindler</foaf:surname>
                        <foaf:givenName>Konrad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_120"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking</dc:title>
        <dcterms:abstract>In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical ﬂow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset [20], targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and ﬁnally creating a uniﬁed evaluation system. With MOTChallenge we aim to pave the way toward a uniﬁed evaluation framework for a more meaningful quantiﬁcation of multi-target tracking.</dcterms:abstract>
        <dc:date>2015-04-08</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>MOTChallenge 2015</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1504.01942</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:17:01</dcterms:dateSubmitted>
        <dc:description>arXiv: 1504.01942</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_120">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/120/Leal-Taixé 等。 - 2015 - MOTChallenge 2015 Towards a Benchmark for Multi-T.pdf"/>
        <dc:title>Leal-Taixé 等。 - 2015 - MOTChallenge 2015 Towards a Benchmark for Multi-T.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1504.01942v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:16:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_122">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1901.08043 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuo</foaf:surname>
                        <foaf:givenName>Jiacheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krähenbühl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_116"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Bottom-up Object Detection by Grouping Extreme and Center Points</dc:title>
        <dcterms:abstract>With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the ﬁve keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classiﬁcation or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.7% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.</dcterms:abstract>
        <dc:date>2019-04-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1901.08043</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:30:46</dcterms:dateSubmitted>
        <dc:description>arXiv: 1901.08043</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_116">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/116/Zhou 等。 - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf"/>
        <dc:title>Zhou 等。 - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1901.08043.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-12 08:04:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s00418-018-1759-5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>151</prism:volume>
                <dc:title>Histochemistry and Cell Biology</dc:title>
                <dc:identifier>DOI 10.1007/s00418-018-1759-5</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Histochem Cell Biol</dcterms:alternative>
                <dc:identifier>ISSN 0948-6143, 1432-119X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Devan</foaf:surname>
                        <foaf:givenName>K. Shaga</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Walther</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>von Einem</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ropinski</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kestler</foaf:surname>
                        <foaf:givenName>H. A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Read</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_69"/>
        <dc:title>Detection of herpesvirus capsids in transmission electron microscopy images using transfer learning</dc:title>
        <dcterms:abstract>fasdsdf</dcterms:abstract>
        <dc:date>2/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s00418-018-1759-5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:30:55</dcterms:dateSubmitted>
        <bib:pages>101-114</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_69">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/69/Devan 等。 - 2019 - Detection of herpesvirus capsids in transmission e.pdf"/>
        <dc:title>Devan 等。 - 2019 - Detection of herpesvirus capsids in transmission e.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_124">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/124/ijcv.pdf"/>
        <dc:title>ijcv.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.columbia.edu/~vondrick/vatic/ijcv.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:37:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s11263-012-0564-1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0920-5691,%201573-1405"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vondrick</foaf:surname>
                        <foaf:givenName>Carl</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patterson</foaf:surname>
                        <foaf:givenName>Donald</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramanan</foaf:surname>
                        <foaf:givenName>Deva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_125"/>
        <dc:title>Efficiently Scaling up Crowdsourced Video Annotation: A Set of Best Practices for High Quality, Economical Video Labeling</dc:title>
        <dc:date>1/2013</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Efficiently Scaling up Crowdsourced Video Annotation</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-012-0564-1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:41:40</dcterms:dateSubmitted>
        <bib:pages>184-204</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0920-5691,%201573-1405">
        <prism:volume>101</prism:volume>
        <dc:title>International Journal of Computer Vision</dc:title>
        <dc:identifier>DOI 10.1007/s11263-012-0564-1</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
        <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_125">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/125/ijcv.pdf"/>
        <dc:title>ijcv.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.columbia.edu/~vondrick/vatic/ijcv.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 12:41:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_128">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>101</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-012-0564-1</dc:identifier>
                <prism:number>1</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vondrick</foaf:surname>
                        <foaf:givenName>Carl</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patterson</foaf:surname>
                        <foaf:givenName>Donald</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramanan</foaf:surname>
                        <foaf:givenName>Deva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_127"/>
        <dc:title>Efficiently Scaling up Crowdsourced Video Annotation: A Set of Best Practices for High Quality, Economical Video Labeling</dc:title>
        <dc:date>1/2013</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Efficiently Scaling up Crowdsourced Video Annotation</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-012-0564-1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 13:06:31</dcterms:dateSubmitted>
        <bib:pages>184-204</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_127">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/127/Vondrick 等。 - 2013 - Efficiently Scaling up Crowdsourced Video Annotati.pdf"/>
        <dc:title>Vondrick 等。 - 2013 - Efficiently Scaling up Crowdsourced Video Annotati.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.columbia.edu/~vondrick/vatic/ijcv.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-14 13:06:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.nature.com/articles/s41699-020-0137-z">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2397-7132"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Masubuchi</foaf:surname>
                        <foaf:givenName>Satoru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Eisuke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seo</foaf:surname>
                        <foaf:givenName>Yuta</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Okazaki</foaf:surname>
                        <foaf:givenName>Shota</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sasagawa</foaf:surname>
                        <foaf:givenName>Takao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Kenji</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taniguchi</foaf:surname>
                        <foaf:givenName>Takashi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Machida</foaf:surname>
                        <foaf:givenName>Tomoki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_129"/>
        <dc:title>Deep-learning-based image segmentation integrated with optical microscopy for automatically searching for two-dimensional materials</dc:title>
        <dc:date>12/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.nature.com/articles/s41699-020-0137-z</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:49:42</dcterms:dateSubmitted>
        <bib:pages>3</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2397-7132">
        <prism:volume>4</prism:volume>
        <dc:title>npj 2D Materials and Applications</dc:title>
        <dc:identifier>DOI 10.1038/s41699-020-0137-z</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>npj 2D Mater Appl</dcterms:alternative>
        <dc:identifier>ISSN 2397-7132</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_129">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/129/Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf"/>
        <dc:title>Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nature.com/articles/s41699-020-0137-z.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:49:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_132">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>4</prism:volume>
                <dc:title>npj 2D Materials and Applications</dc:title>
                <dc:identifier>DOI 10.1038/s41699-020-0137-z</dc:identifier>
                <prism:number>1</prism:number>
                <dcterms:alternative>npj 2D Mater Appl</dcterms:alternative>
                <dc:identifier>ISSN 2397-7132</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Masubuchi</foaf:surname>
                        <foaf:givenName>Satoru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Eisuke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seo</foaf:surname>
                        <foaf:givenName>Yuta</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Okazaki</foaf:surname>
                        <foaf:givenName>Shota</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sasagawa</foaf:surname>
                        <foaf:givenName>Takao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Kenji</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taniguchi</foaf:surname>
                        <foaf:givenName>Takashi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Machida</foaf:surname>
                        <foaf:givenName>Tomoki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_131"/>
        <dc:title>Deep-learning-based image segmentation integrated with optical microscopy for automatically searching for two-dimensional materials</dc:title>
        <dc:date>12/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.nature.com/articles/s41699-020-0137-z</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:51:21</dcterms:dateSubmitted>
        <bib:pages>3</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_131">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/131/Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf"/>
        <dc:title>Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nature.com/articles/s41699-020-0137-z.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:50:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_134">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>4</prism:volume>
                <dc:title>npj 2D Materials and Applications</dc:title>
                <dc:identifier>DOI 10.1038/s41699-020-0137-z</dc:identifier>
                <prism:number>1</prism:number>
                <dcterms:alternative>npj 2D Mater Appl</dcterms:alternative>
                <dc:identifier>ISSN 2397-7132</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Masubuchi</foaf:surname>
                        <foaf:givenName>Satoru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Eisuke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seo</foaf:surname>
                        <foaf:givenName>Yuta</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Okazaki</foaf:surname>
                        <foaf:givenName>Shota</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sasagawa</foaf:surname>
                        <foaf:givenName>Takao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watanabe</foaf:surname>
                        <foaf:givenName>Kenji</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taniguchi</foaf:surname>
                        <foaf:givenName>Takashi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Machida</foaf:surname>
                        <foaf:givenName>Tomoki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_133"/>
        <dc:title>Deep-learning-based image segmentation integrated with optical microscopy for automatically searching for two-dimensional materials</dc:title>
        <dc:date>12/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.nature.com/articles/s41699-020-0137-z</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:52:12</dcterms:dateSubmitted>
        <bib:pages>3</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_133">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/133/Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf"/>
        <dc:title>Masubuchi 等。 - 2020 - Deep-learning-based image segmentation integrated .pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nature.com/articles/s41699-020-0137-z.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-05-15 04:52:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1903.09254">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1903.09254 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naphade</foaf:surname>
                        <foaf:givenName>Milind</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Ming-Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xiaodong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Birchfield</foaf:surname>
                        <foaf:givenName>Stan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Shuo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>Ratnesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anastasiu</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hwang</foaf:surname>
                        <foaf:givenName>Jenq-Neng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_137"/>
        <link:link rdf:resource="#item_135"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification</dc:title>
        <dcterms:abstract>Urban trafﬁc optimization using trafﬁc cameras as sensors is driving the need to advance state-of-the-art multitarget multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale trafﬁc camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban trafﬁc ﬂow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identiﬁcation (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this ﬁeld, propel the state-of-the-art forward, and lead to deployed trafﬁc optimization(s) in the real world.</dcterms:abstract>
        <dc:date>2019-04-05</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CityFlow</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1903.09254</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-01 07:53:22</dcterms:dateSubmitted>
        <dc:description>arXiv: 1903.09254</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_137">
        <rdf:value>Comment: Accepted for oral presentation at CVPR 2019 with review ratings of 2 strong accepts and 1 accept (work done during an internship at NVIDIA)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_135">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/135/Tang 等。 - 2019 - CityFlow A City-Scale Benchmark for Multi-Target .pdf"/>
        <dc:title>Tang 等。 - 2019 - CityFlow A City-Scale Benchmark for Multi-Target .pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1903.09254</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-01 07:53:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1902.00749">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1902.00749 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Ji</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Nian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Minyoung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Wenjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Ming-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_140"/>
        <link:link rdf:resource="#item_138"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Online Multi-Object Tracking with Dual Matching Attention Networks</dc:title>
        <dcterms:abstract>In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a uniﬁed framework to handle noisy detections and frequent interactions between targets. Speciﬁcally, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates diﬀerent levels of attention to diﬀerent samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and oﬄine trackers in terms of identity-preserving metrics.</dcterms:abstract>
        <dc:date>2019-02-02</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1902.00749</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-06 08:38:51</dcterms:dateSubmitted>
        <dc:description>arXiv: 1902.00749</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_140">
       <rdf:value>Comment: Accepted by ECCV 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_138">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/138/Zhu 等。 - 2019 - Online Multi-Object Tracking with Dual Matching At.pdf"/>
        <dc:title>Zhu 等。 - 2019 - Online Multi-Object Tracking with Dual Matching At.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1902.00749v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-06 08:38:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_141">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/141/Object-tracking.pdf"/>
        <dc:title>Object-tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://mcl.usc.edu/wp-content/uploads/2016/12/Object-tracking.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-06 08:54:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1703.07402">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1703.07402 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojke</foaf:surname>
                        <foaf:givenName>Nicolai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bewley</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paulus</foaf:surname>
                        <foaf:givenName>Dietrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_144"/>
        <link:link rdf:resource="#item_142"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Simple Online and Realtime Tracking with a Deep Association Metric</dc:title>
        <dcterms:abstract>Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an ofﬂine pre-training stage where we learn a deep association metric on a largescale person re-identiﬁcation dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.</dcterms:abstract>
        <dc:date>2017-03-21</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1703.07402</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-06 09:00:53</dcterms:dateSubmitted>
        <dc:description>arXiv: 1703.07402</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_144">
       <rdf:value>Comment: 5 pages, 1 figure</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_142">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/142/Wojke 等。 - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf"/>
        <dc:title>Wojke 等。 - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1703.07402v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-06 09:00:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/S1077314220300035">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10773142"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Longyin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>Dawei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Zhaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lei</foaf:surname>
                        <foaf:givenName>Zhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>Ming-Ching</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Honggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lim</foaf:surname>
                        <foaf:givenName>Jongwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Ming-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lyu</foaf:surname>
                        <foaf:givenName>Siwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_145"/>
        <dc:title>UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking</dc:title>
        <dcterms:abstract>Effective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predefined setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traffic scenes (over 140,000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and tracking methods. Our analysis shows the complex effects of detection accuracy on MOT system performance. Based on these observations, we propose effective and informative evaluation metrics for MOT systems that consider the effect of object detection for comprehensive performance analysis.</dcterms:abstract>
        <dc:date>04/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>UA-DETRAC</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S1077314220300035</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-24 10:32:27</dcterms:dateSubmitted>
        <bib:pages>102907</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10773142">
        <prism:volume>193</prism:volume>
        <dc:title>Computer Vision and Image Understanding</dc:title>
        <dc:identifier>DOI 10.1016/j.cviu.2020.102907</dc:identifier>
        <dcterms:alternative>Computer Vision and Image Understanding</dcterms:alternative>
        <dc:identifier>ISSN 10773142</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_145">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/145/Wen 等。 - 2020 - UA-DETRAC A new benchmark and protocol for multi-.pdf"/>
        <dc:title>Wen 等。 - 2020 - UA-DETRAC A new benchmark and protocol for multi-.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2004.14619">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2004.14619 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naphade</foaf:surname>
                        <foaf:givenName>Milind</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Shuo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anastasiu</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>Ming-Ching</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xiaodong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Liang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharma</foaf:surname>
                        <foaf:givenName>Anuj</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chellappa</foaf:surname>
                        <foaf:givenName>Rama</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chakraborty</foaf:surname>
                        <foaf:givenName>Pranamesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_149"/>
        <link:link rdf:resource="#item_147"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Databases</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The 4th AI City Challenge</dc:title>
        <dcterms:abstract>The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can beneﬁt from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leveraged city-scale real trafﬁc data and highquality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efﬁciency. Track 2 addressed city-scale vehicle re-identiﬁcation with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed trafﬁc anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.</dcterms:abstract>
        <dc:date>2020-04-30</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.14619</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-24 10:33:25</dcterms:dateSubmitted>
        <dc:description>arXiv: 2004.14619</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_149">
        <rdf:value>Comment: Organization summary of the 4th AI City Challenge Workshop @ CVPR 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_147">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/147/Naphade 等。 - 2020 - The 4th AI City Challenge.pdf"/>
        <dc:title>Naphade 等。 - 2020 - The 4th AI City Challenge.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2004.14619v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-06-24 10:33:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1506.01186">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1506.01186 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smith</foaf:surname>
                        <foaf:givenName>Leslie N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_152"/>
        <link:link rdf:resource="#item_150"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Cyclical Learning Rates for Training Neural Networks</dc:title>
        <dcterms:abstract>It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.</dcterms:abstract>
        <dc:date>2017-04-04</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1506.01186</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-01 09:38:56</dcterms:dateSubmitted>
        <dc:description>arXiv: 1506.01186</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_152">
        <rdf:value>Comment: Presented at WACV 2017; see https://github.com/bckenstler/CLR for instructions to implement CLR in Keras</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_150">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/150/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf"/>
        <dc:title>Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1506.01186v6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-01 09:38:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_153">
        <z:itemType>newspaperArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Newspaper></bib:Newspaper>
        </dcterms:isPartOf>
    </bib:Article>
    <bib:Article rdf:about="#item_155">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yingying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Xue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xipeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Xiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Guanbin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Shilei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongwu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Errui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_154"/>
        <dc:title>Multi-Granularity Tracking With Modularlized Components for Unsupervised Vehicles Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection on road trafﬁc is a fundamental computer vision task and plays a critical role in video structure analysis and urban trafﬁc analysis. Although it has attracted intense attention in recent years, it remains a very challenging problem due to the complexity of the trafﬁc scene, the dense chaos of trafﬁc ﬂow and the lack of ﬁnegrained abnormal labeled data. In this paper, we propose a multi-granularity tracking approach with modularized components to analyze trafﬁc anomaly detection. The modularized framework consists of a detection module, a background modeling module, a mask extraction module, and a multi-granularity tracking algorithm. Concretely, a boxlevel tracking branch and a pixel-level tracking branch is employed respectively to make abnormal predictions. Each tracking branch helps to capture abnormal abstractions at different granularity levels and provide rich and complementary information for the concept learning of abnormal behaviors. Finally, a novel fusion and backtracking optimization is further performed to reﬁne the abnormal predictions. The experimental results reveal that our framework is superior in the Track4 test set of the NVIDIA AI CITY 2020 CHALLENGE, which ranked ﬁrst in this competition, with a 98.5% F1-score and 4.8737 root mean square error.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_154">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/154/Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf"/>
        <dc:title>Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Li_Multi-Granularity_Tracking_With_Modularlized_Components_for_Unsupervised_Vehicles_Anomaly_Detection_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-03 08:21:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_157">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yingying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Xue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xipeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Xiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Guanbin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Shilei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongwu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Errui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_156"/>
        <dc:title>Multi-Granularity Tracking With Modularlized Components for Unsupervised Vehicles Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection on road trafﬁc is a fundamental computer vision task and plays a critical role in video structure analysis and urban trafﬁc analysis. Although it has attracted intense attention in recent years, it remains a very challenging problem due to the complexity of the trafﬁc scene, the dense chaos of trafﬁc ﬂow and the lack of ﬁnegrained abnormal labeled data. In this paper, we propose a multi-granularity tracking approach with modularized components to analyze trafﬁc anomaly detection. The modularized framework consists of a detection module, a background modeling module, a mask extraction module, and a multi-granularity tracking algorithm. Concretely, a boxlevel tracking branch and a pixel-level tracking branch is employed respectively to make abnormal predictions. Each tracking branch helps to capture abnormal abstractions at different granularity levels and provide rich and complementary information for the concept learning of abnormal behaviors. Finally, a novel fusion and backtracking optimization is further performed to reﬁne the abnormal predictions. The experimental results reveal that our framework is superior in the Track4 test set of the NVIDIA AI CITY 2020 CHALLENGE, which ranked ﬁrst in this competition, with a 98.5% F1-score and 4.8737 root mean square error.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_156">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/156/Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf"/>
        <dc:title>Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Li_Multi-Granularity_Tracking_With_Modularlized_Components_for_Unsupervised_Vehicles_Anomaly_Detection_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-03 08:23:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_159">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yingying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Jie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Xue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xipeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Xiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Guanbin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Shilei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongwu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Errui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_158"/>
        <dc:title>Multi-Granularity Tracking With Modularlized Components for Unsupervised Vehicles Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection on road trafﬁc is a fundamental computer vision task and plays a critical role in video structure analysis and urban trafﬁc analysis. Although it has attracted intense attention in recent years, it remains a very challenging problem due to the complexity of the trafﬁc scene, the dense chaos of trafﬁc ﬂow and the lack of ﬁnegrained abnormal labeled data. In this paper, we propose a multi-granularity tracking approach with modularized components to analyze trafﬁc anomaly detection. The modularized framework consists of a detection module, a background modeling module, a mask extraction module, and a multi-granularity tracking algorithm. Concretely, a boxlevel tracking branch and a pixel-level tracking branch is employed respectively to make abnormal predictions. Each tracking branch helps to capture abnormal abstractions at different granularity levels and provide rich and complementary information for the concept learning of abnormal behaviors. Finally, a novel fusion and backtracking optimization is further performed to reﬁne the abnormal predictions. The experimental results reveal that our framework is superior in the Track4 test set of the NVIDIA AI CITY 2020 CHALLENGE, which ranked ﬁrst in this competition, with a 98.5% F1-score and 4.8737 root mean square error.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_158">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/158/Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf"/>
        <dc:title>Li 等。 - Multi-Granularity Tracking With Modularlized Compo.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Li_Multi-Granularity_Tracking_With_Modularlized_Components_for_Unsupervised_Vehicles_Anomaly_Detection_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-03 08:24:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2002.05540">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2002.05540 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Perreault</foaf:surname>
                        <foaf:givenName>Hughes</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bilodeau</foaf:surname>
                        <foaf:givenName>Guillaume-Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saunier</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Héritier</foaf:surname>
                        <foaf:givenName>Maguelonne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_162"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SpotNet: Self-Attention Multi-Task Network for Object Detection</dc:title>
        <dcterms:abstract>Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical ﬂow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a signiﬁcant mAP improvement on two trafﬁc surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.</dcterms:abstract>
        <dc:date>2020-06-11</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SpotNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2002.05540</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-07 09:48:01</dcterms:dateSubmitted>
        <dc:description>arXiv: 2002.05540</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_162">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/162/Perreault 等。 - 2020 - SpotNet Self-Attention Multi-Task Network for Obj.pdf"/>
        <dc:title>Perreault 等。 - 2020 - SpotNet Self-Attention Multi-Task Network for Obj.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2002.05540.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-07 09:47:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2004.10934">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2004.10934 [cs, eess]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bochkovskiy</foaf:surname>
                        <foaf:givenName>Alexey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chien-Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>Hong-Yuan Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_164"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Image and Video Processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>YOLOv4: Optimal Speed and Accuracy of Object Detection</dc:title>
        <dcterms:abstract>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justiﬁcation of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ∼65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.</dcterms:abstract>
        <dc:date>2020-04-22</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>YOLOv4</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.10934</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-08 11:37:29</dcterms:dateSubmitted>
        <dc:description>arXiv: 2004.10934</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_164">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/164/Bochkovskiy 等。 - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf"/>
        <dc:title>Bochkovskiy 等。 - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2004.10934.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-08 11:37:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1908.01998">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1908.01998 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuo</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Chi-Keung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tai</foaf:surname>
                        <foaf:givenName>Yu-Wing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_168"/>
        <link:link rdf:resource="#item_166"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</dc:title>
        <dcterms:abstract>Conventional methods for object detection usually requires substantial amount of training data and to prepare such high quality training data is labor intensive. In this paper, we propose few-shot object detection which aims to detect objects of unseen class with a few training examples. Central to our method is the Attention-RPN and the multi-relation module which fully exploit the similarity between the few shot training examples and the test set to detect novel objects while suppressing the false detection in background. To train our network, we have prepared a new dataset which contains 1000 categories of varies objects with high quality annotations. To the best of our knowledge, this is also the ﬁrst dataset speciﬁcally designed for few shot object detection. Once our network is trained, we can apply object detection for unseen classes without further training or ﬁne tuning. This is also the major advantage of few shot object detection. Our method is general, and has a wide range of applications. We demonstrate the effectiveness of our method quantitatively and qualitatively on different datasets. The dataset link is: https://github.com/fanq15/Few-Shot-Object-DetectionDataset.</dcterms:abstract>
        <dc:date>2020-05-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1908.01998</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-12 01:31:29</dcterms:dateSubmitted>
        <dc:description>arXiv: 1908.01998</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_168">
        <rdf:value>Comment: CVPR2020 Camera Ready. (Fix Figure 3 and Table 5. More implementation details in the supplementary material.)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_166">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/166/Fan 等。 - 2020 - Few-Shot Object Detection with Attention-RPN and M.pdf"/>
        <dc:title>Fan 等。 - 2020 - Few-Shot Object Detection with Attention-RPN and M.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1908.01998v1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-12 01:31:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2005.09973">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2005.09973 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Xingjia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Yuqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheng</foaf:surname>
                        <foaf:givenName>Kekai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Weiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Haolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Xiaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Chongyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Changsheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_171"/>
        <link:link rdf:resource="#item_169"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Dynamic Refinement Network for Oriented and Densely Packed Object Detection</dc:title>
        <dcterms:abstract>Object detection has achieved remarkable progress in the past decade. However, the detection of oriented and densely packed objects remains challenging because of following inherent reasons: (1) receptive ﬁelds of neurons are all axis-aligned and of the same shape, whereas objects are usually of diverse shapes and align along various directions; (2) detection models are typically trained with generic knowledge and may not generalize well to handle speciﬁc objects at test time; (3) the limited dataset hinders the development on this task. To resolve the ﬁrst two issues, we present a dynamic reﬁnement network which consists of two novel components, i.e., a feature selection module (FSM) and a dynamic reﬁnement head (DRH). Our FSM enables neurons to adjust receptive ﬁelds in accordance with the shapes and orientations of target objects, whereas the DRH empowers our model to reﬁne the prediction dynamically in an object-aware manner. To address the limited availability of related benchmarks, we collect an extensive and fully annotated dataset, namely, SKU110K-R, which is relabeled with oriented bounding boxes based on SKU110K. We perform quantitative evaluations on several publicly available benchmarks including DOTA, HRSC2016, SKU110K, and our own SKU110K-R dataset. Experimental results show that our method achieves consistent and substantial gains compared with baseline approaches. The code and dataset are available at https://github. com/Anymake/DRN_CVPR2020.</dcterms:abstract>
        <dc:date>2020-06-10</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.09973</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-12 02:20:09</dcterms:dateSubmitted>
        <dc:description>arXiv: 2005.09973</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_171">
       <rdf:value>Comment: Accepted by CVPR 2020 as Oral</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_169">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/169/Pan 等。 - 2020 - Dynamic Refinement Network for Oriented and Densel.pdf"/>
        <dc:title>Pan 等。 - 2020 - Dynamic Refinement Network for Oriented and Densel.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2005.09973v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-12 02:19:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_173">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/173/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf"/>
        <dc:title>Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-14 08:03:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_174">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/174/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf"/>
        <dc:title>Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-14 08:07:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_176">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shine</foaf:surname>
                        <foaf:givenName>Linu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_175"/>
        <dc:title>Fractional Data Distillation Model for Anomaly Detection in Traffic Videos</dc:title>
        <dcterms:abstract>Timely automatic detection of anomalies like road accidents forms the key to any intelligent trafﬁc monitoring system. In this paper, we propose a novel Fractional Data Distillation model for segregating trafﬁc anomaly videos from a test dataset, with a precise estimation of the start time of the anomalous event. The model follows a similar approach to that of the typical fractional distillation procedure, where the compounds are separated by varying the temperature. Our model fractionally extracts the anomalous events depending on their nature as the detection process progresses. Here, we employ two anomaly extractors namely Normal and Zoom, of which former works on the normal scale of video and the latter works on the magniﬁed scale on the videos missed by the former, to separate the anomalies. The backbone of this segregation is scanning the background frames using the YOLOv3 detector for spotting possible anomalies. These anomaly candidates are further ﬁltered and compared with detection on the foreground for matching detections to estimate the start time of the anomalous event. Experimental validation on track 4 of 2020 AI City Challenge shows an s4 score of 0.5438, with an F1 score of 0.7018.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>9</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_175">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/175/Shine - Fractional Data Distillation Model for Anomaly Det.pdf"/>
        <dc:title>Shine - Fractional Data Distillation Model for Anomaly Det.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_177">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/177/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf"/>
        <dc:title>Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-14 08:20:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1904.07850">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1904.07850 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Dequan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krähenbühl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_184"/>
        <dcterms:isReferencedBy rdf:resource="#item_179"/>
        <link:link rdf:resource="#item_67"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Objects as Points</dc:title>
        <dcterms:abstract>Detection identiﬁes objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefﬁcient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to ﬁnd center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.</dcterms:abstract>
        <dc:date>2019-04-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.07850</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-15 12:46:20</dcterms:dateSubmitted>
        <dc:description>arXiv: 1904.07850</dc:description>
    </bib:Article>
   <bib:Memo rdf:about="#item_184"><rdf:value></rdf:value></bib:Memo>
    <bib:Memo rdf:about="#item_179">
       <rdf:value>&lt;p&gt;Comment: 12 pages, 5 figures&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_67">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/67/centernet.pdf"/>
        <dc:title>centernet.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_182">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Law</foaf:surname>
                        <foaf:givenName>Hei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Jia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_181"/>
        <dc:title>CornerNet: Detecting Objects as Paired Keypoints</dc:title>
        <dcterms:abstract>We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.1% AP on MS COCO, outperforming all existing one-stage detectors.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>17</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_181">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/181/Law 和 Deng - CornerNet Detecting Objects as Paired Keypoints.pdf"/>
        <dc:title>Law 和 Deng - CornerNet Detecting Objects as Paired Keypoints.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-22 01:55:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_183">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/183/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf"/>
        <dc:title>Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-22 01:56:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.hindawi.com/journals/sp/2020/7845384/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1058-9244,%201875-919X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yuanyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yan</foaf:surname>
                        <foaf:givenName>Wan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Genke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Jiliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Jianan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_185"/>
        <dc:title>CenterFace: Joint Face Detection and Alignment Using Face as Point</dc:title>
        <dcterms:abstract>Face detection and alignment in unconstrained environment is always deployed on edge devices which have limited memory storage and low computing power. This paper proposes a one-stage method named CenterFace to simultaneously predict facial box and landmark location with real-time speed and high accuracy. The proposed method also belongs to the anchor free category. This is achieved by: (a) learning face existing possibility by the semantic maps, (b) learning bounding box, offsets and five landmarks for each position that potentially contains a face. Specifically, the method can run in real-time on a single CPU core and 200 FPS using NVIDIA 2080TI for VGA-resolution images, and can simultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy: 0.935/0.932, Medium: 0.924/0.921, Hard: 0.875/0.873 and FDDB discontinuous: 0.980, continuous: 0.732). A demo of CenterFace can be available at https://github.com/Star-Clouds/CenterFace.</dcterms:abstract>
        <dc:date>2020-07-02</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CenterFace</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.hindawi.com/journals/sp/2020/7845384/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-23 08:14:50</dcterms:dateSubmitted>
        <bib:pages>1-8</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1058-9244,%201875-919X">
        <prism:volume>2020</prism:volume>
        <dc:title>Scientific Programming</dc:title>
        <dc:identifier>DOI 10.1155/2020/7845384</dc:identifier>
        <dcterms:alternative>Scientific Programming</dcterms:alternative>
        <dc:identifier>ISSN 1058-9244, 1875-919X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_185">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/185/Xu 等。 - 2020 - CenterFace Joint Face Detection and Alignment Usi.pdf"/>
        <dc:title>Xu 等。 - 2020 - CenterFace Joint Face Detection and Alignment Usi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://cn.arxiv.org/ftp/arxiv/papers/1911/1911.03599.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-23 08:14:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2003.09119">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2003.09119 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Guoxuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>Yue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Fei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Pengju</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qian</foaf:surname>
                        <foaf:givenName>Chen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_189"/>
        <link:link rdf:resource="#item_187"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection</dc:title>
        <dcterms:abstract>Keypoint-based detectors have achieved pretty-well performance. However, incorrect keypoint matching is still widespread and greatly affects the performance of the detector. In this paper, we propose CentripetalNet which uses centripetal shift to pair corner keypoints from the same instance. CentripetalNet predicts the position and the centripetal shift of the corner points and matches corners whose shifted results are aligned. Combining position information, our approach matches corner points more accurately than the conventional embedding approaches do. Corner pooling extracts information inside the bounding boxes onto the border. To make this information more aware at the corners, we design a cross-star deformable convolution network to conduct feature adaption. Furthermore, we explore instance segmentation on anchor-free detectors by equipping our CentripetalNet with a mask prediction module. On MS-COCO test-dev, our CentripetalNet not only outperforms all existing anchor-free detectors with an AP of 48.0% but also achieves comparable performance to the state-of-the-art instance segmentation approaches with a 40.2% M askAP . Code will be available at https: //github.com/KiveeDong/CentripetalNet.</dcterms:abstract>
        <dc:date>2020-03-20</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CentripetalNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2003.09119</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-23 08:38:08</dcterms:dateSubmitted>
        <dc:description>arXiv: 2003.09119</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_189">
       <rdf:value>Comment: Accepted by CVPR2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_187">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/187/Dong 等。 - 2020 - CentripetalNet Pursuing High-quality Keypoint Pai.pdf"/>
        <dc:title>Dong 等。 - 2020 - CentripetalNet Pursuing High-quality Keypoint Pai.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2003.09119.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-23 08:38:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1911.06667">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1911.06667 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Youngwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Jongyoul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_192"/>
        <link:link rdf:resource="#item_190"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CenterMask : Real-Time Anchor-Free Instance Segmentation</dc:title>
        <dcterms:abstract>We propose a simple yet efﬁcient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchorfree one stage object detector (FCOS [33]) in the same vein with Mask R-CNN [9]. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise. We also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet [19] and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE. With SAG-Mask and VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted each to large and small models, respectively. Using the same ResNet-101-FPN backbone, CenterMask achieves 38.3%, surpassing all previous state-ofthe-art methods while at a much faster speed. CenterMaskLite also outperforms the state-of-the-art by large margins at over 35fps on Titan Xp. We hope that CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance segmentation and backbone network for various vision tasks, respectively. The Code is available at https: //github.com/youngwanLEE/CenterMask.</dcterms:abstract>
        <dc:date>2020-04-02</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CenterMask</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.06667</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:18:40</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.06667</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_192">
       <rdf:value>Comment: CVPR 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_190">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/190/Lee 和 Park - 2020 - CenterMask  Real-Time Anchor-Free Instance Segmen.pdf"/>
        <dc:title>Lee 和 Park - 2020 - CenterMask  Real-Time Anchor-Free Instance Segmen.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1911.06667.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:18:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1904.01355">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1904.01355 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Zhi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Chunhua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_195"/>
        <link:link rdf:resource="#item_193"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>FCOS: Fully Convolutional One-Stage Object Detection</dc:title>
        <dcterms:abstract>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1</dcterms:abstract>
        <dc:date>2019-08-20</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>FCOS</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.01355</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:29:19</dcterms:dateSubmitted>
        <dc:description>arXiv: 1904.01355</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_195">
        <rdf:value>Comment: Accepted to Proc. Int. Conf. Computer Vision 2019. 13 pages. Code is available at: https://github.com/tianzhi0549/FCOS/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_193">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/193/Tian 等。 - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf"/>
        <dc:title>Tian 等。 - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1904.01355.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:29:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1912.02424">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1912.02424 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Shifeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chi</foaf:surname>
                        <foaf:givenName>Cheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>Yongqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lei</foaf:surname>
                        <foaf:givenName>Zhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Stan Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_198"/>
        <link:link rdf:resource="#item_196"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</dc:title>
        <dcterms:abstract>Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we ﬁrst point out that the essential difference between anchor-based and anchor-free detection is actually how to deﬁne positive and negative training samples, which leads to the performance gap between them. If they adopt the same deﬁnition of positive and negative samples during training, there is no obvious difference in the ﬁnal performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It signiﬁcantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve stateof-the-art detectors by a large margin to 50.7% AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS.</dcterms:abstract>
        <dc:date>2020-06-20</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1912.02424</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:38:04</dcterms:dateSubmitted>
        <dc:description>arXiv: 1912.02424</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_198">
        <rdf:value>Comment: Accepted by CVPR 2020 as Oral; Best Paper Nomination</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_196">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/196/Zhang 等。 - 2020 - Bridging the Gap Between Anchor-based and Anchor-f.pdf"/>
        <dc:title>Zhang 等。 - 2020 - Bridging the Gap Between Anchor-based and Anchor-f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1912.02424.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 06:37:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1506.02640">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1506.02640 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Redmon</foaf:surname>
                        <foaf:givenName>Joseph</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Divvala</foaf:surname>
                        <foaf:givenName>Santosh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farhadi</foaf:surname>
                        <foaf:givenName>Ali</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_199"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>You Only Look Once: Unified, Real-Time Object Detection</dc:title>
        <dcterms:abstract>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.</dcterms:abstract>
        <dc:date>2016-05-09</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>You Only Look Once</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1506.02640</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 11:34:33</dcterms:dateSubmitted>
        <dc:description>arXiv: 1506.02640</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_199">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/199/Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf"/>
        <dc:title>Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1506.02640v5.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 11:34:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_202">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2020</prism:volume>
                <dc:title>Scientific Programming</dc:title>
                <dc:identifier>DOI 10.1155/2020/7845384</dc:identifier>
                <dcterms:alternative>Scientific Programming</dcterms:alternative>
                <dc:identifier>ISSN 1058-9244, 1875-919X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yuanyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yan</foaf:surname>
                        <foaf:givenName>Wan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Genke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Jiliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Jianan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_201"/>
        <dc:title>CenterFace: Joint Face Detection and Alignment Using Face as Point</dc:title>
        <dcterms:abstract>Face detection and alignment in unconstrained environment is always deployed on edge devices which have limited memory storage and low computing power. This paper proposes a one-stage method named CenterFace to simultaneously predict facial box and landmark location with real-time speed and high accuracy. The proposed method also belongs to the anchor free category. This is achieved by: (a) learning face existing possibility by the semantic maps, (b) learning bounding box, offsets and five landmarks for each position that potentially contains a face. Specifically, the method can run in real-time on a single CPU core and 200 FPS using NVIDIA 2080TI for VGA-resolution images, and can simultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy: 0.935/0.932, Medium: 0.924/0.921, Hard: 0.875/0.873 and FDDB discontinuous: 0.980, continuous: 0.732). A demo of CenterFace can be available at https://github.com/Star-Clouds/CenterFace.</dcterms:abstract>
        <dc:date>2020-07-02</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CenterFace</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.hindawi.com/journals/sp/2020/7845384/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 11:59:28</dcterms:dateSubmitted>
        <bib:pages>1-8</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_201">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/201/Xu 等。 - 2020 - CenterFace Joint Face Detection and Alignment Usi.pdf"/>
        <dc:title>Xu 等。 - 2020 - CenterFace Joint Face Detection and Alignment Usi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://cn.arxiv.org/ftp/arxiv/papers/1911/1911.03599.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 11:59:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_204">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1506.02640 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Redmon</foaf:surname>
                        <foaf:givenName>Joseph</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Divvala</foaf:surname>
                        <foaf:givenName>Santosh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farhadi</foaf:surname>
                        <foaf:givenName>Ali</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_203"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>You Only Look Once: Unified, Real-Time Object Detection</dc:title>
        <dcterms:abstract>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.</dcterms:abstract>
        <dc:date>2016-05-09</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>You Only Look Once</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1506.02640</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 12:00:21</dcterms:dateSubmitted>
        <dc:description>arXiv: 1506.02640</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_203">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/203/Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf"/>
        <dc:title>Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1506.02640v5.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-24 12:00:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2007.12099">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2007.12099 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>Xiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Kaipeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Guanzhong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dang</foaf:surname>
                        <foaf:givenName>Qingqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Yuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Hui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Jianguo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Shumin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Errui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Shilei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_205"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PP-YOLO: An Effective and Efficient Implementation of Object Detector</dc:title>
        <dcterms:abstract>Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacriﬁce accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efﬁciency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efﬁciency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PPYOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2% mAP) and efﬁciency (72.9 FPS), surpassing the existing state-of-theart detectors such as EfﬁcientDet and YOLOv4. Source code is at https://github.com/PaddlePaddle/ PaddleDetection.</dcterms:abstract>
        <dc:date>2020-07-23</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PP-YOLO</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2007.12099</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-25 10:33:45</dcterms:dateSubmitted>
        <dc:description>arXiv: 2007.12099</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_205">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/205/Long 等。 - 2020 - PP-YOLO An Effective and Efficient Implementation.pdf"/>
        <dc:title>Long 等。 - 2020 - PP-YOLO An Effective and Efficient Implementation.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2007.12099.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-25 10:33:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.11275">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2006.11275 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yin</foaf:surname>
                        <foaf:givenName>Tianwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krähenbühl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_207"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Center-based 3D Object Detection and Tracking</dc:title>
        <dcterms:abstract>Three-dimensional objects are commonly represented as 3D boxes in a pointcloud. This representation mimics the well-studied image-based 2D boundingbox detection, but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difﬁculties enumerating all orientations or ﬁtting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. We use a keypoint detector to ﬁnd centers of objects, and simply regress to other attributes, including 3D size, 3D orientation, and velocity. In our centerbased framework, 3D object tracking simpliﬁes to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efﬁcient, and effective. On the nuScenes dataset, our point-based representations perform 3-4 mAP higher than the box-based counterparts for 3D detection, and 6 AMOTA higher for 3D tracking. Our real-time model runs end-to-end 3D detection and tracking at 30 FPS with 54.2 AMOTA and 48.3 mAP while the best single model achieves 60.3 mAP for 3D detection, and 63.8 AMOTA for 3D tracking. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.</dcterms:abstract>
        <dc:date>2020-06-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.11275</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:12:57</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.11275</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_207">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/207/Yin 等。 - 2020 - Center-based 3D Object Detection and Tracking.pdf"/>
        <dc:title>Yin 等。 - 2020 - Center-based 3D Object Detection and Tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.11275.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:12:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.12671">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2006.12671 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ge</foaf:surname>
                        <foaf:givenName>Runzhou</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Zhuangzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Yihan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Sijia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Li</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_211"/>
        <link:link rdf:resource="#item_209"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>AFDet: Anchor Free One Stage 3D Object Detection</dc:title>
        <dcterms:abstract>High-efﬁciency point cloud 3D object detection operated on embedded systems is important for many robotics applications including autonomous driving. Most previous works try to solve it using anchor-based detection methods which come with two drawbacks: post-processing is relatively complex and computationally expensive; tuning anchor parameters is tricky. We are the ﬁrst to address these drawbacks with an anchor free and Non-Maximum Suppression free one stage detector called AFDet. The entire AFDet can be processed efﬁciently on a CNN accelerator or a GPU with the simpliﬁed post-processing. Without bells and whistles, our proposed AFDet performs competitively with other one stage anchor-based methods on KITTI validation set and Waymo Open Dataset validation set.</dcterms:abstract>
        <dc:date>2020-06-30</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AFDet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.12671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:25:55</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.12671</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_211">
        <rdf:value>Comment: Accepted on May 6th, 2020 by CVPRW 2020, published on June 7th, 2020; Baseline detector for the 1st place solutions of Waymo Open Dataset Challenges 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_209">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/209/Ge 等。 - 2020 - AFDet Anchor Free One Stage 3D Object Detection.pdf"/>
        <dc:title>Ge 等。 - 2020 - AFDet Anchor Free One Stage 3D Object Detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.12671.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:25:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_213">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2006.12671 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ge</foaf:surname>
                        <foaf:givenName>Runzhou</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Zhuangzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Yihan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Sijia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Li</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_214"/>
        <link:link rdf:resource="#item_212"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>AFDet: Anchor Free One Stage 3D Object Detection</dc:title>
        <dcterms:abstract>High-efﬁciency point cloud 3D object detection operated on embedded systems is important for many robotics applications including autonomous driving. Most previous works try to solve it using anchor-based detection methods which come with two drawbacks: post-processing is relatively complex and computationally expensive; tuning anchor parameters is tricky. We are the ﬁrst to address these drawbacks with an anchor free and Non-Maximum Suppression free one stage detector called AFDet. The entire AFDet can be processed efﬁciently on a CNN accelerator or a GPU with the simpliﬁed post-processing. Without bells and whistles, our proposed AFDet performs competitively with other one stage anchor-based methods on KITTI validation set and Waymo Open Dataset validation set.</dcterms:abstract>
        <dc:date>2020-06-30</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AFDet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.12671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:34:48</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.12671</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_214">
        <rdf:value>Comment: Accepted on May 6th, 2020 by CVPRW 2020, published on June 7th, 2020; Baseline detector for the 1st place solutions of Waymo Open Dataset Challenges 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_212">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/212/Ge 等。 - 2020 - AFDet Anchor Free One Stage 3D Object Detection.pdf"/>
        <dc:title>Ge 等。 - 2020 - AFDet Anchor Free One Stage 3D Object Detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.12671.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-27 02:34:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1906.11172">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1906.11172 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zoph</foaf:surname>
                        <foaf:givenName>Barret</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cubuk</foaf:surname>
                        <foaf:givenName>Ekin D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghiasi</foaf:surname>
                        <foaf:givenName>Golnaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shlens</foaf:surname>
                        <foaf:givenName>Jonathon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_215"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning Data Augmentation Strategies for Object Detection</dc:title>
        <dcterms:abstract>Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example, the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regularization methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online at https://github.com/tensorflow/tpu/tree/master/models/official/detection</dcterms:abstract>
        <dc:date>2019-06-26</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1906.11172</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-28 01:32:27</dcterms:dateSubmitted>
        <dc:description>arXiv: 1906.11172</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_215">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/215/Zoph 等。 - 2019 - Learning Data Augmentation Strategies for Object D.pdf"/>
        <dc:title>Zoph 等。 - 2019 - Learning Data Augmentation Strategies for Object D.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1906.11172.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-28 01:32:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_218">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1906.11172 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zoph</foaf:surname>
                        <foaf:givenName>Barret</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cubuk</foaf:surname>
                        <foaf:givenName>Ekin D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghiasi</foaf:surname>
                        <foaf:givenName>Golnaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shlens</foaf:surname>
                        <foaf:givenName>Jonathon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_217"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning Data Augmentation Strategies for Object Detection</dc:title>
        <dcterms:abstract>Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example, the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regularization methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online at https://github.com/tensorflow/tpu/tree/master/models/official/detection</dcterms:abstract>
        <dc:date>2019-06-26</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1906.11172</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-28 01:44:21</dcterms:dateSubmitted>
        <dc:description>arXiv: 1906.11172</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_217">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/217/1906.11172.pdf"/>
        <dc:title>1906.11172.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1906.11172.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-28 01:44:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_219">
        <z:itemType>newspaperArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Newspaper></bib:Newspaper>
        </dcterms:isPartOf>
    </bib:Article>
    <bib:Article rdf:about="http://arxiv.org/abs/1708.02002">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1708.02002 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goyal</foaf:surname>
                        <foaf:givenName>Priya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dollár</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_220"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Focal Loss for Dense Object Detection</dc:title>
        <dcterms:abstract>The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.</dcterms:abstract>
        <dc:date>2018-02-07</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1708.02002</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-31 02:34:15</dcterms:dateSubmitted>
        <dc:description>arXiv: 1708.02002</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_220">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/220/Lin 等。 - 2018 - Focal Loss for Dense Object Detection.pdf"/>
        <dc:title>Lin 等。 - 2018 - Focal Loss for Dense Object Detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1708.02002.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-07-31 02:34:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_223">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Tsung-Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Jiarui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hsu</foaf:surname>
                        <foaf:givenName>Hung-Min</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hwang</foaf:surname>
                        <foaf:givenName>Jenq-Neng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_222"/>
        <dc:title>Multi-View Vehicle Re-Identification using Temporal Attention Model and Metadata Re-ranking</dc:title>
        <dcterms:abstract>Object re-identiﬁcation (ReID) is an arduous task which requires matching an object across different nonoverlapping camera views. Recently, many researchers are working on person ReID by taking advantages of appearance, human pose, temporal constraints, etc. However, vehicle ReID is even more challenging because vehicles have fewer discriminant features than human due to viewpoint orientation, changes in lighting condition and inter-class similarity. In this paper, we propose a viewpoint-aware temporal attention model for vehicle ReID utilizing deep learning features extracted from consecutive frames with vehicle orientation and metadata attributes (i.e., type, brand, color) being taken into consideration. In addition, re-ranking with soft decision boundary is applied as post-processing for result reﬁnement. The proposed method is evaluated on CVPR AI City Challenge 2019 dataset, achieving mAP of 79.17% with the second place ranking in the competition.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>9</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_222">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/222/Huang 等。 - Multi-View Vehicle Re-Identification using Tempora.pdf"/>
        <dc:title>Huang 等。 - Multi-View Vehicle Re-Identification using Tempora.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Huang_Multi-View_Vehicle_Re-Identification_using_Temporal_Attention_Model_and_Metadata_Re-ranking_CVPRW_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-04 02:38:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1512.04412">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1512.04412 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>Jifeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_226"/>
        <link:link rdf:resource="#item_224"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Instance-aware Semantic Segmentation via Multi-task Network Cascades</dc:title>
        <dcterms:abstract>Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems.</dcterms:abstract>
        <dc:date>2015-12-14</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1512.04412</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-04 02:56:20</dcterms:dateSubmitted>
        <dc:description>arXiv: 1512.04412</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_226">
        <rdf:value>&lt;p&gt;Comment: Tech report. 1st-place winner of MS COCO 2015 segmentation competition&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_224">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/224/Dai 等。 - 2015 - Instance-aware Semantic Segmentation via Multi-tas.pdf"/>
        <dc:title>Dai 等。 - 2015 - Instance-aware Semantic Segmentation via Multi-tas.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1512.04412v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-04 02:56:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_227">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/227/maskrcnn_iccv2017_oral_kaiminghe.pdf"/>
        <dc:title>maskrcnn_iccv2017_oral_kaiminghe.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://kaiminghe.com/iccv17maskrcnn/maskrcnn_iccv2017_oral_kaiminghe.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-04 02:59:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6889-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6889-6</dc:identifier>
                <dc:title>Proceedings of the 27th ACM International Conference on Multimedia</dc:title>
                <dc:identifier>DOI 10.1145/3343031.3350924</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Nice France</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>ACM</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yizhou</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Yen-Ting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hwang</foaf:surname>
                        <foaf:givenName>Jenq-Neng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_228"/>
        <dc:title>Monocular Visual Object 3D Localization in Road Scenes</dc:title>
        <dc:date>2019-10-15</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/10.1145/3343031.3350924</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-05 07:18:03</dcterms:dateSubmitted>
        <bib:pages>917-925</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>MM '19: The 27th ACM International Conference on Multimedia</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_228">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/228/Wang 等。 - 2019 - Monocular Visual Object 3D Localization in Road Sc.pdf"/>
        <dc:title>Wang 等。 - 2019 - Monocular Visual Object 3D Localization in Road Sc.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1703.06870">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1703.06870 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gkioxari</foaf:surname>
                        <foaf:givenName>Georgia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dollár</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_232"/>
        <link:link rdf:resource="#item_230"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Mask R-CNN</dc:title>
        <dcterms:abstract>We present a conceptually simple, ﬂexible, and general framework for object instance segmentation. Our approach efﬁciently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.</dcterms:abstract>
        <dc:date>2018-01-24</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1703.06870</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-05 07:20:11</dcterms:dateSubmitted>
        <dc:description>arXiv: 1703.06870</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_232">
       <rdf:value>Comment: open source; appendix on more results</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_230">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/230/1703.pdf"/>
        <dc:title>1703.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1703.06870v3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-05 07:20:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_233">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/233/maskrcnn_iccv2017_oral_kaiminghe.pdf"/>
        <dc:title>maskrcnn_iccv2017_oral_kaiminghe.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://kaiminghe.com/iccv17maskrcnn/maskrcnn_iccv2017_oral_kaiminghe.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-05 07:30:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_234">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/234/maskrcnn_iccv2017_tutorial_kaiminghe.pdf"/>
        <dc:title>maskrcnn_iccv2017_tutorial_kaiminghe.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-05 07:38:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2196-1115"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shorten</foaf:surname>
                        <foaf:givenName>Connor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khoshgoftaar</foaf:surname>
                        <foaf:givenName>Taghi M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_235"/>
        <dc:title>A survey on Image Data Augmentation for Deep Learning</dc:title>
        <dcterms:abstract>Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.</dcterms:abstract>
        <dc:date>12/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-11 11:03:40</dcterms:dateSubmitted>
        <bib:pages>60</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2196-1115">
        <prism:volume>6</prism:volume>
        <dc:title>Journal of Big Data</dc:title>
        <dc:identifier>DOI 10.1186/s40537-019-0197-0</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J Big Data</dcterms:alternative>
        <dc:identifier>ISSN 2196-1115</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_235">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/235/Shorten 和 Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf"/>
        <dc:title>Shorten 和 Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_237">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/237/hdrnet.pdf"/>
        <dc:title>hdrnet.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://groups.csail.mit.edu/graphics/hdrnet/data/hdrnet.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-12 03:52:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1512.02325">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>9905</prism:volume>
                <dc:title>arXiv:1512.02325 [cs]</dc:title>
                <dc:identifier>DOI 10.1007/978-3-319-46448-0_2</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anguelov</foaf:surname>
                        <foaf:givenName>Dragomir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Erhan</foaf:surname>
                        <foaf:givenName>Dumitru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szegedy</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reed</foaf:surname>
                        <foaf:givenName>Scott</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fu</foaf:surname>
                        <foaf:givenName>Cheng-Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Berg</foaf:surname>
                        <foaf:givenName>Alexander C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_240"/>
        <link:link rdf:resource="#item_238"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SSD: Single Shot MultiBox Detector</dc:title>
        <dcterms:abstract>We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .</dcterms:abstract>
        <dc:date>2016</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SSD</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1512.02325</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-13 10:11:37</dcterms:dateSubmitted>
        <dc:description>arXiv: 1512.02325</dc:description>
        <bib:pages>21-37</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_240">
       <rdf:value>Comment: ECCV 2016</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_238">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/238/Liu 等。 - 2016 - SSD Single Shot MultiBox Detector.pdf"/>
        <dc:title>Liu 等。 - 2016 - SSD Single Shot MultiBox Detector.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1512.02325v5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-13 10:10:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1704.04503">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1704.04503 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bodla</foaf:surname>
                        <foaf:givenName>Navaneeth</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Bharat</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chellappa</foaf:surname>
                        <foaf:givenName>Rama</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davis</foaf:surname>
                        <foaf:givenName>Larry S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_243"/>
        <link:link rdf:resource="#item_241"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Soft-NMS -- Improving Object Detection With One Line of Code</dc:title>
        <dcterms:abstract>Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a signiﬁcant overlap (using a pre-deﬁned threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predeﬁned overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7% for both RFCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8% to 40.9% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efﬁciently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for SoftNMS is publicly available on GitHub http://bit.ly/ 2nJLNMu.</dcterms:abstract>
        <dc:date>2017-08-08</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1704.04503</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-16 07:55:16</dcterms:dateSubmitted>
        <dc:description>arXiv: 1704.04503</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_243">
       <rdf:value>Comment: ICCV 2017 camera ready version</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_241">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/241/Bodla 等。 - 2017 - Soft-NMS -- Improving Object Detection With One Li.pdf"/>
        <dc:title>Bodla 等。 - 2017 - Soft-NMS -- Improving Object Detection With One Li.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1704.04503.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-16 07:55:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1911.00068">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1911.00068 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Northcutt</foaf:surname>
                        <foaf:givenName>Curtis G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Lu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chuang</foaf:surname>
                        <foaf:givenName>Isaac L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_246"/>
        <link:link rdf:resource="#item_244"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Confident Learning: Estimating Uncertainty in Dataset Labels</dc:title>
        <dcterms:abstract>Learning exists in the context of data, yet notions of conﬁdence typically focus on model predictions, not label quality. Conﬁdent learning (CL) has emerged as an approach for characterizing, identifying, and learning with noisy labels in datasets, based on the principles of pruning noisy data, counting to estimate noise, and ranking examples to train with conﬁdence. Here, we generalize CL, building on the assumption of a classiﬁcation noise process, to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This generalized CL, open-sourced as cleanlab, is provably consistent across reasonable conditions, and experimentally performant on ImageNet and CIFAR, outperforming seven recent approaches when label noise is non-uniform. cleanlab also quantiﬁes ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training.</dcterms:abstract>
        <dc:date>2020-02-21</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Confident Learning</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.00068</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-17 01:53:26</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.00068</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_246">
        <rdf:value>Comment: Under Review by International Conference of Machine Learning (ICML)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_244">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/244/Northcutt 等。 - 2020 - Confident Learning Estimating Uncertainty in Data.pdf"/>
        <dc:title>Northcutt 等。 - 2020 - Confident Learning Estimating Uncertainty in Data.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1911.00068v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-17 01:53:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Data rdf:about="https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise">
        <z:itemType>computerProgram</z:itemType>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vasu</foaf:surname>
                        <foaf:givenName>Subeesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>deep-neural-networks</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>label-noise</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>noisy-data</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>noisy-labels</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>robust-learning</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>unreliable-labels</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>subeeshvasu/Awesome-Learning-with-Label-Noise</dc:title>
        <dcterms:abstract>A curated list of resources for Learning with Noisy Labels</dcterms:abstract>
        <dc:date>2020-08-15T10:25:52Z</dc:date>
        <z:libraryCatalog>GitHub</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-17 01:54:32</dcterms:dateSubmitted>
        <dc:description>original-date: 2019-07-17T11:02:52Z</dc:description>
    </bib:Data>
    <z:Attachment rdf:about="#item_248">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/248/GetLicenseforStandalone201904.pdf"/>
        <dc:title>GetLicenseforStandalone201904.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://webvpn.tsinghua.edu.cn/http/77726476706e69737468656265737421a1a117d27661391e2b46d1/software/matlab/GetLicenseforStandalone201904.pdf?st=ityQSBvrKtfDXHEWtJ5gOg&amp;e=1597828911&amp;filename=Active_office2010.exe</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-19 09:40:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/8099957/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.474</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Shanshan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benenson</foaf:surname>
                        <foaf:givenName>Rodrigo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schiele</foaf:surname>
                        <foaf:givenName>Bernt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_249"/>
        <dc:title>CityPersons: A Diverse Dataset for Pedestrian Detection</dc:title>
        <dcterms:abstract>Convnets have enabled signiﬁcant progress in pedestrian detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain FasterRCNN to obtain state-of-the-art results on the Caltech dataset.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CityPersons</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8099957/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-20 09:08:38</dcterms:dateSubmitted>
        <bib:pages>4457-4465</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_249">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/249/zhang2017.pdf"/>
        <dc:title>zhang2017.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_251">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/251/Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf"/>
        <dc:title>Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.crcv.ucf.edu/wp-content/uploads/2020/07/Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-21 02:46:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_252">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/252/Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf"/>
        <dc:title>Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.crcv.ucf.edu/wp-content/uploads/2020/07/Publications_Simultaneous-Detection-and-Tracking-with-Motion-Modelling-for-Multiple-Object-Tracking.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-22 11:18:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1912.02973">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1912.02973 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Albert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>Yitao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Haibin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Broeck</foaf:surname>
                        <foaf:givenName>Guy Van den</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soatto</foaf:surname>
                        <foaf:givenName>Stefano</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_253"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>LaTeS: Latent Space Distillation for Teacher-Student Driving Policy Learning</dc:title>
        <dcterms:abstract>We describe a policy learning approach to map visual inputs to driving controls that leverages side information on semantics and affordances of objects in the scene from a secondary teacher model. While the teacher receives semantic segmentation and stop “intention” values as inputs and produces an estimate of the driving controls, the primary student model only receives images as inputs, and attempts to imitate the controls while being biased towards the latent representation of the teacher model. The latent representation encodes task-relevant information in the inputs of the teacher model, which are semantic segmentation of the image, and intention values for driving controls in the presence of objects in the scene such as vehicles, pedestrians and trafﬁc lights. Our student model does not attempt to infer semantic segmentation or intention values from its inputs, nor to mimic the output behavior of the teacher. It instead attempts to capture the representation of the teacher inputs that are relevant for driving. Our training does not require laborious annotations such as maps or objects in three dimensions; even the teacher model just requires twodimensional segmentation and intention values. Moreover, our model runs in real time of 59 FPS. We test our approach on recent simulated and real-world driving datasets, and introduce a more challenging but realistic evaluation protocol that considers a run that reaches the destination successful only if it does not violate common trafﬁc rules.</dcterms:abstract>
        <dc:date>2019-12-05</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>LaTeS</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1912.02973</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 01:19:55</dcterms:dateSubmitted>
        <dc:description>arXiv: 1912.02973</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_253">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/253/Zhao 等。 - 2019 - LaTeS Latent Space Distillation for Teacher-Stude.pdf"/>
        <dc:title>Zhao 等。 - 2019 - LaTeS Latent Space Distillation for Teacher-Stude.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1912.02973.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 01:19:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1911.04252">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1911.04252 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Qizhe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luong</foaf:surname>
                        <foaf:givenName>Minh-Thang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hovy</foaf:surname>
                        <foaf:givenName>Eduard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_257"/>
        <link:link rdf:resource="#item_255"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Self-training with Noisy Student improves ImageNet classification</dc:title>
        <dcterms:abstract>We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean ﬂip rate from 27.8 to 12.2.</dcterms:abstract>
        <dc:date>2020-06-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.04252</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 02:11:46</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.04252</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_257">
       <rdf:value>Comment: CVPR 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_255">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/255/Xie 等。 - 2020 - Self-training with Noisy Student improves ImageNet.pdf"/>
        <dc:title>Xie 等。 - 2020 - Self-training with Noisy Student improves ImageNet.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1911.04252v4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 02:11:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72813-293-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00087</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>RoyChowdhury</foaf:surname>
                        <foaf:givenName>Aruni</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chakrabarty</foaf:surname>
                        <foaf:givenName>Prithvijit</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Ashish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>SouYoung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Huaizu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liangliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Learned-Miller</foaf:surname>
                        <foaf:givenName>Erik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_258"/>
        <dc:title>Automatic Adaptation of Object Detectors to New Domains Using Self-Training</dc:title>
        <dcterms:abstract>This work addresses the unsupervised adaptation of an existing object detector to a new target domain. We assume that a large number of unlabeled videos from this domain are readily available. We automatically obtain labels on the target data by using high-conﬁdence detections from the existing detector, augmented with hard (misclassiﬁed) examples acquired by exploiting temporal cues using a tracker. These automatically-obtained labels are then used for re-training the original model. A modiﬁed knowledge distillation loss is proposed, and we investigate several ways of assigning soft-labels to the training examples from the target domain. Our approach is empirically evaluated on challenging face and pedestrian detection tasks: a face detector trained on WIDER-Face, which consists of highquality images crawled from the web, is adapted to a largescale surveillance data set; a pedestrian detector trained on clear, daytime images from the BDD-100K driving data set is adapted to all other scenarios such as rainy, foggy, nighttime. Our results demonstrate the usefulness of incorporating hard examples obtained from tracking, the advantage of using soft-labels via distillation loss versus hard-labels, and show promising performance as a simple method for unsupervised domain adaptation of object detectors, with minimal dependence on hyper-parameters.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953534/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 02:19:08</dcterms:dateSubmitted>
        <bib:pages>780-790</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_258">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/258/RoyChowdhury 等。 - 2019 - Automatic Adaptation of Object Detectors to New Do.pdf"/>
        <dc:title>RoyChowdhury 等。 - 2019 - Automatic Adaptation of Object Detectors to New Do.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/RoyChowdhury_Automatic_Adaptation_of_Object_Detectors_to_New_Domains_Using_Self-Training_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 02:18:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://blog.csdn.net/weixin_42075898/article/details/104535684">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_261"/>
        <dc:title>目标检测论文《Automatic adaptation of object detectors to new domains using self-training》_weixin_42075898的博客-CSDN博客_automatic adaptation of object detectors to new do</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://blog.csdn.net/weixin_42075898/article/details/104535684</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 12:21:09</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_261">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/261/104535684.html"/>
        <dc:title>目标检测论文《Automatic adaptation of object detectors to new domains using self-training》_weixin_42075898的博客-CSDN博客_automatic adaptation of object detectors to new do</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://blog.csdn.net/weixin_42075898/article/details/104535684</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 12:21:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://medium.com/@nainaakash012/self-training-with-noisy-student-f33640edbab2">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>Medium</dc:title></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nain</foaf:surname>
                        <foaf:givenName>Aakash</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Self-training with Noisy Student</dc:title>
        <dcterms:abstract>2019 has been the year full where a lot of research has been focused on designing efficient deep learning models, self-supervised learning…</dcterms:abstract>
        <dc:date>2019-11-17T18:39:50.575Z</dc:date>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://medium.com/@nainaakash012/self-training-with-noisy-student-f33640edbab2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-08-25 12:23:45</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Article rdf:about="#item_264">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1911.00068 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Northcutt</foaf:surname>
                        <foaf:givenName>Curtis G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Lu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chuang</foaf:surname>
                        <foaf:givenName>Isaac L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_265"/>
        <link:link rdf:resource="#item_263"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Confident Learning: Estimating Uncertainty in Dataset Labels</dc:title>
        <dcterms:abstract>Learning exists in the context of data, yet notions of conﬁdence typically focus on model predictions, not label quality. Conﬁdent learning (CL) has emerged as an approach for characterizing, identifying, and learning with noisy labels in datasets, based on the principles of pruning noisy data, counting to estimate noise, and ranking examples to train with conﬁdence. Here, we generalize CL, building on the assumption of a classiﬁcation noise process, to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This generalized CL, open-sourced as cleanlab, is provably consistent across reasonable conditions, and experimentally performant on ImageNet and CIFAR, outperforming seven recent approaches when label noise is non-uniform. cleanlab also quantiﬁes ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training.</dcterms:abstract>
        <dc:date>2020-02-21</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Confident Learning</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.00068</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-01 01:50:26</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.00068</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_265">
        <rdf:value>&lt;p&gt;Comment: Under Review by International Conference of Machine Learning (ICML)&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_263">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/263/Northcutt 等。 - 2020 - Confident Learning Estimating Uncertainty in Data.pdf"/>
        <dc:title>Northcutt 等。 - 2020 - Confident Learning Estimating Uncertainty in Data.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1911.00068v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-01 01:50:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2005.04757">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2005.04757 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sohn</foaf:surname>
                        <foaf:givenName>Kihyuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zizhao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Chun-Liang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Han</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Chen-Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pfister</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_266"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A Simple Semi-Supervised Learning Framework for Object Detection</dc:title>
        <dcterms:abstract>Semi-supervised learning (SSL) has promising potential for improving the predictive performance of machine learning models using unlabeled data. There has been remarkable progress, but the scope of demonstration in SSL has been limited to image classiﬁcation tasks. In this paper, we propose STAC, a simple yet eﬀective SSL framework for visual object detection along with a data augmentation strategy. STAC deploys highly conﬁdent pseudo labels of localized objects from an unlabeled image and updates the model by enforcing consistency via strong augmentations. We propose new experimental protocols to evaluate performance of semi-supervised object detection using MSCOCO and demonstrate the eﬃcacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the AP0.5 from 76.30 to 79.08; on MS-COCO, STAC demonstrates 2× higher data eﬃciency by achieving 24.38 mAP using only 5% labeled data than supervised baseline that marks 23.86% using 10% labeled data. The code is available at https://github.com/google-research/ssl_detection/.</dcterms:abstract>
        <dc:date>2020-05-10</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.04757</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-01 02:14:06</dcterms:dateSubmitted>
        <dc:description>arXiv: 2005.04757</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_266">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/266/Sohn 等。 - 2020 - A Simple Semi-Supervised Learning Framework for Ob.pdf"/>
        <dc:title>Sohn 等。 - 2020 - A Simple Semi-Supervised Learning Framework for Ob.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2005.04757.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-01 02:14:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1708.04552">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1708.04552 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>DeVries</foaf:surname>
                        <foaf:givenName>Terrance</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taylor</foaf:surname>
                        <foaf:givenName>Graham W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_268"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Improved Regularization of Convolutional Neural Networks with Cutout</dc:title>
        <dcterms:abstract>Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overﬁtting and therefore require proper regularization in order to generalize well.</dcterms:abstract>
        <dc:date>2017-11-29</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1708.04552</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 01:27:28</dcterms:dateSubmitted>
        <dc:description>arXiv: 1708.04552</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_268">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/268/1708.pdf"/>
        <dc:title>1708.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1708.04552v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 01:27:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2002.08709">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2002.08709 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ishida</foaf:surname>
                        <foaf:givenName>Takashi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yamane</foaf:surname>
                        <foaf:givenName>Ikko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sakai</foaf:surname>
                        <foaf:givenName>Tomoya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Niu</foaf:surname>
                        <foaf:givenName>Gang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sugiyama</foaf:surname>
                        <foaf:givenName>Masashi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_270"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Do We Need Zero Training Loss After Achieving Zero Training Error?</dc:title>
        <dcterms:abstract>Overparameterized deep networks have the capacity to memorize training data with zero training error. Even a er memorization, the training loss continues to approach zero, making the model overcon dent and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, they o en fail to maintain a moderate level of training loss, ending up with a too small or too large loss. We propose a direct solution called ooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the ooding level. Our approach makes the loss oat around the ooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the ooding level. is can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. With ooding, the model will continue to “random walk” with the same non-zero training loss, and we expect it to dri into an area with a at loss landscape that leads to be er generalization. We experimentally show that ooding improves performance and as a byproduct, induces a double descent curve of the test loss.</dcterms:abstract>
        <dc:date>2020-02-20</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2002.08709</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 02:21:25</dcterms:dateSubmitted>
        <dc:description>arXiv: 2002.08709</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_270">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/270/Ishida 等。 - 2020 - Do We Need Zero Training Loss After Achieving Zero.pdf"/>
        <dc:title>Ishida 等。 - 2020 - Do We Need Zero Training Loss After Achieving Zero.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2002.08709.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 02:21:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s11263-009-0275-4">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>88</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-009-0275-4</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Everingham</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Gool</foaf:surname>
                        <foaf:givenName>Luc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Christopher K. I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winn</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_272"/>
        <dc:title>The Pascal Visual Object Classes (VOC) Challenge</dc:title>
        <dcterms:abstract>The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.</dcterms:abstract>
        <dc:date>6/2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-009-0275-4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 03:17:25</dcterms:dateSubmitted>
        <bib:pages>303-338</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_272">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/272/ijcv_voc09.pdf"/>
        <dc:title>ijcv_voc09.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 03:17:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_275">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>88</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-009-0275-4</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Everingham</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Gool</foaf:surname>
                        <foaf:givenName>Luc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Christopher K. I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winn</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_274"/>
        <dc:title>The Pascal Visual Object Classes (VOC) Challenge</dc:title>
        <dcterms:abstract>The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.</dcterms:abstract>
        <dc:date>6/2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-009-0275-4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 03:18:08</dcterms:dateSubmitted>
        <bib:pages>303-338</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_274">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/274/Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf"/>
        <dc:title>Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 03:18:05</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1805.09501">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1805.09501 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cubuk</foaf:surname>
                        <foaf:givenName>Ekin D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zoph</foaf:surname>
                        <foaf:givenName>Barret</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mane</foaf:surname>
                        <foaf:givenName>Dandelion</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vasudevan</foaf:surname>
                        <foaf:givenName>Vijay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_278"/>
        <link:link rdf:resource="#item_276"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>AutoAugment: Learning Augmentation Policies from Data</dc:title>
        <dcterms:abstract>Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on ImageNet transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.</dcterms:abstract>
        <dc:date>2019-04-11</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AutoAugment</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1805.09501</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 04:02:21</dcterms:dateSubmitted>
        <dc:description>arXiv: 1805.09501</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_278">
       <rdf:value>Comment: CVPR 2019</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_276">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/276/1805.pdf"/>
        <dc:title>1805.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1805.09501v3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 04:02:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/7485869/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0162-8828,%202160-9292"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Shaoqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_281"/>
        <dc:title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</dc:title>
        <dcterms:abstract>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.</dcterms:abstract>
        <dc:date>2017-6-1</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Faster R-CNN</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7485869/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:23:06</dcterms:dateSubmitted>
        <bib:pages>1137-1149</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0162-8828,%202160-9292">
        <prism:volume>39</prism:volume>
        <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
        <dc:identifier>DOI 10.1109/TPAMI.2016.2577031</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>IEEE Trans. Pattern Anal. Mach. Intell.</dcterms:alternative>
        <dc:identifier>ISSN 0162-8828, 2160-9292</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_281">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/281/Ren 等。 - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf"/>
        <dc:title>Ren 等。 - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:23:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_284">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>88</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-009-0275-4</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Everingham</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Gool</foaf:surname>
                        <foaf:givenName>Luc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Christopher K. I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winn</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_283"/>
        <dc:title>The Pascal Visual Object Classes (VOC) Challenge</dc:title>
        <dcterms:abstract>The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.</dcterms:abstract>
        <dc:date>6/2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-009-0275-4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:30:31</dcterms:dateSubmitted>
        <bib:pages>303-338</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_283">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/283/Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf"/>
        <dc:title>Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:30:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_286">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>88</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-009-0275-4</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Everingham</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Gool</foaf:surname>
                        <foaf:givenName>Luc</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Christopher K. I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winn</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_285"/>
        <dc:title>The Pascal Visual Object Classes (VOC) Challenge</dc:title>
        <dcterms:abstract>The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.</dcterms:abstract>
        <dc:date>6/2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-009-0275-4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:31:04</dcterms:dateSubmitted>
        <bib:pages>303-338</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_285">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/285/Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf"/>
        <dc:title>Everingham 等。 - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-03 08:30:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1902.07296">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1902.07296 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kisantal</foaf:surname>
                        <foaf:givenName>Mate</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojna</foaf:surname>
                        <foaf:givenName>Zbigniew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murawski</foaf:surname>
                        <foaf:givenName>Jakub</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naruniec</foaf:surname>
                        <foaf:givenName>Jacek</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>Kyunghyun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_287"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Augmentation for small object detection</dc:title>
        <dcterms:abstract>In the recent years, object detection has experienced impressive progress. Despite these improvements, there is still a signiﬁcant gap in the performance between the detection of small and large objects. We analyze the current state-of-the-art model, Mask-RCNN, on a challenging dataset, MS COCO. We show that the overlap between small ground-truth objects and the predicted anchors is much lower than the expected IoU threshold. We conjecture this is due to two factors; (1) only a few images are containing small objects, and (2) small objects do not appear enough even within each image containing them. We thus propose to oversample those images with small objects and augment each of those images by copy-pasting small objects many times. It allows us to trade oﬀ the quality of the detector on large objects with that on small objects. We evaluate diﬀerent pasting augmentation strategies, and ultimately, we achieve 9.7% relative improvement on the instance segmentation and 7.1% on the object detection of small objects, compared to the current state of the art method on MS COCO.</dcterms:abstract>
        <dc:date>2019-02-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1902.07296</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-07 10:03:41</dcterms:dateSubmitted>
        <dc:description>arXiv: 1902.07296</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_287">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/287/Kisantal 等。 - 2019 - Augmentation for small object detection.pdf"/>
        <dc:title>Kisantal 等。 - 2019 - Augmentation for small object detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1902.07296v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-07 10:03:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/8954244/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00094</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Xingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuo</foaf:surname>
                        <foaf:givenName>Jiacheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krahenbuhl</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_289"/>
        <dc:title>Bottom-Up Object Detection by Grouping Extreme and Center Points</dc:title>
        <dcterms:abstract>With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the ﬁve keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classiﬁcation or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.7% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8954244/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-08 09:30:42</dcterms:dateSubmitted>
        <bib:pages>850-859</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_289">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/289/Zhou 等。 - 2019 - Bottom-Up Object Detection by Grouping Extreme and.pdf"/>
        <dc:title>Zhou 等。 - 2019 - Bottom-Up Object Detection by Grouping Extreme and.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Bottom-Up_Object_Detection_by_Grouping_Extreme_and_Center_Points_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-08 09:30:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-3-319-10601-4%20978-3-319-10602-1">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>8693</prism:volume>
                <dc:identifier>ISBN 978-3-319-10601-4 978-3-319-10602-1</dc:identifier>
                <dc:title>Computer Vision – ECCV 2014</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fleet</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pajdla</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schiele</foaf:surname>
                        <foaf:givenName>Bernt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tuytelaars</foaf:surname>
                        <foaf:givenName>Tinne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maire</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belongie</foaf:surname>
                        <foaf:givenName>Serge</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hays</foaf:surname>
                        <foaf:givenName>James</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Perona</foaf:surname>
                        <foaf:givenName>Pietro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramanan</foaf:surname>
                        <foaf:givenName>Deva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dollár</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zitnick</foaf:surname>
                        <foaf:givenName>C. Lawrence</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_291"/>
        <dc:title>Microsoft COCO: Common Objects in Context</dc:title>
        <dcterms:abstract>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</dcterms:abstract>
        <dc:date>2014</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Microsoft COCO</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-319-10602-1_48</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-16 00:36:48</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-319-10602-1_48</dc:description>
        <bib:pages>740-755</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_291">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/291/Lin 等。 - 2014 - Microsoft COCO Common Objects in Context.pdf"/>
        <dc:title>Lin 等。 - 2014 - Microsoft COCO Common Objects in Context.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1405.0312v3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-16 00:36:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_294">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbu</foaf:surname>
                        <foaf:givenName>Andrei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mayo</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alverio</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gutfreund</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tenenbaum</foaf:surname>
                        <foaf:givenName>Josh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_293"/>
        <dc:title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</dc:title>
        <dcterms:abstract>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>11</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_293">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/293/Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf"/>
        <dc:title>Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-20 09:47:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_296">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbu</foaf:surname>
                        <foaf:givenName>Andrei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mayo</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alverio</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gutfreund</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tenenbaum</foaf:surname>
                        <foaf:givenName>Josh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_295"/>
        <dc:title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</dc:title>
        <dcterms:abstract>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>11</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_295">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/295/Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf"/>
        <dc:title>Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-21 01:16:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_298">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbu</foaf:surname>
                        <foaf:givenName>Andrei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mayo</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alverio</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gutfreund</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tenenbaum</foaf:surname>
                        <foaf:givenName>Josh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_297"/>
        <dc:title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</dc:title>
        <dcterms:abstract>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>11</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_297">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/297/Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf"/>
        <dc:title>Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-22 01:26:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_300">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1902.07296 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kisantal</foaf:surname>
                        <foaf:givenName>Mate</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojna</foaf:surname>
                        <foaf:givenName>Zbigniew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murawski</foaf:surname>
                        <foaf:givenName>Jakub</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naruniec</foaf:surname>
                        <foaf:givenName>Jacek</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>Kyunghyun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_299"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Augmentation for small object detection</dc:title>
        <dcterms:abstract>In the recent years, object detection has experienced impressive progress. Despite these improvements, there is still a signiﬁcant gap in the performance between the detection of small and large objects. We analyze the current state-of-the-art model, Mask-RCNN, on a challenging dataset, MS COCO. We show that the overlap between small ground-truth objects and the predicted anchors is much lower than the expected IoU threshold. We conjecture this is due to two factors; (1) only a few images are containing small objects, and (2) small objects do not appear enough even within each image containing them. We thus propose to oversample those images with small objects and augment each of those images by copy-pasting small objects many times. It allows us to trade oﬀ the quality of the detector on large objects with that on small objects. We evaluate diﬀerent pasting augmentation strategies, and ultimately, we achieve 9.7% relative improvement on the instance segmentation and 7.1% on the object detection of small objects, compared to the current state of the art method on MS COCO.</dcterms:abstract>
        <dc:date>2019-02-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1902.07296</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-22 09:30:44</dcterms:dateSubmitted>
        <dc:description>arXiv: 1902.07296</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_299">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/299/Kisantal 等。 - 2019 - Augmentation for small object detection.pdf"/>
        <dc:title>Kisantal 等。 - 2019 - Augmentation for small object detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1902.07296.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-22 09:30:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_302">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Fei-Fei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_301"/>
        <dc:title>crowdsourcing, benchmarking &amp; other cool things</dc:title>
        <dc:date>2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>64</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_301">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/301/Li - 2010 - crowdsourcing, benchmarking &amp; other cool things.pdf"/>
        <dc:title>Li - 2010 - crowdsourcing, benchmarking &amp; other cool things.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.image-net.org/papers/ImageNet_2010.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-23 08:29:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_304">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Everingham</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winn</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_303"/>
        <dc:title>The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>32</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_303">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/303/Everingham 和 Winn - The PASCAL Visual Object Classes Challenge 2012 (V.pdf"/>
        <dc:title>Everingham 和 Winn - The PASCAL Visual Object Classes Challenge 2012 (V.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-23 10:32:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_306">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbu</foaf:surname>
                        <foaf:givenName>Andrei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mayo</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alverio</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gutfreund</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tenenbaum</foaf:surname>
                        <foaf:givenName>Josh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_305"/>
        <dc:title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</dc:title>
        <dcterms:abstract>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>11</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_305">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/305/Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf"/>
        <dc:title>Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1811.00982">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>128</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-020-01316-z</dc:identifier>
                <prism:number>7</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kuznetsova</foaf:surname>
                        <foaf:givenName>Alina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rom</foaf:surname>
                        <foaf:givenName>Hassan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alldrin</foaf:surname>
                        <foaf:givenName>Neil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uijlings</foaf:surname>
                        <foaf:givenName>Jasper</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krasin</foaf:surname>
                        <foaf:givenName>Ivan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pont-Tuset</foaf:surname>
                        <foaf:givenName>Jordi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kamali</foaf:surname>
                        <foaf:givenName>Shahab</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Popov</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malloci</foaf:surname>
                        <foaf:givenName>Matteo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kolesnikov</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duerig</foaf:surname>
                        <foaf:givenName>Tom</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>Vittorio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_309"/>
        <link:link rdf:resource="#item_307"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</dc:title>
        <dcterms:abstract>We present Open Images V4, a dataset of 9.2M images with uniﬁed annotations for image classiﬁcation, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predeﬁned list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15× more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having uniﬁed annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classiﬁcation, object detection, and visual relationship detection.</dcterms:abstract>
        <dc:date>7/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The Open Images Dataset V4</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1811.00982</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-24 07:53:12</dcterms:dateSubmitted>
        <dc:description>arXiv: 1811.00982</dc:description>
        <bib:pages>1956-1981</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_309">
        <rdf:value>Comment: Accepted to International Journal of Computer Vision, 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_307">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/307/Kuznetsova 等。 - 2020 - The Open Images Dataset V4 Unified image classifi.pdf"/>
        <dc:title>Kuznetsova 等。 - 2020 - The Open Images Dataset V4 Unified image classifi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1811.00982v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-24 07:52:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1912.05170">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1912.05170 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Algan</foaf:surname>
                        <foaf:givenName>Görkem</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ulusoy</foaf:surname>
                        <foaf:givenName>Ilkay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_310"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey</dc:title>
        <dcterms:abstract>Image classiﬁcation systems recently made a big leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data in order to be trained properly. This is not always feasible due to several factors, such as expensiveness of labeling process or difﬁculty of correctly classifying data even for the experts. Because of these practical challenges, label noise is a common problem in datasets and numerous methods to train deep networks with label noise are proposed in the literature. Although deep networks are known to be relatively robust to label noise, their tendency to overﬁt data makes them vulnerable to memorizing even total random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its negative effects to train deep neural networks efﬁciently. Even though an extensive survey of machine learning techniques under label noise exists, literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the ﬁrst group aim to estimate the structure of the noise and use this information to avoid the negative effects of noisy labels during training. On the other hand, methods in the second group try to come up with algorithms that are inherently noise robust by using approaches like robust losses, regularizers or other learning paradigms.</dcterms:abstract>
        <dc:date>2020-06-05</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Image Classification with Deep Learning in the Presence of Noisy Labels</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1912.05170</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:11:17</dcterms:dateSubmitted>
        <dc:description>arXiv: 1912.05170</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_310">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/310/Algan 和 Ulusoy - 2020 - Image Classification with Deep Learning in the Pre.pdf"/>
        <dc:title>Algan 和 Ulusoy - 2020 - Image Classification with Deep Learning in the Pre.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1912.05170v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:11:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2007.08199">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2007.08199 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Hwanjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Minseok</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Dongmin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Jae-Gil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_314"/>
        <link:link rdf:resource="#item_312"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning from Noisy Labels with Deep Neural Networks: A Survey</dc:title>
        <dcterms:abstract>Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we ﬁrst describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 47 state-of-the-art robust training methods, all of which are categorized into seven groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.</dcterms:abstract>
        <dc:date>2020-07-21</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Learning from Noisy Labels with Deep Neural Networks</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2007.08199</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:15:03</dcterms:dateSubmitted>
        <dc:description>arXiv: 2007.08199</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_314">
        <rdf:value>Comment: If your paper is highly related, but it is missing, please contact me: songhwanjun@kaist.ac.kr</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_312">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/312/Song 等。 - 2020 - Learning from Noisy Labels with Deep Neural Networ.pdf"/>
        <dc:title>Song 等。 - 2020 - Learning from Noisy Labels with Deep Neural Networ.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2007.08199v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:14:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1406.2080">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1406.2080 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sukhbaatar</foaf:surname>
                        <foaf:givenName>Sainbayar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bruna</foaf:surname>
                        <foaf:givenName>Joan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paluri</foaf:surname>
                        <foaf:givenName>Manohar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bourdev</foaf:surname>
                        <foaf:givenName>Lubomir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fergus</foaf:surname>
                        <foaf:givenName>Rob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_317"/>
        <link:link rdf:resource="#item_315"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Training Convolutional Networks with Noisy Labels</dc:title>
        <dcterms:abstract>The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modiﬁcations to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classiﬁcation benchmark.</dcterms:abstract>
        <dc:date>2015-04-10</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1406.2080</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:16:42</dcterms:dateSubmitted>
        <dc:description>arXiv: 1406.2080</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_317">
        <rdf:value>Comment: Accepted as a workshop contribution at ICLR 2015</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_315">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/315/Sukhbaatar 等。 - 2015 - Training Convolutional Networks with Noisy Labels.pdf"/>
        <dc:title>Sukhbaatar 等。 - 2015 - Training Convolutional Networks with Noisy Labels.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1406.2080</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:16:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_319">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2007.08199 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Hwanjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Minseok</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Dongmin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Jae-Gil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_320"/>
        <link:link rdf:resource="#item_318"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Learning from Noisy Labels with Deep Neural Networks: A Survey</dc:title>
        <dcterms:abstract>Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we ﬁrst describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 47 state-of-the-art robust training methods, all of which are categorized into seven groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.</dcterms:abstract>
        <dc:date>2020-07-21</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Learning from Noisy Labels with Deep Neural Networks</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2007.08199</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:18:48</dcterms:dateSubmitted>
        <dc:description>arXiv: 2007.08199</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_320">
       <rdf:value>&lt;p&gt;只关注分类&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_318">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/318/2007.pdf"/>
        <dc:title>2007.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2007.08199v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 02:18:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/8100179/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.696</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Veit</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alldrin</foaf:surname>
                        <foaf:givenName>Neil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chechik</foaf:surname>
                        <foaf:givenName>Gal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krasin</foaf:surname>
                        <foaf:givenName>Ivan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belongie</foaf:surname>
                        <foaf:givenName>Serge</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_323"/>
        <link:link rdf:resource="#item_321"/>
        <dc:title>Learning from Noisy Large-Scale Datasets with Minimal Supervision</dc:title>
        <dcterms:abstract>We present an approach to effectively use millions of images with noisy annotations in conjunction with a small subset of cleanly-annotated images to learn powerful image representations. One common approach to combine clean and noisy data is to ﬁrst pre-train a network using the large noisy dataset and then ﬁne-tune with the clean dataset. We show this approach does not fully leverage the information contained in the clean set. Thus, we demonstrate how to use the clean annotations to reduce the noise in the large dataset before ﬁne-tuning the network using both the clean set and the full set with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy annotations and to accurately classify images. We evaluate our approach on the recently released Open Images dataset, containing ∼9 million images, multiple annotations per image and over 6000 unique classes. For the small clean set of annotations we use a quarter of the validation set with ∼40k images. Our results demonstrate that the proposed approach clearly outperforms direct ﬁne-tuning across all major categories of classes in the Open Image dataset. Further, our approach is particularly effective for a large number of classes with wide range of noise in annotations (2080% false positive annotations).</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8100179/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 07:40:53</dcterms:dateSubmitted>
        <bib:pages>6575-6583</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_323">
        <rdf:value>&lt;p&gt;数据集：open image。只关注分类&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_321">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/321/Veit 等。 - 2017 - Learning from Noisy Large-Scale Datasets with Mini.pdf"/>
        <dc:title>Veit 等。 - 2017 - Learning from Noisy Large-Scale Datasets with Mini.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cornell.edu/se3/wp-content/uploads/2017/04/DeepLabelCleaning_CVPR.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 07:40:43</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72817-168-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01134</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Xiaopeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jianzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Mingliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_325"/>
        <dc:title>Noise-Aware Fully Webly Supervised Object Detection</dc:title>
        <dcterms:abstract>We investigate the emerging task of learning object detectors with sole image-level labels on the web without requiring any other supervision like precise annotations or additional images from well-annotated benchmark datasets. Such a task, termed as fully webly supervised object detection, is extremely challenging, since image-level labels on the web are always noisy, leading to poor performance of the learned detectors. In this work, we propose an end-toend framework to jointly learn webly supervised detectors and reduce the negative impact of noisy labels. Such noise is heterogeneous, which is further categorized into two types, namely background noise and foreground noise. Regarding the background noise, we propose a residual learning structure incorporated with weakly supervised detection, which decomposes background noise and models clean data. To explicitly learn the residual feature between clean data and noisy labels, we further propose a spatially-sensitive entropy criterion, which exploits the conditional distribution of detection results to estimate the conﬁdence of background categories being noise. Regarding the foreground noise, a bagging-mixup learning is introduced, which suppresses foreground noisy signals from incorrectly labelled images, whilst maintaining the diversity of training data. We evaluate the proposed approach on popular benchmark datasets by training detectors on web images, which are retrieved by the corresponding category tags from photo-sharing sites. Extensive experiments show that our method achieves signiﬁcant improvements over the state-of-the-art methods 1.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9156477/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:32:12</dcterms:dateSubmitted>
        <bib:pages>11323-11332</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_325">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/325/Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf"/>
        <dc:title>Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Noise-Aware_Fully_Webly_Supervised_Object_Detection_CVPR_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:32:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9156477/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01134</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Xiaopeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jianzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Mingliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_328"/>
        <dc:title>Noise-Aware Fully Webly Supervised Object Detection</dc:title>
        <dcterms:abstract>We investigate the emerging task of learning object detectors with sole image-level labels on the web without requiring any other supervision like precise annotations or additional images from well-annotated benchmark datasets. Such a task, termed as fully webly supervised object detection, is extremely challenging, since image-level labels on the web are always noisy, leading to poor performance of the learned detectors. In this work, we propose an end-toend framework to jointly learn webly supervised detectors and reduce the negative impact of noisy labels. Such noise is heterogeneous, which is further categorized into two types, namely background noise and foreground noise. Regarding the background noise, we propose a residual learning structure incorporated with weakly supervised detection, which decomposes background noise and models clean data. To explicitly learn the residual feature between clean data and noisy labels, we further propose a spatially-sensitive entropy criterion, which exploits the conditional distribution of detection results to estimate the conﬁdence of background categories being noise. Regarding the foreground noise, a bagging-mixup learning is introduced, which suppresses foreground noisy signals from incorrectly labelled images, whilst maintaining the diversity of training data. We evaluate the proposed approach on popular benchmark datasets by training detectors on web images, which are retrieved by the corresponding category tags from photo-sharing sites. Extensive experiments show that our method achieves signiﬁcant improvements over the state-of-the-art methods 1.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9156477/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:33:37</dcterms:dateSubmitted>
        <bib:pages>11323-11332</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_328">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/328/Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf"/>
        <dc:title>Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Noise-Aware_Fully_Webly_Supervised_Object_Detection_CVPR_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:33:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_331">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01134</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Xiaopeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jianzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Mingliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_330"/>
        <dc:title>Noise-Aware Fully Webly Supervised Object Detection</dc:title>
        <dcterms:abstract>We investigate the emerging task of learning object detectors with sole image-level labels on the web without requiring any other supervision like precise annotations or additional images from well-annotated benchmark datasets. Such a task, termed as fully webly supervised object detection, is extremely challenging, since image-level labels on the web are always noisy, leading to poor performance of the learned detectors. In this work, we propose an end-toend framework to jointly learn webly supervised detectors and reduce the negative impact of noisy labels. Such noise is heterogeneous, which is further categorized into two types, namely background noise and foreground noise. Regarding the background noise, we propose a residual learning structure incorporated with weakly supervised detection, which decomposes background noise and models clean data. To explicitly learn the residual feature between clean data and noisy labels, we further propose a spatially-sensitive entropy criterion, which exploits the conditional distribution of detection results to estimate the conﬁdence of background categories being noise. Regarding the foreground noise, a bagging-mixup learning is introduced, which suppresses foreground noisy signals from incorrectly labelled images, whilst maintaining the diversity of training data. We evaluate the proposed approach on popular benchmark datasets by training detectors on web images, which are retrieved by the corresponding category tags from photo-sharing sites. Extensive experiments show that our method achieves signiﬁcant improvements over the state-of-the-art methods 1.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9156477/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:41:44</dcterms:dateSubmitted>
        <bib:pages>11323-11332</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_330">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/330/Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf"/>
        <dc:title>Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Noise-Aware_Fully_Webly_Supervised_Object_Detection_CVPR_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-27 08:41:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://academic.oup.com/nsr/article/5/1/44/4093912">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2095-5138,%202053-714X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Zhi-Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_334"/>
        <dc:title>A brief introduction to weakly supervised learning</dc:title>
        <dcterms:abstract>Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.</dcterms:abstract>
        <dc:date>2018-01-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://academic.oup.com/nsr/article/5/1/44/4093912</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 02:05:22</dcterms:dateSubmitted>
        <bib:pages>44-53</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2095-5138,%202053-714X">
        <prism:volume>5</prism:volume>
        <dc:title>National Science Review</dc:title>
        <dc:identifier>DOI 10.1093/nsr/nwx106</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2095-5138, 2053-714X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_334">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/334/Zhou - 2018 - A brief introduction to weakly supervised learning.pdf"/>
        <dc:title>Zhou - 2018 - A brief introduction to weakly supervised learning.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://watermark.silverchair.com/nwx106.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAApowggKWBgkqhkiG9w0BBwagggKHMIICgwIBADCCAnwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMAx_Wr_daHOVQgGCUAgEQgIICTYKzb5PA14v-dq2vnzSSxDQ0LDIqo4haMBYmZ7sUctny_5Qh0AEK6mQ9Z8EqXAqJflOWu0Mkd-YJEUikgxm9q9s5VLWTL-zcbc2zvuKF7Mgo80IkXBim1A8v6RzMUDKRW1qPg-m3RmEaDkLeALZBGCcJLW6lE4ufsBD2pQQWuuph5UgkAvF4P7KYUSiWopZ2Gu3NcB_XsdG-_MlUNXOHKYLohMEsducgSsbNjwRM33RIMwhPPZcui_i8wUrvHr8FcKCZUoBTm_IQNE0dQOas8Not87DZENyAwhZcCt_dPCnfbteux90ALRsizbAtiWcdGHch4Di3lTFI_3Z5srtMmxqyKcRC0FC6Q4xUrR46Z043BiGvo5az173u6p7Z18jO-eJy720a3YA4QdK723FQL-ixVaZT9eDQm6eJY-0slKJmx6h8MvxLC4Z8E-GvuJzkjKKFwW3gAdI7TTueBjTbHQHUss6FxKMIPa__Jtr5G0YbDazR6eW_yFbNQ6GxVzF_OrIrvZW32feGfQ25fLc_LhvVCy4kUG6nnsDYbfqlEvsVPw0QNr0XeEIdY7A1nOUwqDxpoQYpcaxcmQciZbAUeTo56lTbp1eVlWf4jMW4AUjMHqii4Oz2oYbQquwTRGJDFVjmxerpSRys1EpE-ORSsKSg803gMplAYlqijL5JfT9IQRPC5Z4p0gs8vOna-Yei8ogo_0thOUSuDq_5_eehIJgkyPD0TX1LZCGFCHKydEVNqsj7NkmZWpDihMr6v21cBcFoibRuMnbTct0bBVc</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 02:05:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4673-8851-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-8851-1</dc:identifier>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2016.311</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Las Vegas, NV, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bilen</foaf:surname>
                        <foaf:givenName>Hakan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vedaldi</foaf:surname>
                        <foaf:givenName>Andrea</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_336"/>
        <dc:subject>WSDDN</dc:subject>
        <dc:title>Weakly Supervised Deep Detection Networks</dc:title>
        <dcterms:abstract>Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classiﬁcation tasks. We propose a weakly supervised deep detection architecture that modiﬁes one such network to operate at the level of image regions, performing simultaneously region selection and classiﬁcation. Trained as an image classiﬁer, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and ﬁne-tuning techniques for the task of image-level classiﬁcation as well.</dcterms:abstract>
        <dc:date>6/2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7780680/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 02:17:04</dcterms:dateSubmitted>
        <bib:pages>2846-2854</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_336">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/336/Bilen 和 Vedaldi - 2016 - Weakly Supervised Deep Detection Networks.pdf"/>
        <dc:title>Bilen 和 Vedaldi - 2016 - Weakly Supervised Deep Detection Networks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2016/papers/Bilen_Weakly_Supervised_Deep_CVPR_2016_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 02:17:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4799-5118-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-5118-5</dc:identifier>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2014.49</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Columbus, OH, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arbelaez</foaf:surname>
                        <foaf:givenName>Pablo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pont-Tuset</foaf:surname>
                        <foaf:givenName>Jordi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barron</foaf:surname>
                        <foaf:givenName>Jon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marques</foaf:surname>
                        <foaf:givenName>Ferran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malik</foaf:surname>
                        <foaf:givenName>Jitendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_338"/>
        <dc:title>Multiscale Combinatorial Grouping</dc:title>
        <dcterms:abstract>We propose a uniﬁed approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we ﬁrst develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efﬁciently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.</dcterms:abstract>
        <dc:date>6/2014</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909443</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 03:03:20</dcterms:dateSubmitted>
        <bib:pages>328-335</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_338">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/338/Arbelaez 等。 - 2014 - Multiscale Combinatorial Grouping.pdf"/>
        <dc:title>Arbelaez 等。 - 2014 - Multiscale Combinatorial Grouping.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/resources/MCG_CVPR2014.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 03:03:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1802.09129">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1802.09129 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ge</foaf:surname>
                        <foaf:givenName>Weifeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Sibei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Yizhou</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_342"/>
        <link:link rdf:resource="#item_340"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning</dc:title>
        <dcterms:abstract>Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still signiﬁcantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we ﬁrst obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-speciﬁc deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, ﬁltering and fusing object instances, pixel labeling for the training images, and task-speciﬁc network training. To obtain clean object instances in the training images, we propose a novel algorithm for ﬁltering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to ﬁlter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classiﬁcation as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012.</dcterms:abstract>
        <dc:date>2018-02-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1802.09129</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 07:28:58</dcterms:dateSubmitted>
        <dc:description>arXiv: 1802.09129</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_342">
        <rdf:value>Comment: accepted by IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_340">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/340/Ge 等。 - 2018 - Multi-Evidence Filtering and Fusion for Multi-Labe.pdf"/>
        <dc:title>Ge 等。 - 2018 - Multi-Evidence Filtering and Fusion for Multi-Labe.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1802.09129</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 07:28:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1704.00138">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1704.00138 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Peng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xinggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Xiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Wenyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_345"/>
        <link:link rdf:resource="#item_343"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Multiple Instance Detection Network with Online Instance Classifier Refinement</dc:title>
        <dcterms:abstract>Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classiﬁers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classiﬁer reﬁnement algorithm to integrate MIL and the instance classiﬁer reﬁnement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to reﬁne instance classiﬁer online. The iterative instance classiﬁer reﬁnement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that signiﬁcantly outperforms the previous state-of-the-art.</dcterms:abstract>
        <dc:date>2017-04-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1704.00138</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 08:35:39</dcterms:dateSubmitted>
        <dc:description>arXiv: 1704.00138</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_345">
        <rdf:value>Comment: Accepted by CVPR 2017, IEEE Conference on Computer Vision and Pattern Recognition 2017</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_343">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/343/Tang 等。 - 2017 - Multiple Instance Detection Network with Online In.pdf"/>
        <dc:title>Tang 等。 - 2017 - Multiple Instance Detection Network with Online In.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1704.00138v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 08:35:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9157038/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.00320</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choe</foaf:surname>
                        <foaf:givenName>Junsuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>Seong Joon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Seungho</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chun</foaf:surname>
                        <foaf:givenName>Sanghyuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akata</foaf:surname>
                        <foaf:givenName>Zeynep</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shim</foaf:surname>
                        <foaf:givenName>Hyunjung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_346"/>
        <dc:title>Evaluating Weakly Supervised Object Localization Methods Right</dc:title>
        <dcterms:abstract>Weakly-supervised object localization (WSOL) has gained popularity over the last years for its promise to train localization models with only image-level labels. Since the seminal WSOL work of class activation mapping (CAM), the ﬁeld has focused on how to expand the attention regions to cover objects more broadly and localize them better. However, these strategies rely on full localization supervision to validate hyperparameters and for model selection, which is in principle prohibited under the WSOL setup. In this paper, we argue that WSOL task is ill-posed with only image-level labels, and propose a new evaluation protocol where full supervision is limited to only a small held-out set not overlapping with the test set. We observe that, under our protocol, the ﬁve most recent WSOL methods have not made a major improvement over the CAM baseline. Moreover, we report that existing WSOL methods have not reached the few-shot learning baseline, where the full-supervision at validation time is used for model training instead. Based on our ﬁndings, we discuss some future directions for WSOL. Source code and dataset are available at https://github.com/clovaai/wsolevaluation.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9157038/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 09:10:27</dcterms:dateSubmitted>
        <bib:pages>3130-3139</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_346">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/346/Choe 等。 - 2020 - Evaluating Weakly Supervised Object Localization M.pdf"/>
        <dc:title>Choe 等。 - 2020 - Evaluating Weakly Supervised Object Localization M.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2020/papers/Choe_Evaluating_Weakly_Supervised_Object_Localization_Methods_Right_CVPR_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-28 09:10:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/7780688/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-8851-1</dc:identifier>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2016.319</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Las Vegas, NV, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_348"/>
        <dc:title>Learning Deep Features for Discriminative Localization</dc:title>
        <dcterms:abstract>In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we ﬁnd that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classiﬁcation task1.</dcterms:abstract>
        <dc:date>6/2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7780688/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 01:53:24</dcterms:dateSubmitted>
        <bib:pages>2921-2929</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_348">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/348/Zhou 等。 - 2016 - Learning Deep Features for Discriminative Localiza.pdf"/>
        <dc:title>Zhou 等。 - 2016 - Learning Deep Features for Discriminative Localiza.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 01:53:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_351">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_350"/>
        <dc:title>OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</dc:title>
        <dcterms:abstract>With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>12</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_350">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/350/iclr2015_zhou.pdf"/>
        <dc:title>iclr2015_zhou.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_353">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_352"/>
        <dc:title>OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</dc:title>
        <dcterms:abstract>With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>12</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_352">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/352/Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf"/>
        <dc:title>Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 02:14:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_355">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_354"/>
        <dc:title>OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</dc:title>
        <dcterms:abstract>With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>12</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_354">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/354/Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf"/>
        <dc:title>Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 02:14:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_357">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_356"/>
        <dc:title>OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</dc:title>
        <dcterms:abstract>With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>12</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_356">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/356/Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf"/>
        <dc:title>Zhou 等。 - 2015 - OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 02:15:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5090-0641-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5090-0641-0</dc:identifier>
                <dc:title>2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
                <dc:identifier>DOI 10.1109/WACV.2016.7477688</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Lake Placid, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bazzani</foaf:surname>
                        <foaf:givenName>Loris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergamo</foaf:surname>
                        <foaf:givenName>Alessandra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anguelov</foaf:surname>
                        <foaf:givenName>Dragomir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torresani</foaf:surname>
                        <foaf:givenName>Lorenzo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_358"/>
        <dc:title>Self-taught object localization with deep networks</dc:title>
        <dcterms:abstract>Generating rich manual annotations for an image dataset is a crucial limit of the current state of the art in object localization and detection. This paper introduces self-taught object localization, a novel approach that leverages on deep convolutional networks trained for wholeimage recognition to localize objects in images without additional human supervision, i.e., without using any groundtruth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artiﬁcially graying out different regions of the image. We observe that graying out a region that contains an object typically causes a signiﬁcant drop in recognition. This intuition is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. For a small number of hypotheses, our object localization scheme greatly outperforms prior subwindow proposal methods in terms of both recall and precision. Our experiments on a challenging dataset of 200 classes indicate that our automaticallygenerated hypotheses can be used to train object detectors in a weakly-supervised fashion with recognition results remarkably close to those obtained by training on manually annotated bounding boxes.</dcterms:abstract>
        <dc:date>3/2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7477688/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 02:25:55</dcterms:dateSubmitted>
        <bib:pages>1-9</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_358">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/358/Self-taught_Object_Localization_with_Deep_Networks.pdf"/>
        <dc:title>Self-taught_Object_Localization_with_Deep_Networks.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1710.09412">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1710.09412 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cisse</foaf:surname>
                        <foaf:givenName>Moustapha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dauphin</foaf:surname>
                        <foaf:givenName>Yann N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lopez-Paz</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_362"/>
        <link:link rdf:resource="#item_360"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>mixup: Beyond Empirical Risk Minimization</dc:title>
        <dcterms:abstract>Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also ﬁnd that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.</dcterms:abstract>
        <dc:date>2018-04-27</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>mixup</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1710.09412</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 03:30:55</dcterms:dateSubmitted>
        <dc:description>arXiv: 1710.09412</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_362">
        <rdf:value>Comment: ICLR camera ready version. Changes vs V1: fix repo URL; add ablation studies; add mixup + dropout etc</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_360">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/360/Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf"/>
        <dc:title>Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1710.09412v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-09-29 03:30:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_364">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jeong</foaf:surname>
                        <foaf:givenName>Jisoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Seungeui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Jeesoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kwak</foaf:surname>
                        <foaf:givenName>Nojun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_363"/>
        <dc:title>Consistency-based Semi-supervised learning for Object Detection</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>23</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_363">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/363/Jeong 等。 - Consistency-based Semi-supervised learning for Obj.pdf"/>
        <dc:title>Jeong 等。 - Consistency-based Semi-supervised learning for Obj.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdfs.semanticscholar.org/presentation/0bb4/c93e0b907e3e690711cddba485e880155d40.pdf?_ga=2.200318123.427666413.1602902932-662408763.1597203686</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-17 02:50:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/8099510/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.27</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Papadopoulos</foaf:surname>
                        <foaf:givenName>Dim P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uijlings</foaf:surname>
                        <foaf:givenName>Jasper R. R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keller</foaf:surname>
                        <foaf:givenName>Frank</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>Vittorio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_365"/>
        <dc:title>Training Object Class Detectors with Click Supervision</dc:title>
        <dcterms:abstract>Training object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort; (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes; (3) as the center-click task is very fast, our scheme reduces total annotation time by 9× to 18×.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8099510/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-18 07:24:48</dcterms:dateSubmitted>
        <bib:pages>180-189</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_365">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/365/Papadopoulos_Training_Object_Class_CVPR_2017_paper.pdf"/>
        <dc:title>Papadopoulos_Training_Object_Class_CVPR_2017_paper.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_368">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.27</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Papadopoulos</foaf:surname>
                        <foaf:givenName>Dim P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uijlings</foaf:surname>
                        <foaf:givenName>Jasper R. R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keller</foaf:surname>
                        <foaf:givenName>Frank</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>Vittorio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_367"/>
        <dc:title>Training Object Class Detectors with Click Supervision</dc:title>
        <dcterms:abstract>Training object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort; (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes; (3) as the center-click task is very fast, our scheme reduces total annotation time by 9× to 18×.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8099510/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-18 07:25:01</dcterms:dateSubmitted>
        <bib:pages>180-189</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_367">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/367/Papadopoulos 等。 - 2017 - Training Object Class Detectors with Click Supervi.pdf"/>
        <dc:title>Papadopoulos 等。 - 2017 - Training Object Class Detectors with Click Supervi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2017/papers/Papadopoulos_Training_Object_Class_CVPR_2017_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-18 07:24:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8950115/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2169-3536"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Qinghui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>Xianing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bian</foaf:surname>
                        <foaf:givenName>Shanfeng Bian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_369"/>
        <dc:title>Vehicle-Damage-Detection Segmentation Algorithm Based on Improved Mask RCNN</dc:title>
        <dcterms:abstract>Traffic congestion due to vehicular accidents seriously affects normal travel, and accurate and effective mitigating measures and methods must be studied. To resolve traffic accident compensation problems quickly, a vehicle-damage-detection segmentation algorithm based on transfer learning and an improved mask regional convolutional neural network (Mask RCNN) is proposed in this paper. The experiment first collects car damage pictures for preprocessing and uses Labelme to make data set labels, which are divided into training sets and test sets. The residual network (ResNet) is optimized, and feature extraction is performed in combination with Feature Pyramid Network (FPN). Then, the proportion and threshold of the Anchor in the region proposal network (RPN) are adjusted. The spatial information of the feature map is preserved by bilinear interpolation in ROIAlign, and different weights are introduced in the loss function for different-scale targets. Finally, the results of self-made dedicated dataset training and testing show that the improved Mask RCNN has better Average Precision (AP) value, detection accuracy and masking accuracy, and improves the efficiency of solving traffic accident compensation problems.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8950115/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-23 07:12:14</dcterms:dateSubmitted>
        <bib:pages>6997-7004</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2169-3536">
        <prism:volume>8</prism:volume>
        <dc:title>IEEE Access</dc:title>
        <dc:identifier>DOI 10.1109/ACCESS.2020.2964055</dc:identifier>
        <dcterms:alternative>IEEE Access</dcterms:alternative>
        <dc:identifier>ISSN 2169-3536</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_369">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/369/Vehicle-Damage-Detection_Segmentation_Algorithm_Ba.pdf"/>
        <dc:title>Vehicle-Damage-Detection_Segmentation_Algorithm_Ba.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_372">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>Artiﬁcial Intelligence</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Amores</foaf:surname>
                        <foaf:givenName>Jaume</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_371"/>
        <dc:title>Multiple instance classification: Review, taxonomy and comparative study</dc:title>
        <dcterms:abstract>Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classiﬁcation task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods.</dcterms:abstract>
        <dc:date>2013</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>25</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_371">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/371/amores2013.pdf"/>
        <dc:title>amores2013.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://medium.com/swlh/multiple-instance-learning-c49bd21f5620">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>Medium</dc:title></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheng</foaf:surname>
                        <foaf:givenName>Lori</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_374"/>
        <dc:title>Multiple Instance Learning</dc:title>
        <dcterms:abstract>with MNIST dataset using Pytorch</dcterms:abstract>
        <dc:date>2020-09-05T14:27:17.830Z</dc:date>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://medium.com/swlh/multiple-instance-learning-c49bd21f5620</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 07:42:11</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_374">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/374/multiple-instance-learning-c49bd21f5620.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://medium.com/swlh/multiple-instance-learning-c49bd21f5620</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 07:42:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="#item_375">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Multiple Instance Learning. with MNIST dataset using Pytorch | by Lori Sheng | The Startup | Medium</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://medium.com/swlh/multiple-instance-learning-c49bd21f5620</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 07:42:45</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Document rdf:about="#item_376">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Multiple Instance Learning. with MNIST dataset using Pytorch | by Lori Sheng | The Startup | Medium</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://medium.com/swlh/multiple-instance-learning-c49bd21f5620</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 07:42:56</dcterms:dateSubmitted>
    </bib:Document>
    <bib:BookSection rdf:about="urn:isbn:978-3-030-01251-9%20978-3-030-01252-6">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>11215</prism:volume>
                <dc:identifier>ISBN 978-3-030-01251-9 978-3-030-01252-6</dc:identifier>
                <dc:title>Computer Vision – ECCV 2018</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>Vittorio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hebert</foaf:surname>
                        <foaf:givenName>Martial</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sminchisescu</foaf:surname>
                        <foaf:givenName>Cristian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weiss</foaf:surname>
                        <foaf:givenName>Yair</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>Qingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Jianfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_377"/>
        <dc:title>Zero-Annotation Object Detection with Web Knowledge Transfer</dc:title>
        <dcterms:abstract>Object detection is one of the major problems in computer vision, and has been extensively studied. Most of the existing detection works rely on labor-intensive supervision, such as ground truth bounding boxes of objects or at least image-level annotations. On the contrary, we propose an object detection method that does not require any form of human annotation on target tasks, by exploiting freely available web images. In order to facilitate eﬀective knowledge transfer from web images, we introduce a multi-instance multi-label domain adaption learning framework with two key innovations. First of all, we propose an instance-level adversarial domain adaptation network with attention on foreground objects to transfer the object appearances from web domain to target domain. Second, to preserve the class-speciﬁc semantic structure of transferred object features, we propose a simultaneous transfer mechanism to transfer the supervision across domains through pseudo strong label generation. With our end-to-end framework that simultaneously learns a weakly supervised detector and transfers knowledge across domains, we achieved signiﬁcant improvements over baseline methods on the benchmark datasets.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-030-01252-6_23</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 08:07:02</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-030-01252-6_23</dc:description>
        <bib:pages>387-403</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_377">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/377/Tao 等。 - 2018 - Zero-Annotation Object Detection with Web Knowledg.pdf"/>
        <dc:title>Tao 等。 - 2018 - Zero-Annotation Object Detection with Web Knowledg.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_ECCV_2018/papers/Qingyi_Tao_Zero-Annotation_Object_Detection_ECCV_2018_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 08:06:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/S0004370213000581">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:00043702"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Amores</foaf:surname>
                        <foaf:givenName>Jaume</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_379"/>
        <dc:title>Multiple instance classification: Review, taxonomy and comparative study</dc:title>
        <dcterms:abstract>Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classiﬁcation task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods.</dcterms:abstract>
        <dc:date>08/2013</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Multiple instance classification</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S0004370213000581</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 08:08:24</dcterms:dateSubmitted>
        <bib:pages>81-105</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:00043702">
        <prism:volume>201</prism:volume>
        <dc:title>Artificial Intelligence</dc:title>
        <dc:identifier>DOI 10.1016/j.artint.2013.06.003</dc:identifier>
        <dcterms:alternative>Artificial Intelligence</dcterms:alternative>
        <dc:identifier>ISSN 00043702</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_379">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/379/Amores - 2013 - Multiple instance classification Review, taxonomy.pdf"/>
        <dc:title>Amores - 2013 - Multiple instance classification Review, taxonomy.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8489885/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1520-9210,%201941-0077"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>Qingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Jianfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_380"/>
        <dc:title>Exploiting Web Images for Weakly Supervised Object Detection</dc:title>
        <dcterms:abstract>In recent years, the performance of object detection has advanced signiﬁcantly with the evolution of deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labelling. Object detection without bounding box annotations, i.e, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difﬁcult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labour free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes, especially for the classes that are often considered hard in other works.</dcterms:abstract>
        <dc:date>5/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8489885/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-24 08:08:28</dcterms:dateSubmitted>
        <bib:pages>1135-1146</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1520-9210,%201941-0077">
        <prism:volume>21</prism:volume>
        <dc:title>IEEE Transactions on Multimedia</dc:title>
        <dc:identifier>DOI 10.1109/TMM.2018.2875597</dc:identifier>
        <prism:number>5</prism:number>
        <dcterms:alternative>IEEE Trans. Multimedia</dcterms:alternative>
        <dc:identifier>ISSN 1520-9210, 1941-0077</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_380">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/380/Tao 等。 - 2019 - Exploiting Web Images for Weakly Supervised Object.pdf"/>
        <dc:title>Tao 等。 - 2019 - Exploiting Web Images for Weakly Supervised Object.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2002.01087">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2002.01087 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Chenhao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Siwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Dongqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Wayne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_383"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Object Instance Mining for Weakly Supervised Object Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection (WSOD) using only image-level annotations has attracted growing attention over the past few years. Existing approaches using multiple instance learning easily fall into local optima, because such mechanism tends to learn from the most discriminative object in an image for each category. Therefore, these methods suffer from missing object instances which degrade the performance of WSOD. To address this problem, this paper introduces an end-to-end object instance mining (OIM) framework for weakly supervised object detection. OIM attempts to detect all possible object instances existing in each image by introducing information propagation on the spatial and appearance graphs, without any additional annotations. During the iterative learning process, the less discriminative object instances from the same class can be gradually detected and utilized for training. In addition, we design an object instance reweighted loss to learn larger portion of each object instance to further improve the performance. The experimental results on two publicly available databases, VOC 2007 and 2012, demonstrate the efﬁcacy of proposed approach.</dcterms:abstract>
        <dc:date>2020-02-03</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2002.01087</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-25 07:07:57</dcterms:dateSubmitted>
        <dc:description>arXiv: 2002.01087</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_383">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/383/Lin 等。 - 2020 - Object Instance Mining for Weakly Supervised Objec.pdf"/>
        <dc:title>Lin 等。 - 2020 - Object Instance Mining for Weakly Supervised Objec.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2002.01087.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-10-25 07:07:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4673-6964-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-6964-0</dc:identifier>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2015.7298655</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Boston, MA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Shuran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lichtenberg</foaf:surname>
                        <foaf:givenName>Samuel P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Jianxiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_385"/>
        <dc:title>SUN RGB-D: A RGB-D scene understanding benchmark suite</dc:title>
        <dcterms:abstract>Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.</dcterms:abstract>
        <dc:date>6/2015</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SUN RGB-D</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7298655/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 00:58:43</dcterms:dateSubmitted>
        <bib:pages>567-576</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_385">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/385/Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf"/>
        <dc:title>Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://3dvision.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 00:58:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1612.00593">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1612.00593 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Charles R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mo</foaf:surname>
                        <foaf:givenName>Kaichun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guibas</foaf:surname>
                        <foaf:givenName>Leonidas J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_389"/>
        <link:link rdf:resource="#item_387"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</dc:title>
        <dcterms:abstract>Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.</dcterms:abstract>
        <dc:date>2017-04-10</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PointNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1612.00593</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 03:12:19</dcterms:dateSubmitted>
        <dc:description>arXiv: 1612.00593</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_389">
       <rdf:value>Comment: CVPR 2017</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_387">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/387/Qi 等。 - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf"/>
        <dc:title>Qi 等。 - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1612.00593v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 03:12:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_391">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1612.00593 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Charles R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mo</foaf:surname>
                        <foaf:givenName>Kaichun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guibas</foaf:surname>
                        <foaf:givenName>Leonidas J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_392"/>
        <link:link rdf:resource="#item_390"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</dc:title>
        <dcterms:abstract>Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.</dcterms:abstract>
        <dc:date>2017-04-10</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PointNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1612.00593</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 03:12:56</dcterms:dateSubmitted>
        <dc:description>arXiv: 1612.00593</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_392">
       <rdf:value>Comment: CVPR 2017</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_390">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/390/Qi 等。 - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf"/>
        <dc:title>Qi 等。 - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1612.00593v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-01 03:12:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_394">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Charles R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Chenxia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guibas</foaf:surname>
                        <foaf:givenName>Leonidas J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_393"/>
        <dc:title>Frustum PointNets for 3D Object Detection From RGB-D Data</dc:title>
        <dcterms:abstract>In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efﬁciently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efﬁciency as well as high recall for even small objects. Beneﬁted from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_393">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/393/Qi 等。 - Frustum PointNets for 3D Object Detection From RGB.pdf"/>
        <dc:title>Qi 等。 - Frustum PointNets for 3D Object Detection From RGB.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 01:16:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/7298655/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-6964-0</dc:identifier>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2015.7298655</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Boston, MA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Shuran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lichtenberg</foaf:surname>
                        <foaf:givenName>Samuel P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Jianxiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_395"/>
        <dc:title>SUN RGB-D: A RGB-D scene understanding benchmark suite</dc:title>
        <dcterms:abstract>Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.</dcterms:abstract>
        <dc:date>6/2015</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SUN RGB-D</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7298655/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 01:47:49</dcterms:dateSubmitted>
        <bib:pages>567-576</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_395">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/395/Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf"/>
        <dc:title>Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://3dvision.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 01:47:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_398">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-6964-0</dc:identifier>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2015.7298655</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Boston, MA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Shuran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lichtenberg</foaf:surname>
                        <foaf:givenName>Samuel P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Jianxiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_397"/>
        <dc:title>SUN RGB-D: A RGB-D scene understanding benchmark suite</dc:title>
        <dcterms:abstract>Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.</dcterms:abstract>
        <dc:date>6/2015</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SUN RGB-D</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7298655/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 01:48:10</dcterms:dateSubmitted>
        <bib:pages>567-576</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_397">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/397/Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf"/>
        <dc:title>Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://zhuanlan.zhihu.com/p/39578493">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>知乎专栏</dc:title></z:Website>
        </dcterms:isPartOf>
        <dcterms:isReferencedBy rdf:resource="#item_400"/>
        <link:link rdf:resource="#item_401"/>
        <dc:title>你的机器学习论文严谨吗？青年学者痛批学界“歪风”</dc:title>
        <dcterms:abstract>作者： Zachary C. Lipton，Jacob Steinhardt编译：Bot 编者按：这是一篇即将在ICML 2018研讨会上发表的文章，它的作者是卡内基梅隆大学助理教授Zachary C. Lipton和斯坦福大学研究生Jacob Steinhardt。虽然对于这…</dcterms:abstract>
        <z:language>zh</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://zhuanlan.zhihu.com/p/39578493</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 13:30:46</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Memo rdf:about="#item_400">
        <rdf:value>&lt;blockquote&gt;虽然这些弊端背后的原因尚未可知，但机器学习社区迅速扩张、缺乏论文审查人员、学术成就和短期成功之间的错位奖励（论文引用、关注度和创业机会）等都是可能的诱因。尽管这些弊端都有补&lt;/blockquote&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_401">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/401/39578493.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://zhuanlan.zhihu.com/p/39578493</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-02 13:30:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_402">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/402/深度学习歪风邪气.pdf"/>
        <dc:title>深度学习歪风邪气.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/7477688/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5090-0641-0</dc:identifier>
                <dc:title>2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
                <dc:identifier>DOI 10.1109/WACV.2016.7477688</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Lake Placid, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bazzani</foaf:surname>
                        <foaf:givenName>Loris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergamo</foaf:surname>
                        <foaf:givenName>Alessandra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anguelov</foaf:surname>
                        <foaf:givenName>Dragomir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torresani</foaf:surname>
                        <foaf:givenName>Lorenzo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_403"/>
        <dc:title>Self-taught object localization with deep networks</dc:title>
        <dcterms:abstract>Generating rich manual annotations for an image dataset is a crucial limit of the current state of the art in object localization and detection. This paper introduces self-taught object localization, a novel approach that leverages on deep convolutional networks trained for wholeimage recognition to localize objects in images without additional human supervision, i.e., without using any groundtruth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artiﬁcially graying out different regions of the image. We observe that graying out a region that contains an object typically causes a signiﬁcant drop in recognition. This intuition is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. For a small number of hypotheses, our object localization scheme greatly outperforms prior subwindow proposal methods in terms of both recall and precision. Our experiments on a challenging dataset of 200 classes indicate that our automaticallygenerated hypotheses can be used to train object detectors in a weakly-supervised fashion with recognition results remarkably close to those obtained by training on manually annotated bounding boxes.</dcterms:abstract>
        <dc:date>3/2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7477688/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-03 07:47:10</dcterms:dateSubmitted>
        <bib:pages>1-9</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_403">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/403/Bazzani 等。 - 2016 - Self-taught object localization with deep networks.pdf"/>
        <dc:title>Bazzani 等。 - 2016 - Self-taught object localization with deep networks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Dragomir_Anguelov/publication/265687273_Self-taught_Object_Localization_with_Deep_Networks/links/555b3af008aec5ac22320a45/Self-taught-Object-Localization-with-Deep-Networks.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-03 07:47:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s11263-013-0620-5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>104</prism:volume>
                <dc:title>International Journal of Computer Vision</dc:title>
                <dc:identifier>DOI 10.1007/s11263-013-0620-5</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>Int J Comput Vis</dcterms:alternative>
                <dc:identifier>ISSN 0920-5691, 1573-1405</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uijlings</foaf:surname>
                        <foaf:givenName>J. R. R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van de Sande</foaf:surname>
                        <foaf:givenName>K. E. A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gevers</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smeulders</foaf:surname>
                        <foaf:givenName>A. W. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_405"/>
        <dc:title>Selective Search for Object Recognition</dc:title>
        <dcterms:abstract>This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1.</dcterms:abstract>
        <dc:date>9/2013</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11263-013-0620-5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-04 03:00:24</dcterms:dateSubmitted>
        <bib:pages>154-171</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_405">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/405/selectiveSearchDraft.pdf"/>
        <dc:title>selectiveSearchDraft.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_407">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/407/zeilerECCV2014.pdf"/>
        <dc:title>zeilerECCV2014.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-05 04:10:52</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/6909475/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-5118-5</dc:identifier>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2014.81</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Columbus, OH, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Donahue</foaf:surname>
                        <foaf:givenName>Jeff</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darrell</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malik</foaf:surname>
                        <foaf:givenName>Jitendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_409"/>
        <dc:title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</dc:title>
        <dcterms:abstract>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ ˜rbg/rcnn.</dcterms:abstract>
        <dc:date>6/2014</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6909475/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-05 07:31:37</dcterms:dateSubmitted>
        <bib:pages>580-587</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_409">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/409/Girshick 等。 - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf"/>
        <dc:title>Girshick 等。 - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-05 07:31:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_411">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-5118-5</dc:identifier>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2014.81</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Columbus, OH, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Donahue</foaf:surname>
                        <foaf:givenName>Jeff</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darrell</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malik</foaf:surname>
                        <foaf:givenName>Jitendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_412"/>
        <link:link rdf:resource="#item_408"/>
        <dc:title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</dc:title>
        <dcterms:abstract>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ ˜rbg/rcnn.</dcterms:abstract>
        <dc:date>6/2014</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6909475/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-05 07:31:52</dcterms:dateSubmitted>
        <bib:pages>580-587</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_412">
       <rdf:value>&lt;p&gt;RCNN&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_408">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/408/Girshick 等。 - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf"/>
        <dc:title>Girshick 等。 - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/8954216/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00074</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Durand</foaf:surname>
                        <foaf:givenName>Thibaut</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mehrasa</foaf:surname>
                        <foaf:givenName>Nazanin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mori</foaf:surname>
                        <foaf:givenName>Greg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_413"/>
        <dc:title>Learning a Deep ConvNet for Multi-Label Classification With Partial Labels</dc:title>
        <dcterms:abstract>Deep ConvNets have shown great performance for single-label image classiﬁcation (e.g. ImageNet), but it is necessary to move beyond the single-label classiﬁcation task because pictures of everyday life are inherently multilabel. Multi-label classiﬁcation is a more difﬁcult task than single-label classiﬁcation because both the input images and output label spaces are more complex. Furthermore, collecting clean multi-label annotations is more difﬁcult to scale-up than single-label annotations. To reduce the annotation cost, we propose to train a model with partial labels i.e. only some labels are known per image. We ﬁrst empirically compare different labeling strategies to show the potential for using partial labels on multi-label datasets. Then to learn with partial labels, we introduce a new classiﬁcation loss that exploits the proportion of known labels per example. Our approach allows the use of the same training settings as when learning with all the annotations. We further explore several curriculum learning based strategies to predict missing labels. Experiments are performed on three large-scale multi-label datasets: MS COCO, NUS-WIDE and Open Images.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8954216/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-06 03:30:38</dcterms:dateSubmitted>
        <bib:pages>647-657</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_413">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/413/Durand 等。 - 2019 - Learning a Deep ConvNet for Multi-Label Classifica.pdf"/>
        <dc:title>Durand 等。 - 2019 - Learning a Deep ConvNet for Multi-Label Classifica.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/Durand_Learning_a_Deep_ConvNet_for_Multi-Label_Classification_With_Partial_Labels_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-06 03:30:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_416">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Felzenszwalb</foaf:surname>
                        <foaf:givenName>Pedro F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McAllester</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramanan</foaf:surname>
                        <foaf:givenName>Deva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_415"/>
        <dc:title>Object Detection with Discriminatively Trained Part Based Models</dc:title>
        <dcterms:abstract>We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difﬁcult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a marginsensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is speciﬁed for the positive examples. This leads to an iterative training algorithm that alternates between ﬁxing latent values for positive examples and optimizing the latent SVM objective function.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>20</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_415">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/415/Felzenszwalb 等。 - Object Detection with Discriminatively Trained Par.pdf"/>
        <dc:title>Felzenszwalb 等。 - Object Detection with Discriminatively Trained Par.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 06:05:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/8953706/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00079</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liujuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_417"/>
        <dc:title>Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</dc:title>
        <dcterms:abstract>Weakly supervised learning has attracted growing research attention due to the signiﬁcant saving in annotation cost for tasks that require intra-image annotations, such as object detection and semantic segmentation. To this end, existing weakly supervised object detection and semantic segmentation approaches follow an iterative label mining and model training pipeline. However, such a self-enforcement pipeline makes both tasks easy to be trapped in local minimums. In this paper, we join weakly supervised object detection and segmentation tasks with a multi-task learning scheme for the ﬁrst time, which uses their respective failure patterns to complement each other’s learning. Such crosstask enforcement helps both tasks to leap out of their respective local minimums. In particular, we present an efﬁcient and effective framework termed Weakly Supervised Joint Detection and Segmentation (WS-JDS). WS-JDS has two branches for the above two tasks, which share the same backbone network. In the learning stage, it uses the same cyclic training paradigm but with a speciﬁc loss function such that the two branches beneﬁt each other. Extensive experiments have been conducted on the widely-used Pascal VOC and COCO benchmarks, which demonstrate that our model has achieved competitive performance with the state-of-the-art algorithms.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953706/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 11:11:47</dcterms:dateSubmitted>
        <bib:pages>697-707</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_417">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/417/Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf"/>
        <dc:title>Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/Shen_Cyclic_Guidance_for_Weakly_Supervised_Joint_Detection_and_Segmentation_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 11:11:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_420">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00079</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liujuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_419"/>
        <dc:title>Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</dc:title>
        <dcterms:abstract>Weakly supervised learning has attracted growing research attention due to the signiﬁcant saving in annotation cost for tasks that require intra-image annotations, such as object detection and semantic segmentation. To this end, existing weakly supervised object detection and semantic segmentation approaches follow an iterative label mining and model training pipeline. However, such a self-enforcement pipeline makes both tasks easy to be trapped in local minimums. In this paper, we join weakly supervised object detection and segmentation tasks with a multi-task learning scheme for the ﬁrst time, which uses their respective failure patterns to complement each other’s learning. Such crosstask enforcement helps both tasks to leap out of their respective local minimums. In particular, we present an efﬁcient and effective framework termed Weakly Supervised Joint Detection and Segmentation (WS-JDS). WS-JDS has two branches for the above two tasks, which share the same backbone network. In the learning stage, it uses the same cyclic training paradigm but with a speciﬁc loss function such that the two branches beneﬁt each other. Extensive experiments have been conducted on the widely-used Pascal VOC and COCO benchmarks, which demonstrate that our model has achieved competitive performance with the state-of-the-art algorithms.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953706/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 13:19:21</dcterms:dateSubmitted>
        <bib:pages>697-707</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_419">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/419/Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf"/>
        <dc:title>Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_421">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/421/UWEETR-2010-0006.pdf"/>
        <dc:title>UWEETR-2010-0006.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1504.08083">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1504.08083 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_424"/>
        <link:link rdf:resource="#item_422"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Fast R-CNN</dc:title>
        <dcterms:abstract>This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.</dcterms:abstract>
        <dc:date>2015-09-27</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1504.08083</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 15:32:26</dcterms:dateSubmitted>
        <dc:description>arXiv: 1504.08083</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_424">
       <rdf:value>Comment: To appear in ICCV 2015</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_422">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/422/Girshick - 2015 - Fast R-CNN.pdf"/>
        <dc:title>Girshick - 2015 - Fast R-CNN.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1504.08083</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-07 15:32:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_425">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/425/Shen 等。 - 2018 - Generative Adversarial Learning Towards Fast Weakl.pdf"/>
        <dc:title>Shen 等。 - 2018 - Generative Adversarial Learning Towards Fast Weakl.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Generative_Adversarial_Learning_CVPR_2018_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:02:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-6420-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-6420-9</dc:identifier>
                <dc:title>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2018.00604</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Salt Lake City, UT, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Shengchuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zuo</foaf:surname>
                        <foaf:givenName>Wangmeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Generative Adversarial Learning Towards Fast Weakly Supervised Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection has attracted extensive research efforts in recent years. Without the need of annotating bounding boxes, the existing methods usually follow a two/multi-stage pipeline with an online compulsive stage to extract object proposals, which is an order of magnitude slower than fast fully supervised object detectors such as SSD [31] and YOLO [34]. In this paper, we speedup online weakly supervised object detectors by orders of magnitude by proposing a novel generative adversarial learning paradigm. In the proposed paradigm, the generator is a one-stage object detector to generate bounding boxes from images. To guide the learning of object-level generator, a surrogator is introduced to mine high-quality bounding boxes for training. We further adapt a structural similarity loss in combination with an adversarial loss into the training objective, which solves the challenge that the bounding boxes produced by the surrogator may not well capture their ground truth. Our one-stage detector outperforms all existing schemes in terms of detection accuracy, running at 118 frames per second, which is up to 438× faster than the state-of-the-art weakly supervised detectors [8, 30, 15, 27, 45]. The code will be available publicly soon.</dcterms:abstract>
        <dc:date>6/2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8578702/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:02:31</dcterms:dateSubmitted>
        <bib:pages>5764-5773</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_427">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/427/Shen 等。 - 2018 - Weakly Supervised Object Detection via Object-Spec.pdf"/>
        <dc:title>Shen 等。 - 2018 - Weakly Supervised Object Detection via Object-Spec.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8333805/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162-237X,%202162-2388"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Changhu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Xi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Xuelong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Weakly Supervised Object Detection via Object-Specific Pixel Gradient</dc:title>
        <dcterms:abstract>Most existing object detection algorithms are trained based upon a set of fully annotated object regions or bounding boxes, which are typically labor-intensive. On the contrary, nowadays there is a signiﬁcant amount of imagelevel annotations cheaply available on the Internet. It is hence a natural thought to explore such “weak” supervision to beneﬁt the training of object detectors. In this paper, we propose a novel scheme to perform weakly supervised object localization, termed object-speciﬁc pixel gradient (OPG). The OPG is trained by using image-level annotations alone, which performs in an iterative manner to localize potential objects in a given image robustly and efﬁciently. In particular, we ﬁrst extract an OPG map to reveal the contributions of individual pixels to a given object category, upon which an iterative mining scheme is further introduced to extract instances or components of this object. Moreover, a novel average and max pooling layer is introduced to improve the localization accuracy. In the task of weakly supervised object localization, the OPG achieves a state-of-the-art 44.5% top5 error on ILSVRC 2013, which outperforms competing methods, including Oquab et al. and region-based convolutional neural networks on the Pascal VOC 2012, with gains of 2.6% and 2.3%, respectively. In the task of object detection, OPG achieves a comparable performance of 27.0% mean average precision on Pascal VOC 2007. In all experiments, the OPG only adopts the off-the-shelf pretrained CNN model, without using any object proposals. Therefore, it also signiﬁcantly improves the detection speed, i.e., achieving three times faster compared with the stateof-the-art method.</dcterms:abstract>
        <dc:date>12/2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8333805/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:03:23</dcterms:dateSubmitted>
        <bib:pages>5960-5970</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162-237X,%202162-2388">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE Transactions on Neural Networks and Learning Systems</dc:title>
        <dc:identifier>DOI 10.1109/TNNLS.2018.2816021</dc:identifier>
        <prism:number>12</prism:number>
        <dcterms:alternative>IEEE Trans. Neural Netw. Learning Syst.</dcterms:alternative>
        <dc:identifier>ISSN 2162-237X, 2162-2388</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_429">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/429/Shen 等。 - Enabling Deep Residual Networks for Weakly Supervi.pdf"/>
        <dc:title>Shen 等。 - Enabling Deep Residual Networks for Weakly Supervi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530120.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:04:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_430">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Feiyue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yunsheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Enabling Deep Residual Networks for Weakly Supervised Object Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection (WSOD) has attracted extensive research attention due to its great ﬂexibility of exploiting largescale image-level annotation for detector training. Whilst deep residual networks such as ResNet and DenseNet have become the standard backbones for many computer vision tasks, the cutting-edge WSOD methods still rely on plain networks, e.g., VGG, as backbones. It is indeed not trivial to employ deep residual networks for WSOD, which even shows signiﬁcant deterioration of detection accuracy and non-convergence. In this paper, we discover the intrinsic root with sophisticated analysis and propose a sequence of design principles to take full advantages of deep residual learning for WSOD from the perspectives of adding redundancy, improving robustness and aligning features. First, a redundant adaptation neck is key for eﬀective object instance localization and discriminative feature learning. Second, small-kernel convolutions and MaxPool down-samplings help improve the robustness of information ﬂow, which gives ﬁner object boundaries and make the detector more sensitivity to small objects. Third, dilated convolution is essential to align the proposal features and exploit diverse local information by extracting highresolution feature maps. Extensive experiments show that the proposed principles enable deep residual networks to establishes new state-of-thearts on PASCAL VOC and MS COCO.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>19</bib:pages>
    </bib:Article>
    <bib:Article rdf:about="#item_432">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Feiyue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>UWSOD: Toward Fully-Supervised-Level Capacity Weakly Supervised Object Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection (WSOD) has attracted extensive research attention due to its great ﬂexibility of exploiting large-scale dataset with only image-level annotations for detector training. Despite its great advance in recent years, WSOD still suffers limited performance, which is far below that of fully supervised object detection (FSOD). As most WSOD methods depend on object proposal algorithms to generate candidate regions and are also confronted with challenges like low-quality predicted bounding boxes and large scale variation. In this paper, we propose a uniﬁed WSOD framework, termed UWSOD, to develop a high-capacity general detection model with only image-level labels, which is self-contained and does not require external modules or additional supervision. To this end, we exploit three important components, i.e., object proposal generation, bounding-box ﬁne-tuning and scale-invariant features. First, we propose an anchorbased self-supervised proposal generator to hypothesize object locations, which is trained end-to-end with supervision created by UWSOD for both objectness classiﬁcation and regression. Second, we develop a step-wise bounding-box ﬁnetuning to reﬁne both detection scores and coordinates by progressively select highconﬁdence object proposals as positive samples, which bootstraps the quality of predicted bounding boxes. Third, we construct a multi-rate resampling pyramid to aggregate multi-scale contextual information, which is the ﬁrst in-network feature hierarchy to handle scale variation in WSOD. Extensive experiments on PASCAL VOC and MS COCO show that the proposed UWSOD achieves competitive results with the state-of-the-art WSOD methods while not requiring external modules or additional supervision. Moreover, the upper-bound performance of UWSOD with class-agnostic ground-truth bounding boxes approaches Faster R-CNN, which demonstrates UWSOD has fully-supervised-level capacity. The code is available at: https://github.com/shenyunhang/UWSOD.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>15</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_433">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/433/Shen 等。 - 2020 - Category-Aware Spatial Constraint for Weakly Super.pdf"/>
        <dc:title>Shen 等。 - 2020 - Category-Aware Spatial Constraint for Weakly Super.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dacemirror.sci-hub.se/journal-article/0de42df1835893a79f9f9efd20349cce/shen2019.pdf?rand=5faddb6723249?download=true</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:05:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8809899/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1057-7149,%201941-0042"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Kuiyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Cheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Changhu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Category-Aware Spatial Constraint for Weakly Supervised Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection has attracted increasing research attention recently. To this end, most existing schemes rely on scoring category-independent region proposals, which is formulated as a multiple instance learning problem. During this process, the proposal scores are aggregated and supervised by only image-level labels, which often fails to locate object boundaries precisely. In this paper, we break through such a restriction by taking a deeper look into the score aggregation stage and propose a Category-aware Spatial Constraint (CSC) scheme for proposals, which is integrated into weakly supervised object detection in an end-to-end learning manner. In particular, we incorporate the global shape information of objects as an unsupervised constraint, which is inferred from buildin foreground-and-background cues, termed Category-speciﬁc Pixel Gradient (CPG) maps. Speciﬁcally, each region proposal is weighted according to how well it covers the estimated shape of objects. For each category, a multi-center regularization is further introduced to penalize the violations between centers cluster and high-score proposals in a given image. Extensive experiments are done on the most widely-used benchmark Pascal VOC and COCO, which shows that our approach signiﬁcantly improves weakly supervised object detection without adding new learnable parameters to the existing models nor changing the structures of CNNs.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8809899/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:05:06</dcterms:dateSubmitted>
        <bib:pages>843-858</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1057-7149,%201941-0042">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE Transactions on Image Processing</dc:title>
        <dc:identifier>DOI 10.1109/TIP.2019.2933735</dc:identifier>
        <dcterms:alternative>IEEE Trans. on Image Process.</dcterms:alternative>
        <dc:identifier>ISSN 1057-7149, 1941-0042</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="#item_436">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>29</prism:volume>
                <dc:title>IEEE Transactions on Image Processing</dc:title>
                <dc:identifier>DOI 10.1109/TIP.2019.2933735</dc:identifier>
                <dcterms:alternative>IEEE Trans. on Image Process.</dcterms:alternative>
                <dc:identifier>ISSN 1057-7149, 1941-0042</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Kuiyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Cheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Changhu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_435"/>
        <dc:title>Category-Aware Spatial Constraint for Weakly Supervised Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection has attracted increasing research attention recently. To this end, most existing schemes rely on scoring category-independent region proposals, which is formulated as a multiple instance learning problem. During this process, the proposal scores are aggregated and supervised by only image-level labels, which often fails to locate object boundaries precisely. In this paper, we break through such a restriction by taking a deeper look into the score aggregation stage and propose a Category-aware Spatial Constraint (CSC) scheme for proposals, which is integrated into weakly supervised object detection in an end-to-end learning manner. In particular, we incorporate the global shape information of objects as an unsupervised constraint, which is inferred from buildin foreground-and-background cues, termed Category-speciﬁc Pixel Gradient (CPG) maps. Speciﬁcally, each region proposal is weighted according to how well it covers the estimated shape of objects. For each category, a multi-center regularization is further introduced to penalize the violations between centers cluster and high-score proposals in a given image. Extensive experiments are done on the most widely-used benchmark Pascal VOC and COCO, which shows that our approach signiﬁcantly improves weakly supervised object detection without adding new learnable parameters to the existing models nor changing the structures of CNNs.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8809899/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 03:59:12</dcterms:dateSubmitted>
        <bib:pages>843-858</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_435">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/435/Shen 等。 - 2020 - Category-Aware Spatial Constraint for Weakly Super.pdf"/>
        <dc:title>Shen 等。 - 2020 - Category-Aware Spatial Constraint for Weakly Super.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dacemirror.sci-hub.se/journal-article/0de42df1835893a79f9f9efd20349cce/shen2019.pdf?rand=5faddb6723249?download=true</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 03:59:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_437">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Feiyue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_431"/>
        <dc:title>UWSOD: Toward Fully-Supervised-Level Capacity Weakly Supervised Object Detection</dc:title>
        <dcterms:abstract>Weakly supervised object detection (WSOD) has attracted extensive research attention due to its great ﬂexibility of exploiting large-scale dataset with only image-level annotations for detector training. Despite its great advance in recent years, WSOD still suffers limited performance, which is far below that of fully supervised object detection (FSOD). As most WSOD methods depend on object proposal algorithms to generate candidate regions and are also confronted with challenges like low-quality predicted bounding boxes and large scale variation. In this paper, we propose a uniﬁed WSOD framework, termed UWSOD, to develop a high-capacity general detection model with only image-level labels, which is self-contained and does not require external modules or additional supervision. To this end, we exploit three important components, i.e., object proposal generation, bounding-box ﬁne-tuning and scale-invariant features. First, we propose an anchorbased self-supervised proposal generator to hypothesize object locations, which is trained end-to-end with supervision created by UWSOD for both objectness classiﬁcation and regression. Second, we develop a step-wise bounding-box ﬁnetuning to reﬁne both detection scores and coordinates by progressively select highconﬁdence object proposals as positive samples, which bootstraps the quality of predicted bounding boxes. Third, we construct a multi-rate resampling pyramid to aggregate multi-scale contextual information, which is the ﬁrst in-network feature hierarchy to handle scale variation in WSOD. Extensive experiments on PASCAL VOC and MS COCO show that the proposed UWSOD achieves competitive results with the state-of-the-art WSOD methods while not requiring external modules or additional supervision. Moreover, the upper-bound performance of UWSOD with class-agnostic ground-truth bounding boxes approaches Faster R-CNN, which demonstrates UWSOD has fully-supervised-level capacity. The code is available at: https://github.com/shenyunhang/UWSOD.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>15</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_431">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/431/Shen 等。 - UWSOD Toward Fully-Supervised-Level Capacity Weak.pdf"/>
        <dc:title>Shen 等。 - UWSOD Toward Fully-Supervised-Level Capacity Weak.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://papers.nips.cc/paper/2020/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-13 01:04:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://github.com/charlesq34/frustum-pointnets">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_439"/>
        <dc:title>charlesq34/frustum-pointnets: Frustum PointNets for 3D Object Detection from RGB-D Data</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/charlesq34/frustum-pointnets</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 10:43:14</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_439">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/439/frustum-pointnets.html"/>
        <dc:title>charlesq34/frustum-pointnets: Frustum PointNets for 3D Object Detection from RGB-D Data</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/charlesq34/frustum-pointnets</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 10:43:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_441">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00079</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liujuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_440"/>
        <dc:title>Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</dc:title>
        <dcterms:abstract>Weakly supervised learning has attracted growing research attention due to the signiﬁcant saving in annotation cost for tasks that require intra-image annotations, such as object detection and semantic segmentation. To this end, existing weakly supervised object detection and semantic segmentation approaches follow an iterative label mining and model training pipeline. However, such a self-enforcement pipeline makes both tasks easy to be trapped in local minimums. In this paper, we join weakly supervised object detection and segmentation tasks with a multi-task learning scheme for the ﬁrst time, which uses their respective failure patterns to complement each other’s learning. Such crosstask enforcement helps both tasks to leap out of their respective local minimums. In particular, we present an efﬁcient and effective framework termed Weakly Supervised Joint Detection and Segmentation (WS-JDS). WS-JDS has two branches for the above two tasks, which share the same backbone network. In the learning stage, it uses the same cyclic training paradigm but with a speciﬁc loss function such that the two branches beneﬁt each other. Extensive experiments have been conducted on the widely-used Pascal VOC and COCO benchmarks, which demonstrate that our model has achieved competitive performance with the state-of-the-art algorithms.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953706/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 11:52:07</dcterms:dateSubmitted>
        <bib:pages>697-707</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_440">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/440/Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf"/>
        <dc:title>Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/Shen_Cyclic_Guidance_for_Weakly_Supervised_Joint_Detection_and_Segmentation_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 11:52:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_443">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00079</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liujuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_442"/>
        <dc:title>Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</dc:title>
        <dcterms:abstract>Weakly supervised learning has attracted growing research attention due to the signiﬁcant saving in annotation cost for tasks that require intra-image annotations, such as object detection and semantic segmentation. To this end, existing weakly supervised object detection and semantic segmentation approaches follow an iterative label mining and model training pipeline. However, such a self-enforcement pipeline makes both tasks easy to be trapped in local minimums. In this paper, we join weakly supervised object detection and segmentation tasks with a multi-task learning scheme for the ﬁrst time, which uses their respective failure patterns to complement each other’s learning. Such crosstask enforcement helps both tasks to leap out of their respective local minimums. In particular, we present an efﬁcient and effective framework termed Weakly Supervised Joint Detection and Segmentation (WS-JDS). WS-JDS has two branches for the above two tasks, which share the same backbone network. In the learning stage, it uses the same cyclic training paradigm but with a speciﬁc loss function such that the two branches beneﬁt each other. Extensive experiments have been conducted on the widely-used Pascal VOC and COCO benchmarks, which demonstrate that our model has achieved competitive performance with the state-of-the-art algorithms.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953706/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 11:52:12</dcterms:dateSubmitted>
        <bib:pages>697-707</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_442">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/442/Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf"/>
        <dc:title>Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2019/papers/Shen_Cyclic_Guidance_for_Weakly_Supervised_Joint_Detection_and_Segmentation_CVPR_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-14 11:52:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/8099807/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.324</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xiaolong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shrivastava</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_444"/>
        <dc:title>A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</dc:title>
        <dcterms:abstract>How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy – collect large-scale datasets which have object instances under different conditions. The hope is that the ﬁnal classiﬁer can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen; yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difﬁcult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>A-Fast-RCNN</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8099807/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-17 05:45:04</dcterms:dateSubmitted>
        <bib:pages>3039-3048</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_444">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/444/Wang 等。 - 2017 - A-Fast-RCNN Hard Positive Generation via Adversar.pdf"/>
        <dc:title>Wang 等。 - 2017 - A-Fast-RCNN Hard Positive Generation via Adversar.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_A-Fast-RCNN_Hard_Positive_CVPR_2017_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-17 05:44:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_448">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>Seong Joon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_446"/>
        <dc:title>Evaluating weakly-supervised models</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>82</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_446">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/446/Oh - Evaluating weakly-supervised models.pdf"/>
        <dc:title>Oh - Evaluating weakly-supervised models.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_449">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bilen</foaf:surname>
                        <foaf:givenName>Hakan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_451"/>
        <link:link rdf:resource="#item_450"/>
        <dc:title>ECCV 2020 Weakly Supervised Learning in Computer Vision</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>43</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_451">
       <rdf:value>&lt;p&gt;大佬tutorial&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_450">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Bilen_ECCV 2020 Weakly Supervised Learning in Computer Vision.pdf"/>
        <dc:title>Bilen_ECCV 2020 Weakly Supervised Learning in Computer Vision.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1702.08591">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1702.08591 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Balduzzi</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Frean</foaf:surname>
                        <foaf:givenName>Marcus</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leary</foaf:surname>
                        <foaf:givenName>Lennox</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lewis</foaf:surname>
                        <foaf:givenName>J. P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Kurt Wan-Duo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McWilliams</foaf:surname>
                        <foaf:givenName>Brian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_454"/>
        <link:link rdf:resource="#item_452"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The Shattered Gradients Problem: If resnets are the answer, then what is the question?</dc:title>
        <dcterms:abstract>A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite wellchosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Speciﬁcally, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.</dcterms:abstract>
        <dc:date>2018-06-06</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The Shattered Gradients Problem</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1702.08591</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-19 07:41:18</dcterms:dateSubmitted>
        <dc:description>arXiv: 1702.08591</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_454">
       <rdf:value>&lt;p&gt;Comment: ICML 2017, final version&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_452">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/452/Balduzzi 等。 - 2018 - The Shattered Gradients Problem If resnets are th.pdf"/>
        <dc:title>Balduzzi 等。 - 2018 - The Shattered Gradients Problem If resnets are th.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1702.08591v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-19 07:41:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_455">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/455/补充材料 Enabling Deep Residual Networks.pdf"/>
        <dc:title>补充材料 Enabling Deep Residual Networks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530120-supp.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-19 12:46:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1512.03385">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1512.03385 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xiangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Shaoqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_458"/>
        <link:link rdf:resource="#item_456"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Residual Learning for Image Recognition</dc:title>
        <dcterms:abstract>Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</dcterms:abstract>
        <dc:date>2015-12-10</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1512.03385</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-19 14:04:11</dcterms:dateSubmitted>
        <dc:description>arXiv: 1512.03385</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_458">
       <rdf:value>Comment: Tech report</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_456">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/456/resnet.pdf"/>
        <dc:title>resnet.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_460">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yan</foaf:surname>
                        <foaf:givenName>Ziang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Weishen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Jin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Changshui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_459"/>
        <dc:title>Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>9</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_459">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/459/Yan 等。 - Weakly- and Semi-Supervised Object Detection with .pdf"/>
        <dc:title>Yan 等。 - Weakly- and Semi-Supervised Object Detection with .pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1702.08740.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-20 01:26:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8554285/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1051-8215,%201558-2205"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Dingwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Junwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Guangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Long</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_461"/>
        <dc:title>Learning Object Detectors With Semi-Annotated Weak Labels</dc:title>
        <dcterms:abstract>For alleviating the human labor of annotating the training data for learning object detectors, recent research has paid great attention on semi-supervised object detection (SSOD) and weakly supervised object detection (WSOD) approaches. In SSOD, instead of annotating all the instances in the whole training set, people only need to annotate a part of the training instances using bounding boxes. In WSOD, people need to annotate the image-level tags on all training images to indicate the object categories contained by the corresponding images, while more detailed bounding box annotations are not needed any more. Along this line of research, this paper makes a further step to alleviate the human labor in annotating training data, leading to the problem of object detection with semi-annotated weak labels (ODSAWL). Instead of labelling image-level tags on all training images, ODSAWL only needs the image-level tags for a small portion of the training images, and then the object detectors can be learnt from such small portion of the weakly-labelled training images and the rest unlabelled training images. To address such a challenging problem, this paper proposes a cross model co-training framework, which collaborates an object localizer and a tag generator in an alternative optimization procedure. Speciﬁcally, during the learning procedure, these two (deep) models can transfer the needed knowledge (including labels and visual patterns) to each other. The whole learning procedure is accomplished in a few stages under the guidance of a progressive learning curriculum. To demonstrate the effectiveness of the proposed approach, we implement comprehensive experiments on three benchmark datasets, where the obtained experimental results are quite encouraging. Notably, by only using about 15% weakly labelled training images, the proposed approach can effectively approach to, or even outperform, the state-of-theart WSOD methods.</dcterms:abstract>
        <dc:date>12/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8554285/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-20 02:55:29</dcterms:dateSubmitted>
        <bib:pages>3622-3635</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1051-8215,%201558-2205">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE Transactions on Circuits and Systems for Video Technology</dc:title>
        <dc:identifier>DOI 10.1109/TCSVT.2018.2884173</dc:identifier>
        <prism:number>12</prism:number>
        <dcterms:alternative>IEEE Trans. Circuits Syst. Video Technol.</dcterms:alternative>
        <dc:identifier>ISSN 1051-8215, 1558-2205</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_461">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/461/Zhang 等。 - 2019 - Learning Object Detectors With Semi-Annotated Weak.pdf"/>
        <dc:title>Zhang 等。 - 2019 - Learning Object Detectors With Semi-Annotated Weak.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Book rdf:about="urn:isbn:978-3-030-58606-5%20978-3-030-58607-2">
        <z:itemType>book</z:itemType>
        <dcterms:isPartOf>
            <bib:Series>
               <dc:title>Lecture Notes in Computer Science</dc:title>
            </bib:Series>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vedaldi</foaf:surname>
                        <foaf:givenName>Andrea</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bischof</foaf:surname>
                        <foaf:givenName>Horst</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brox</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Frahm</foaf:surname>
                        <foaf:givenName>Jan-Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_463"/>
        <dc:title>Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X</dc:title>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Computer Vision – ECCV 2020</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-030-58607-2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 00:50:41</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1007/978-3-030-58607-2</dc:description>
        <prism:volume>12355</prism:volume>
        <dc:identifier>ISBN 978-3-030-58606-5 978-3-030-58607-2</dc:identifier>
    </bib:Book>
    <z:Attachment rdf:about="#item_463">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/463/10.1007@978-3-030-58607-2.pdf"/>
        <dc:title>10.1007@978-3-030-58607-2.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1910.07153">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1910.07153 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Mingfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zizhao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Guo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arik</foaf:surname>
                        <foaf:givenName>Sercan O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davis</foaf:surname>
                        <foaf:givenName>Larry S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pfister</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_468"/>
        <dcterms:isReferencedBy rdf:resource="#item_493"/>
        <dcterms:isReferencedBy rdf:resource="#item_467"/>
        <link:link rdf:resource="#item_465"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Consistency-based Semi-supervised Active Learning: Towards Minimizing Labeling Cost</dc:title>
        <dcterms:abstract>Active learning (AL) combines data labeling and model training to minimize the labeling cost by prioritizing the selection of high value data that can best improve model performance. In pool-based active learning, accessible unlabeled data are not used for model training in most conventional methods. Here, we propose to unify unlabeled sample selection and model training towards minimizing labeling cost, and make two contributions towards that end. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data during the training stage. Second, we propose a consistency-based sample selection metric that is coherent with the training objective such that the selected samples are eﬀective at improving model performance. We conduct extensive experiments on image classiﬁcation tasks. The experimental results on CIFAR-10, CIFAR-100 and ImageNet demonstrate the superior performance of our proposed method with limited labeled data, compared to the existing methods and the alternative AL and SSL combinations. Additionally, we also study an important yet under-explored problem – “When can we start learning-based AL selection?”. We propose a measure that is empirically correlated with the AL target loss and is potentially useful for determining the proper starting point of learning-based AL methods.</dcterms:abstract>
        <dc:date>2020-07-18</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Consistency-based Semi-supervised Active Learning</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1910.07153</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 00:58:35</dcterms:dateSubmitted>
        <dc:description>arXiv: 1910.07153</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_468">
        <rdf:value>&lt;p&gt;只在CIFAR-10, CIFAR-100 and ImageNet实验&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_493">
        <rdf:value>&lt;p&gt;主动学习，选择最有价值的样本：半监督方法搞不定的样本，挑出来人工标注&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_467">
       <rdf:value>Comment: Accepted by ECCV2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_465">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/465/1910.pdf"/>
        <dc:title>1910.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1910.07153</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 00:58:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2011.10043">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2011.10043 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Zhenda</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Yutong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Yue</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Stephen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Han</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_488"/>
        <link:link rdf:resource="#item_469"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning</dc:title>
        <dcterms:abstract>Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of contrastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The ﬁrst task directly applies contrastive learning at the pixel level. We additionally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Speciﬁcally, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pretraining not only regular backbone networks but also head networks used for dense downstream tasks, and are complementary to instance-level contrastive methods. These results demonstrate the strong potential of deﬁning pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning.</dcterms:abstract>
        <dc:date>2020-11-19</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Propagate Yourself</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2011.10043</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 01:10:59</dcterms:dateSubmitted>
        <dc:description>arXiv: 2011.10043</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_488">
       <rdf:value>&lt;p&gt;暂时 没人cite&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_469">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/469/Xie 等。 - 2020 - Propagate Yourself Exploring Pixel-Level Consiste.pdf"/>
        <dc:subject>没人cite</dc:subject>
        <dc:title>Xie 等。 - 2020 - Propagate Yourself Exploring Pixel-Level Consiste.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2011.10043.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 01:10:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p&gt;没人cite&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-3788-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-3788-3</dc:identifier>
                <dc:title>2018 24th International Conference on Pattern Recognition (ICPR)</dc:title>
                <dc:identifier>DOI 10.1109/ICPR.2018.8546088</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Beijing</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jiasi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xinggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Wenyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_471"/>
        <dc:title>Weakly- and Semi-supervised Faster R-CNN with Curriculum Learning</dc:title>
        <dcterms:abstract>Object detection is a core problem in computer vision and pattern recognition. In this paper, we study the problem of learning an effective object detector using weaklyannotated images (i.e., only the image level annotation is given) and a small proportion of fully-annotated images (i.e., bounding box level annotation is given) with curriculum learning. Our method is built upon Faster R-CNN. Different from previous weakly-supervised object detectors which rely on hand-craft object proposals, the proposed method learns a region proposal network using weakly- and semi-supervised training data. The weakly-labeled images are fed into the deep network in the order of from easy to complex; the process is formulated as curriculum learning. We name the Faster R-CNN trained using Weakly- And Semi-Supervised data with Curriculum Learning as WASSCL R-CNN. WASSCL R-CNN is validated on the PASCAL VOC 2007 benchmark, and obtains 90% of a fullysupervised Faster R-CNN’s performance (measured using mAP) with only 15% of fully-supervised annotations together with image-level annotations for the rest images. The results show that the proposed learning framework can signiﬁcantly reduce the labeling efforts for obtaining reliable object detectors.</dcterms:abstract>
        <dc:date>8/2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8546088/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:37:32</dcterms:dateSubmitted>
        <bib:pages>2416-2421</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2018 24th International Conference on Pattern Recognition (ICPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_471">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/471/Wang 等。 - 2018 - Weakly- and Semi-supervised Faster R-CNN with Curr.pdf"/>
        <dc:title>Wang 等。 - 2018 - Weakly- and Semi-supervised Faster R-CNN with Curr.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://xinggangw.info/pubs/wasscl-icpr18.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:37:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/8546088/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-3788-3</dc:identifier>
                <dc:title>2018 24th International Conference on Pattern Recognition (ICPR)</dc:title>
                <dc:identifier>DOI 10.1109/ICPR.2018.8546088</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Beijing</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jiasi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xinggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Wenyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_473"/>
        <dc:title>Weakly- and Semi-supervised Faster R-CNN with Curriculum Learning</dc:title>
        <dcterms:abstract>Object detection is a core problem in computer vision and pattern recognition. In this paper, we study the problem of learning an effective object detector using weaklyannotated images (i.e., only the image level annotation is given) and a small proportion of fully-annotated images (i.e., bounding box level annotation is given) with curriculum learning. Our method is built upon Faster R-CNN. Different from previous weakly-supervised object detectors which rely on hand-craft object proposals, the proposed method learns a region proposal network using weakly- and semi-supervised training data. The weakly-labeled images are fed into the deep network in the order of from easy to complex; the process is formulated as curriculum learning. We name the Faster R-CNN trained using Weakly- And Semi-Supervised data with Curriculum Learning as WASSCL R-CNN. WASSCL R-CNN is validated on the PASCAL VOC 2007 benchmark, and obtains 90% of a fullysupervised Faster R-CNN’s performance (measured using mAP) with only 15% of fully-supervised annotations together with image-level annotations for the rest images. The results show that the proposed learning framework can signiﬁcantly reduce the labeling efforts for obtaining reliable object detectors.</dcterms:abstract>
        <dc:date>8/2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8546088/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:49:08</dcterms:dateSubmitted>
        <bib:pages>2416-2421</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2018 24th International Conference on Pattern Recognition (ICPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_473">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/473/Wang 等。 - 2018 - Weakly- and Semi-supervised Faster R-CNN with Curr.pdf"/>
        <dc:title>Wang 等。 - 2018 - Weakly- and Semi-supervised Faster R-CNN with Curr.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://xinggangw.info/pubs/wasscl-icpr18.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:48:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2007.09162">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2007.09162 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yandong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Di</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>Danfeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Liqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gong</foaf:surname>
                        <foaf:givenName>Boqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_477"/>
        <link:link rdf:resource="#item_475"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Improving Object Detection with Selective Self-supervised Self-training</dc:title>
        <dcterms:abstract>We study how to leverage Web images to augment humancurated object detection datasets. Our approach is two-pronged. On the one hand, we retrieve Web images by image-to-image search, which incurs less domain shift from the curated data than other search methods. The Web images are diverse, supplying a wide variety of object poses, appearances, their interactions with the context, etc. On the other hand, we propose a novel learning method motivated by two parallel lines of work that explore unlabeled data for image classiﬁcation: self-training and self-supervised learning. They fail to improve object detectors in their vanilla forms due to the domain gap between the Web images and curated datasets. To tackle this challenge, we propose a selective net to rectify the supervision signals in Web images. It not only identiﬁes positive bounding boxes but also creates a safe zone for mining hard negative boxes. We report state-of-the-art results on detecting backpacks and chairs from everyday scenes, along with other challenging object classes.</dcterms:abstract>
        <dc:date>2020-07-24</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2007.09162</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:51:10</dcterms:dateSubmitted>
        <dc:description>arXiv: 2007.09162</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_477">
       <rdf:value>Comment: Accepted to ECCV 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_475">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/475/Li 等。 - 2020 - Improving Object Detection with Selective Self-sup.pdf"/>
        <dc:title>Li 等。 - 2020 - Improving Object Detection with Selective Self-sup.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/2007.09162.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-22 07:51:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1911.05722">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1911.05722 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>Haoqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yuxin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Saining</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_480"/>
        <link:link rdf:resource="#item_478"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Momentum Contrast for Unsupervised Visual Representation Learning</dc:title>
        <dcterms:abstract>We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classiﬁcation. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.</dcterms:abstract>
        <dc:date>2020-03-23</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1911.05722</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-23 02:00:58</dcterms:dateSubmitted>
        <dc:description>arXiv: 1911.05722</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_480">
        <rdf:value>Comment: CVPR 2020 camera-ready. Code: https://github.com/facebookresearch/moco</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_478">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/478/He 等。 - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf"/>
        <dc:title>He 等。 - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1911.05722</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-23 02:00:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9008567/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72814-803-8</dc:identifier>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2019.00937</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seoul, Korea (South)</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Charles R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Litany</foaf:surname>
                        <foaf:givenName>Or</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guibas</foaf:surname>
                        <foaf:givenName>Leonidas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_481"/>
        <dc:title>Deep Hough Voting for 3D Object Detection in Point Clouds</dc:title>
        <dcterms:abstract>Current 3D object detection methods are heavily inﬂuenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird’s eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to ﬁrst principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data – samples from 2D manifolds in 3D space – we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efﬁciency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.</dcterms:abstract>
        <dc:date>10/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9008567/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-24 01:29:49</dcterms:dateSubmitted>
        <bib:pages>9276-9285</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_481">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/481/Qi 等。 - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf"/>
        <dc:title>Qi 等。 - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1904.09664.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-24 01:29:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_483">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/483/10.1007@s11390-019-1975-z.pdf"/>
        <dc:title>10.1007@s11390-019-1975-z.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_485">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/485/1612.08242"/>
        <dc:title>1612.08242</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_486">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/486/yolo9000.pdf"/>
        <dc:title>yolo9000.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_487">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/487/HarisKhan_YOLO9000review_v2.pdf"/>
        <dc:title>HarisKhan_YOLO9000review_v2.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.05278">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2006.05278 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ouali</foaf:surname>
                        <foaf:givenName>Yassine</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hudelot</foaf:surname>
                        <foaf:givenName>Céline</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tami</foaf:surname>
                        <foaf:givenName>Myriam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_491"/>
        <link:link rdf:resource="#item_489"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>An Overview of Deep Semi-Supervised Learning</dc:title>
        <dcterms:abstract>Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classiﬁcation) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and eﬀort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-eﬃcient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the ﬁeld, followed by a summarization of the dominant semi-supervised approaches in deep learning1.</dcterms:abstract>
        <dc:date>2020-07-06</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.05278</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 03:17:40</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.05278</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_491">
       <rdf:value>Comment: Preprint</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_489">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/489/2006.05278v2.pdf"/>
        <dc:title>2006.05278v2.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.05278v2.pdf?</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 03:17:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_495">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jeong</foaf:surname>
                        <foaf:givenName>Jisoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Seungeui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Jeesoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kwak</foaf:surname>
                        <foaf:givenName>Nojun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_494"/>
        <dc:title>Consistency-based Semi-supervised Learning for Object detection</dc:title>
        <dcterms:abstract>Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Speciﬁcally, the consistency constraint is applied not only for object classiﬁcation but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage2 and two-stage detectors3 and the results show the effectiveness of our method.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_494">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/494/Jeong 等。 - Consistency-based Semi-supervised Learning for Obj.pdf"/>
        <dc:title>Jeong 等。 - Consistency-based Semi-supervised Learning for Obj.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://papers.nips.cc/paper/2019/file/d0f4dae80c3d0277922f8371d5827292-Paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 05:25:05</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_497">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shu</foaf:surname>
                        <foaf:givenName>Guang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>Mubarak</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_496"/>
        <dc:title>Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</dc:title>
        <dcterms:abstract>We propose a novel approach to boost the performance of generic object detectors on videos by learning videospeciﬁc features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-conﬁdence detections from a generic detector, then iteratively learn new video-speciﬁc features and reﬁne the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: ﬁrst it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that signiﬁcant performance improvement can be achieved with our proposed method.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>8</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_496">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/496/yang2013.pdf"/>
        <dc:title>yang2013.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_499">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Misra</foaf:surname>
                        <foaf:givenName>Ishan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shrivastava</foaf:surname>
                        <foaf:givenName>Abhinav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hebert</foaf:surname>
                        <foaf:givenName>Martial</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_498"/>
        <dc:title>Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos</dc:title>
        <dcterms:abstract>We present a semi-supervised approach that localizes multiple unknown object instances in long videos. We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances. We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift. Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data. Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. In contrast, existing approaches either do not consider multiple object instances per video, or rely heavily on the motion of the objects present. The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality, coverage (recall), diversity, and relevance to training an object detector.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_498">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/498/Misra 等。 - Watch and Learn Semi-Supervised Learning of Objec.pdf"/>
        <dc:title>Misra 等。 - Watch and Learn Semi-Supervised Learning of Objec.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2015/papers/Misra_Watch_and_Learn_2015_CVPR_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 11:32:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_500">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/500/2006.11692.pdf"/>
        <dc:title>2006.11692.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.11692.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 12:37:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_501">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/501/2006.11692.pdf"/>
        <dc:title>2006.11692.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.11692.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 12:45:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Book rdf:about="urn:isbn:978-3-030-61165-1%20978-3-030-61166-8">
        <z:itemType>book</z:itemType>
        <dcterms:isPartOf>
            <bib:Series>
               <dc:title>Lecture Notes in Computer Science</dc:title>
            </bib:Series>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cardoso</foaf:surname>
                        <foaf:givenName>Jaime</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Nguyen</foaf:surname>
                        <foaf:givenName>Hien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heller</foaf:surname>
                        <foaf:givenName>Nicholas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Henriques Abreu</foaf:surname>
                        <foaf:givenName>Pedro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Isgum</foaf:surname>
                        <foaf:givenName>Ivana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silva</foaf:surname>
                        <foaf:givenName>Wilson</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cruz</foaf:surname>
                        <foaf:givenName>Ricardo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pereira Amorim</foaf:surname>
                        <foaf:givenName>Jose</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patel</foaf:surname>
                        <foaf:givenName>Vishal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roysam</foaf:surname>
                        <foaf:givenName>Badri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Kevin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Steve</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Ngan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luu</foaf:surname>
                        <foaf:givenName>Khoa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sznitman</foaf:surname>
                        <foaf:givenName>Raphael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheplygina</foaf:surname>
                        <foaf:givenName>Veronika</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mateus</foaf:surname>
                        <foaf:givenName>Diana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Trucco</foaf:surname>
                        <foaf:givenName>Emanuele</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abbasi</foaf:surname>
                        <foaf:givenName>Samaneh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_502"/>
        <dc:title>Interpretable and Annotation-Efficient Learning for Medical Image Computing: Third International Workshop, iMIMIC 2020, Second International Workshop, MIL3ID 2020, and 5th International Workshop, LABELS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings</dc:title>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Interpretable and Annotation-Efficient Learning for Medical Image Computing</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-030-61166-8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-25 13:15:05</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1007/978-3-030-61166-8</dc:description>
        <prism:volume>12446</prism:volume>
        <dc:identifier>ISBN 978-3-030-61165-1 978-3-030-61166-8</dc:identifier>
    </bib:Book>
    <z:Attachment rdf:about="#item_502">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/502/10.1007@978-3-030-61166-8.pdf"/>
        <dc:title>10.1007@978-3-030-61166-8.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.mdpi.com/1424-8220/16/8/1325">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1424-8220"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yongzheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Guizhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yunpeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Xinkai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Yalong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_504"/>
        <dc:title>A Hybrid Vehicle Detection Method Based on Viola-Jones and HOG + SVM from UAV Images</dc:title>
        <dcterms:abstract>A new hybrid vehicle detection scheme which integrates the Viola-Jones (V-J) and linear SVM classiﬁer with HOG feature (HOG + SVM) methods is proposed for vehicle detection from low-altitude unmanned aerial vehicle (UAV) images. As both V-J and HOG + SVM are sensitive to on-road vehicles’ in-plane rotation, the proposed scheme ﬁrst adopts a roadway orientation adjustment method, which rotates each UAV image to align the roads with the horizontal direction so the original V-J or HOG + SVM method can be directly applied to achieve fast detection and high accuracy. To address the issue of descending detection speed for V-J and HOG + SVM, the proposed scheme further develops an adaptive switching strategy which sophistically integrates V-J and HOG + SVM methods based on their different descending trends of detection speed to improve detection efﬁciency. A comprehensive evaluation shows that the switching strategy, combined with the road orientation adjustment method, can signiﬁcantly improve the efﬁciency and effectiveness of the vehicle detection from UAV images. The results also show that the proposed vehicle detection method is competitive compared with other existing vehicle detection methods. Furthermore, since the proposed vehicle detection method can be performed on videos captured from moving UAV platforms without the need of image registration or additional road database, it has great potentials of ﬁeld applications. Future research will be focusing on expanding the current method for detecting other transportation modes such as buses, trucks, motors, bicycles, and pedestrians.</dcterms:abstract>
        <dc:date>2016-08-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://www.mdpi.com/1424-8220/16/8/1325</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 01:52:41</dcterms:dateSubmitted>
        <bib:pages>1325</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1424-8220">
        <prism:volume>16</prism:volume>
        <dc:title>Sensors</dc:title>
        <dc:identifier>DOI 10.3390/s16081325</dc:identifier>
        <prism:number>8</prism:number>
        <dcterms:alternative>Sensors</dcterms:alternative>
        <dc:identifier>ISSN 1424-8220</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_504">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/504/eee65964eeb00acda72ca387a2b191cf58c5.pdf"/>
        <dc:title>eee65964eeb00acda72ca387a2b191cf58c5.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdfs.semanticscholar.org/3dfa/eee65964eeb00acda72ca387a2b191cf58c5.pdf?_ga=2.18956920.1909546599.1605961032-662408763.1597203686</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 01:52:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-9471-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-9471-8</dc:identifier>
                <dc:title>2019 International Conference on Computational Intelligence in Data Science (ICCIDS)</dc:title>
                <dc:identifier>DOI 10.1109/ICCIDS.2019.8862136</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Chennai, India</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anuja Prasad</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mary</foaf:surname>
                        <foaf:givenName>Leena</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_508"/>
        <link:link rdf:resource="#item_506"/>
        <dc:title>A Comparative Study of Different Features for Vehicle Classification</dc:title>
        <dcterms:abstract>This paper presents a comparative study of different features for vehicle classiﬁcation. Real-time vehicle classiﬁcation system using computer vision is relatively cheaper and easy to install. As trafﬁc is heterogeneous in India, road planning and trafﬁc management is challenging. So an automated vehicle detection and classiﬁcation system is useful for trafﬁc survey, planning, signal time optimization and surveillance. In this work, trafﬁc video data is collected using a camera placed on the top of a vehicle parking on the side of a road at an angle of approximately 45°. Both audio and video are used for vehicle detection. The presence of a vehicle is detected from frames corresponding to the peaks in the short time energy of audio. The process of adaptive background subtraction is performed on the selected frames to separate the vehicle from the background. After background subtraction, morphological processes such as erosion, dilation and closing are applied to get the region of interest. There may be mulitiple frames with the same vehicle are detected at this stage. To reduce the multiple occurrences of the same vehicle in selected frames, Speeded-Up Robust Feature (SURF) matching algorithm is used. Different features like Histogram Oriented Gradient (HOG), Local Binary Pattern (LBP), KAZE, Binary Robust Invariant Scale Keypoint (BRISK) features of selected frames are extracted and Support Vector Machine (SVM) models are developed. Vehicle classiﬁcation accuracy of various features are compared using a 20 minutes trafﬁc video. It is observed that HOG gives the best result compared to KAZE, LBP and BRISK, with an accuracy of 85.50%.</dcterms:abstract>
        <dc:date>2/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8862136/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 02:04:10</dcterms:dateSubmitted>
        <bib:pages>1-5</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 International Conference on Computational Intelligence in Data Science (ICCIDS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_508">
        <rdf:value>&lt;ol&gt;
&lt;li&gt;&lt;span style=&quot;font-size: 9.9626pt; font-family: NimbusRomNo9L-Regu; color: #000000;&quot;&gt;测试集一共才300辆车左右&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-size: 9.9626pt; font-family: NimbusRomNo9L-Regu; color: #000000;&quot;&gt;分析音轨，峰值代表有车经过&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-size: 9.9626pt; font-family: NimbusRomNo9L-Regu; color: #000000;&quot;&gt;LBP and HOG gives the best result with an accuracy of &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-size: 9.9626pt; font-family: NimbusRomNo9L-Regu; color: #000000;&quot;&gt;86.80% and 88.50% &lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_506">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/506/Anuja Prasad 和 Mary - 2019 - A Comparative Study of Different Features for Vehi.pdf"/>
        <dc:title>Anuja Prasad 和 Mary - 2019 - A Comparative Study of Different Features for Vehi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://sci-hub.se/downloads/2019-10-28/d0/anujaprasad2019.pdf?rand=5fbf0c2f3d52c?download=true</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 02:04:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/7726065/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1524-9050,%201558-0016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yongzheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Guizhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Xinkai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yunpeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Yalong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_509"/>
        <dc:title>An Enhanced Viola-Jones Vehicle Detection Method From Unmanned Aerial Vehicles Imagery</dc:title>
        <dcterms:abstract>This research develops an advanced vehicle detection method, which improves the original Viola–Jones (V-J) object detection scheme for better vehicle detections from lowaltitude unmanned aerial vehicle (UAV) imagery. The original V-J method is sensitive to objects’ in-plane rotation, and therefore has difﬁculties in detecting vehicles with unknown orientations in UAV images. To address this issue, this research proposes a road orientation adjustment method, which rotates each UAV image once so that the roads and on-road vehicles on rotated images will be aligned with the horizontal direction and the V-J vehicle detector. Then, the original V-J can be directly applied to achieve better efﬁciency and accuracy. The enhanced V-J method is further applied for vehicle tracking. Testing results show that both vehicle detection and tracking methods are competitive compared with other existing methods. Future research will focus on expanding the current methods to detect other transport modes, such as buses, trucks, motorcycles, bicycles, and pedestrians.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7726065/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 03:09:42</dcterms:dateSubmitted>
        <bib:pages>1845-1856</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1524-9050,%201558-0016">
        <prism:volume>18</prism:volume>
        <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
        <dc:identifier>DOI 10.1109/TITS.2016.2617202</dc:identifier>
        <prism:number>7</prism:number>
        <dcterms:alternative>IEEE Trans. Intell. Transport. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 1524-9050, 1558-0016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_509">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/509/Xu 等。 - 2017 - An Enhanced Viola-Jones Vehicle Detection Method F.pdf"/>
        <dc:title>Xu 等。 - 2017 - An Enhanced Viola-Jones Vehicle Detection Method F.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_511">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/511/2006.05278v2.pdf"/>
        <dc:title>2006.05278v2.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.05278v2.pdf?</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 07:27:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_513">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ouali</foaf:surname>
                        <foaf:givenName>Yassine</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hudelot</foaf:surname>
                        <foaf:givenName>Céline</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tami</foaf:surname>
                        <foaf:givenName>Myriam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paris-Saclay</foaf:surname>
                        <foaf:givenName>Université</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_512"/>
        <dc:title>An Overview of Deep Semi-Supervised Learning</dc:title>
        <dcterms:abstract>Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classiﬁcation) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and eﬀort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-eﬃcient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the ﬁeld, followed by a summarization of the dominant semi-supervised approaches in deep learning1.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>43</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_512">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/512/Ouali 等。 - An Overview of Deep Semi-Supervised Learning.pdf"/>
        <dc:title>Ouali 等。 - An Overview of Deep Semi-Supervised Learning.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_514">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/514/fan2019.pdf"/>
        <dc:title>fan2019.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://sci-hub.se/downloads/2020-05-28/ca/fan2019.pdf?rand=5fbf5b69f34fa?download=true</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 07:41:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_515">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/515/2006.05278v2.pdf"/>
        <dc:title>2006.05278v2.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.05278v2.pdf?</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-26 07:56:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_516">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/516/tang2019.pdf"/>
        <dc:title>tang2019.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_517">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/517/fan2019.pdf"/>
        <dc:title>fan2019.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_518">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/518/yolo9000.pdf"/>
        <dc:title>yolo9000.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_519">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/519/xu2016.pdf"/>
        <dc:title>xu2016.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_520">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/520/2006.05278v2.pdf"/>
        <dc:title>2006.05278v2.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_521">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/521/yang2013.pdf"/>
        <dc:title>yang2013.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_522">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/522/HarisKhan_YOLO9000review_v2.pdf"/>
        <dc:title>HarisKhan_YOLO9000review_v2.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.10029">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2006.10029 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Ting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kornblith</foaf:surname>
                        <foaf:givenName>Simon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Swersky</foaf:surname>
                        <foaf:givenName>Kevin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Norouzi</foaf:surname>
                        <foaf:givenName>Mohammad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoffrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_525"/>
        <link:link rdf:resource="#item_523"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Big Self-Supervised Models are Strong Semi-Supervised Learners</dc:title>
        <dcterms:abstract>One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.</dcterms:abstract>
        <dc:date>2020-10-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.10029</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-27 08:12:35</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.10029</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_525">
        <rdf:value>Comment: NeurIPS'2020. Code and pretrained models at https://github.com/google-research/simclr</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_523">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/523/Chen 等。 - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf"/>
        <dc:title>Chen 等。 - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.10029.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-27 08:12:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.11692">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2006.11692 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>Jihun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Seungbum</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jeong</foaf:surname>
                        <foaf:givenName>Sanha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choi</foaf:surname>
                        <foaf:givenName>Min-Kook</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_528"/>
        <link:link rdf:resource="#item_526"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Semi-Supervised Object Detection with Sparsely Annotated Dataset</dc:title>
        <dcterms:abstract>In training object detector based on convolutional neural networks, selection of effective positive examples for training is an important factor. However, when training an anchor-based detectors with sparse annotations on an image, effort to ﬁnd effective positive examples can hinder training performance. When using the anchor-based training for the ground truth bounding box to collect positive examples under given IoU, it is often possible to include objects from other classes in the current training class, or objects that are needed to be trained can only be sampled as negative examples. We used two approaches to solve this problem: 1) the use of an anchorless object detector and 2) a semi-supervised learning-based object detection using a single object tracker. The proposed technique performs single object tracking by using the sparsely annotated bounding box as an anchor in the temporal domain for successive frames. From the tracking results, dense annotations for training images were generated in an automated manner and used for training the object detector. We applied the proposed single object tracking-based semi-supervised learning to the Epic-Kitchens dataset. As a result, we were able to achieve runner-up performance in the Unseen section while achieving the ﬁrst place in the Seen section of the Epic-Kitchens 2020 object detection challenge under IoU &gt; 0.5 evaluation.</dcterms:abstract>
        <dc:date>2020-06-20</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.11692</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-27 13:32:32</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.11692</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_528">
        <rdf:value>&lt;p&gt;Comment: Challenge Winner in Epic-Kitchens 2020 Object Detection Challenge (EPIC@CVPR 2020)&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_526">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/526/Yoon 等。 - 2020 - Semi-Supervised Object Detection with Sparsely Ann.pdf"/>
        <dc:title>Yoon 等。 - 2020 - Semi-Supervised Object Detection with Sparsely Ann.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.11692.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-27 13:32:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_529">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2016.308</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szegedy</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanhoucke</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ioffe</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shlens</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojna</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_531"/>
        <link:link rdf:resource="#item_532"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Benchmark testing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computational efficiency</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computational modeling</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer architecture</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>computer vision</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Computer vision</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Convolution</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>deep convolutional networks</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>ILSVRC 2012 classification challenge validation set</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>image classification</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>inception architecture</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>neural nets</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Training</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Rethinking the Inception Architecture for Computer Vision</dc:title>
        <dcterms:abstract>Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.</dcterms:abstract>
        <dc:date>June 2016</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:description>ISSN: 1063-6919</dc:description>
        <bib:pages>2818-2826</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_531">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="C:\Users\noway\Desktop\论文\Szegedy_2016_Rethinking the Inception Architecture for Computer Vision.pdf"/>
        <dc:title>Szegedy_2016_Rethinking the Inception Architecture for Computer Vision.pdf</dc:title>
        <z:linkMode>2</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_532">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/532/7780677.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/7780677/?arnumber=7780677</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-29 01:07:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2011.08036">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2011.08036 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chien-Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bochkovskiy</foaf:surname>
                        <foaf:givenName>Alexey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>Hong-Yuan Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_533"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Scaled-YOLOv4: Scaling Cross Stage Partial Network</dc:title>
        <dcterms:abstract>We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modiﬁes not only the depth, width, resolution, but also structure of the network. YOLOv4large model achieves state-of-the-art results: 55.4% AP (73.3% AP50) for the MS COCO dataset at a speed of 15 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 55.8% AP (73.2 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of ∼443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.</dcterms:abstract>
        <dc:date>2020-11-16</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Scaled-YOLOv4</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2011.08036</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 05:48:24</dcterms:dateSubmitted>
        <dc:description>arXiv: 2011.08036</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_533">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/533/Wang 等。 - 2020 - Scaled-YOLOv4 Scaling Cross Stage Partial Network.pdf"/>
        <dc:title>Wang 等。 - 2020 - Scaled-YOLOv4 Scaling Cross Stage Partial Network.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2011.08036v1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 05:48:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_537">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/537/（备份）Kisantal 等。 - 2019 - Augmentation for small object detection.pdf"/>
        <dc:title>（备份）Kisantal 等。 - 2019 - Augmentation for small object detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_538">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/538/（备份）Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf"/>
        <dc:title>（备份）Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_539">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1710.09412 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cisse</foaf:surname>
                        <foaf:givenName>Moustapha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dauphin</foaf:surname>
                        <foaf:givenName>Yann N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lopez-Paz</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_540"/>
        <link:link rdf:resource="#item_535"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>mixup: Beyond Empirical Risk Minimization</dc:title>
        <dcterms:abstract>Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also ﬁnd that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.</dcterms:abstract>
        <dc:date>2018-04-27</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>mixup</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1710.09412</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:19:30</dcterms:dateSubmitted>
        <dc:description>arXiv: 1710.09412</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_540">
        <rdf:value>Comment: ICLR camera ready version. Changes vs V1: fix repo URL; add ablation studies; add mixup + dropout etc</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_535">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/535/Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf"/>
        <dc:title>Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_545">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>5</prism:volume>
                <dc:title>National Science Review</dc:title>
                <dc:identifier>DOI 10.1093/nsr/nwx106</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 2095-5138, 2053-714X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Zhi-Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_536"/>
        <dc:title>A brief introduction to weakly supervised learning</dc:title>
        <dcterms:abstract>Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.</dcterms:abstract>
        <dc:date>2018-01-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://academic.oup.com/nsr/article/5/1/44/4093912</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:19:48</dcterms:dateSubmitted>
        <bib:pages>44-53</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_536">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/536/Zhou - 2018 - A brief introduction to weakly supervised learning.pdf"/>
        <dc:title>Zhou - 2018 - A brief introduction to weakly supervised learning.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_546">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>5</prism:volume>
                <dc:title>National Science Review</dc:title>
                <dc:identifier>DOI 10.1093/nsr/nwx106</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 2095-5138, 2053-714X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Zhi-Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_541"/>
        <dc:title>A brief introduction to weakly supervised learning</dc:title>
        <dcterms:abstract>Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.</dcterms:abstract>
        <dc:date>2018-01-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://academic.oup.com/nsr/article/5/1/44/4093912</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:20:04</dcterms:dateSubmitted>
        <bib:pages>44-53</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_541">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/541/Zhou - 2018 - A brief introduction to weakly supervised learning.pdf"/>
        <dc:title>Zhou - 2018 - A brief introduction to weakly supervised learning.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_547">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1902.07296 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kisantal</foaf:surname>
                        <foaf:givenName>Mate</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojna</foaf:surname>
                        <foaf:givenName>Zbigniew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murawski</foaf:surname>
                        <foaf:givenName>Jakub</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naruniec</foaf:surname>
                        <foaf:givenName>Jacek</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>Kyunghyun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_542"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Augmentation for small object detection</dc:title>
        <dcterms:abstract>In the recent years, object detection has experienced impressive progress. Despite these improvements, there is still a signiﬁcant gap in the performance between the detection of small and large objects. We analyze the current state-of-the-art model, Mask-RCNN, on a challenging dataset, MS COCO. We show that the overlap between small ground-truth objects and the predicted anchors is much lower than the expected IoU threshold. We conjecture this is due to two factors; (1) only a few images are containing small objects, and (2) small objects do not appear enough even within each image containing them. We thus propose to oversample those images with small objects and augment each of those images by copy-pasting small objects many times. It allows us to trade oﬀ the quality of the detector on large objects with that on small objects. We evaluate diﬀerent pasting augmentation strategies, and ultimately, we achieve 9.7% relative improvement on the instance segmentation and 7.1% on the object detection of small objects, compared to the current state of the art method on MS COCO.</dcterms:abstract>
        <dc:date>2019-02-19</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1902.07296</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:20:09</dcterms:dateSubmitted>
        <dc:description>arXiv: 1902.07296</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_542">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/542/Kisantal 等。 - 2019 - Augmentation for small object detection.pdf"/>
        <dc:title>Kisantal 等。 - 2019 - Augmentation for small object detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_548">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01134</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Xiaopeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jianzhuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Mingliang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_543"/>
        <dc:title>Noise-Aware Fully Webly Supervised Object Detection</dc:title>
        <dcterms:abstract>We investigate the emerging task of learning object detectors with sole image-level labels on the web without requiring any other supervision like precise annotations or additional images from well-annotated benchmark datasets. Such a task, termed as fully webly supervised object detection, is extremely challenging, since image-level labels on the web are always noisy, leading to poor performance of the learned detectors. In this work, we propose an end-toend framework to jointly learn webly supervised detectors and reduce the negative impact of noisy labels. Such noise is heterogeneous, which is further categorized into two types, namely background noise and foreground noise. Regarding the background noise, we propose a residual learning structure incorporated with weakly supervised detection, which decomposes background noise and models clean data. To explicitly learn the residual feature between clean data and noisy labels, we further propose a spatially-sensitive entropy criterion, which exploits the conditional distribution of detection results to estimate the conﬁdence of background categories being noise. Regarding the foreground noise, a bagging-mixup learning is introduced, which suppresses foreground noisy signals from incorrectly labelled images, whilst maintaining the diversity of training data. We evaluate the proposed approach on popular benchmark datasets by training detectors on web images, which are retrieved by the corresponding category tags from photo-sharing sites. Extensive experiments show that our method achieves signiﬁcant improvements over the state-of-the-art methods 1.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9156477/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:20:16</dcterms:dateSubmitted>
        <bib:pages>11323-11332</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_543">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/543/Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf"/>
        <dc:title>Shen 等。 - 2020 - Noise-Aware Fully Webly Supervised Object Detectio.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_549">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1710.09412 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Hongyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cisse</foaf:surname>
                        <foaf:givenName>Moustapha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dauphin</foaf:surname>
                        <foaf:givenName>Yann N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lopez-Paz</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_550"/>
        <link:link rdf:resource="#item_544"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>mixup: Beyond Empirical Risk Minimization</dc:title>
        <dcterms:abstract>Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also ﬁnd that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.</dcterms:abstract>
        <dc:date>2018-04-27</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>mixup</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1710.09412</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 08:20:22</dcterms:dateSubmitted>
        <dc:description>arXiv: 1710.09412</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_550">
        <rdf:value>Comment: ICLR camera ready version. Changes vs V1: fix repo URL; add ablation studies; add mixup + dropout etc</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_544">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/544/Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf"/>
        <dc:title>Zhang 等。 - 2018 - mixup Beyond Empirical Risk Minimization.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9008127/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72814-803-8</dc:identifier>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2019.00202</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seoul, Korea (South)</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Yew Siang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Gim Hee</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_552"/>
        <link:link rdf:resource="#item_484"/>
        <dc:title>Transferable Semi-Supervised 3D Object Detection From RGB-D Data</dc:title>
        <dcterms:abstract>We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semisupervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Boxto-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.</dcterms:abstract>
        <dc:date>10/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9008127/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-11-30 10:48:51</dcterms:dateSubmitted>
        <bib:pages>1931-1940</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_552">
       <rdf:value>&lt;p&gt;半监督 3D检测&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_484">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/484/Tang 和 Lee - 2019 - Transferable Semi-Supervised 3D Object Detection F.pdf"/>
        <dc:title>Tang 和 Lee - 2019 - Transferable Semi-Supervised 3D Object Detection F.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_554">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2006.05278 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ouali</foaf:surname>
                        <foaf:givenName>Yassine</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hudelot</foaf:surname>
                        <foaf:givenName>Céline</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tami</foaf:surname>
                        <foaf:givenName>Myriam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_555"/>
        <link:link rdf:resource="#item_553"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>An Overview of Deep Semi-Supervised Learning</dc:title>
        <dcterms:abstract>Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classiﬁcation) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and eﬀort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-eﬃcient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the ﬁeld, followed by a summarization of the dominant semi-supervised approaches in deep learning1.</dcterms:abstract>
        <dc:date>2020-07-06</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.05278</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-01 07:29:14</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.05278</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_555">
       <rdf:value>Comment: Preprint</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_553">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/553/综述 - An Overview of Deep Semi-Supervised Learning.pdf"/>
        <dc:title>综述 - An Overview of Deep Semi-Supervised Learning.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2006.06882">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:2006.06882 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zoph</foaf:surname>
                        <foaf:givenName>Barret</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghiasi</foaf:surname>
                        <foaf:givenName>Golnaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>Yin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Hanxiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cubuk</foaf:surname>
                        <foaf:givenName>Ekin D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_558"/>
        <link:link rdf:resource="#item_556"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Rethinking Pre-training and Self-training</dc:title>
        <dcterms:abstract>Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+.</dcterms:abstract>
        <dc:date>2020-11-15</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2006.06882</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-01 07:38:22</dcterms:dateSubmitted>
        <dc:description>arXiv: 2006.06882</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_558">
        <rdf:value>Comment: Accepted for publication at the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_556">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/556/Zoph 等。 - 2020 - Rethinking Pre-training and Self-training.pdf"/>
        <dc:title>Zoph 等。 - 2020 - Rethinking Pre-training and Self-training.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2006.06882.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-01 07:38:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_560">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:2011.08036 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chien-Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bochkovskiy</foaf:surname>
                        <foaf:givenName>Alexey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>Hong-Yuan Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_559"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Scaled-YOLOv4: Scaling Cross Stage Partial Network</dc:title>
        <dcterms:abstract>We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modiﬁes not only the depth, width, resolution, but also structure of the network. YOLOv4large model achieves state-of-the-art results: 55.4% AP (73.3% AP50) for the MS COCO dataset at a speed of 15 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 55.8% AP (73.2 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of ∼443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.</dcterms:abstract>
        <dc:date>2020-11-16</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Scaled-YOLOv4</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2011.08036</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-01 07:57:58</dcterms:dateSubmitted>
        <dc:description>arXiv: 2011.08036</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_559">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/559/Wang 等。 - 2020 - Scaled-YOLOv4 Scaling Cross Stage Partial Network.pdf"/>
        <dc:title>Wang 等。 - 2020 - Scaled-YOLOv4 Scaling Cross Stage Partial Network.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2011.08036v1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-01 07:57:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72819-360-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72819-360-1</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</dc:title>
                <dc:identifier>DOI 10.1109/CVPRW50498.2020.00203</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chien-Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mark Liao</foaf:surname>
                        <foaf:givenName>Hong-Yuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yueh-Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Ping-Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hsieh</foaf:surname>
                        <foaf:givenName>Jun-Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yeh</foaf:surname>
                        <foaf:givenName>I-Hau</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_561"/>
        <dc:title>CSPNet: A New Backbone that can Enhance Learning Capability of CNN</dc:title>
        <dcterms:abstract>Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CSPNet</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9150780/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-02 02:51:13</dcterms:dateSubmitted>
        <bib:pages>1571-1580</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_561">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/561/Wang 等。 - 2020 - CSPNet A New Backbone that can Enhance Learning C.pdf"/>
        <dc:title>Wang 等。 - 2020 - CSPNet A New Backbone that can Enhance Learning C.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-02 02:51:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Google AI Blog: Revisiting the Unreasonable Effectiveness of Data</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-04 11:38:14</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Article rdf:about="#item_565">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Chen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_564"/>
        <dc:title>Learning from Web-scale Image Data For Visual Recognition</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>48</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_564">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/564/Sun - Learning from Web-scale Image Data For Visual Reco.pdf"/>
        <dc:title>Sun - Learning from Web-scale Image Data For Visual Reco.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://data.vision.ee.ethz.ch/cvl/webvision/2017/files/slides_Chen.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-04 11:42:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_567"/>
        <dc:title>Google AI Blog: Advancing Self-Supervised and Semi-Supervised Learning with SimCLR</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-04 22:22:22</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_567">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/567/advancing-self-supervised-and-semi.html"/>
        <dc:title>Google AI Blog: Advancing Self-Supervised and Semi-Supervised Learning with SimCLR</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-04 22:22:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_569">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbu</foaf:surname>
                        <foaf:givenName>Andrei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mayo</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alverio</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gutfreund</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tenenbaum</foaf:surname>
                        <foaf:givenName>Josh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_568"/>
        <dc:title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</dc:title>
        <dcterms:abstract>We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>11</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_568">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/568/Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf"/>
        <dc:title>Barbu 等。 - ObjectNet A large-scale bias-controlled dataset f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-04 23:07:43</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1904.09664">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1904.09664 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Charles R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Litany</foaf:surname>
                        <foaf:givenName>Or</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guibas</foaf:surname>
                        <foaf:givenName>Leonidas J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_572"/>
        <link:link rdf:resource="#item_570"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Hough Voting for 3D Object Detection in Point Clouds</dc:title>
        <dcterms:abstract>Current 3D object detection methods are heavily inﬂuenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird’s eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to ﬁrst principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data – samples from 2D manifolds in 3D space – we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efﬁciency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.</dcterms:abstract>
        <dc:date>2019-08-22</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.09664</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 00:34:36</dcterms:dateSubmitted>
        <dc:description>arXiv: 1904.09664</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_572">
       <rdf:value>Comment: ICCV 2019</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_570">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/570/Qi 等。 - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf"/>
        <dc:title>Qi 等。 - 2019 - Deep Hough Voting for 3D Object Detection in Point.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1904.09664.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 00:34:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-61284-386-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-61284-386-5</dc:identifier>
                <dc:title>2011 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2011.5980567</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Shanghai, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rusu</foaf:surname>
                        <foaf:givenName>Radu Bogdan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cousins</foaf:surname>
                        <foaf:givenName>Steve</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_573"/>
        <dc:title>3D is here: Point Cloud Library (PCL)</dc:title>
        <dcterms:abstract>With the advent of new, low-cost 3D sensing hardware such as the Kinect, and continued efforts in advanced point cloud processing, 3D perception gains more and more importance in robotics, as well as other ﬁelds.</dcterms:abstract>
        <dc:date>05/2011</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>3D is here</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/5980567/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 01:17:53</dcterms:dateSubmitted>
        <bib:pages>1-4</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2011 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_573">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/573/Rusu 和 Cousins - 2011 - 3D is here Point Cloud Library (PCL).pdf"/>
        <dc:title>Rusu 和 Cousins - 2011 - 3D is here Point Cloud Library (PCL).pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pointclouds.org/assets/pdf/pcl_icra2011.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 01:17:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9009553/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72814-803-8</dc:identifier>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
                <dc:identifier>DOI 10.1109/ICCV.2019.00852</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seoul, Korea (South)</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>Shuai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Zeming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Tianyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peng</foaf:surname>
                        <foaf:givenName>Chao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Gang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xiangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Jing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_575"/>
        <dc:title>Objects365: A Large-Scale, High-Quality Dataset for Object Detection</dc:title>
        <dcterms:abstract>In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models signiﬁcantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the ﬁnetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been veriﬁed on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrainedmodels have been released at www.objects365.org.</dcterms:abstract>
        <dc:date>10/2019</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Objects365</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9009553/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 01:57:49</dcterms:dateSubmitted>
        <bib:pages>8429-8438</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_575">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/575/Shao 等。 - 2019 - Objects365 A Large-Scale, High-Quality Dataset fo.pdf"/>
        <dc:title>Shao 等。 - 2019 - Objects365 A Large-Scale, High-Quality Dataset fo.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 01:57:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_578">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/578/1911.pdf"/>
        <dc:title>1911.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1911.04252</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 02:24:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/8100237/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.754</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yim</foaf:surname>
                        <foaf:givenName>Junho</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joo</foaf:surname>
                        <foaf:givenName>Donggyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bae</foaf:surname>
                        <foaf:givenName>Jihoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Junmo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_579"/>
        <dc:title>A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</dc:title>
        <dcterms:abstract>We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we deﬁne the distilled knowledge to be transferred in terms of ﬂow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the ﬂow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>A Gift from Knowledge Distillation</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8100237/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:26:32</dcterms:dateSubmitted>
        <bib:pages>7130-7138</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_579">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/579/Yim 等。 - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf"/>
        <dc:title>Yim 等。 - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:26:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_582">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-0457-1</dc:identifier>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2017.754</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Honolulu, HI</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yim</foaf:surname>
                        <foaf:givenName>Junho</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joo</foaf:surname>
                        <foaf:givenName>Donggyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bae</foaf:surname>
                        <foaf:givenName>Jihoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Junmo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_581"/>
        <dc:title>A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</dc:title>
        <dcterms:abstract>We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we deﬁne the distilled knowledge to be transferred in terms of ﬂow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the ﬂow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.</dcterms:abstract>
        <dc:date>7/2017</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>A Gift from Knowledge Distillation</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8100237/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:26:53</dcterms:dateSubmitted>
        <bib:pages>7130-7138</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_581">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/581/Yim 等。 - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf"/>
        <dc:title>Yim 等。 - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:26:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1503.02531">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1503.02531 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoffrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vinyals</foaf:surname>
                        <foaf:givenName>Oriol</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dean</foaf:surname>
                        <foaf:givenName>Jeff</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_585"/>
        <dcterms:isReferencedBy rdf:resource="#item_592"/>
        <link:link rdf:resource="#item_583"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Distilling the Knowledge in a Neural Network</dc:title>
        <dcterms:abstract>A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.</dcterms:abstract>
        <dc:date>2015-03-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1503.02531</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:31:22</dcterms:dateSubmitted>
        <dc:description>arXiv: 1503.02531</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_585">
        <rdf:value>&lt;p&gt;Comment: NIPS 2014 Deep Learning Workshop&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_592">
       <rdf:value>&lt;p&gt;Hinton 首创&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_583">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/583/Hinton 等。 - 2015 - Distilling the Knowledge in a Neural Network.pdf"/>
        <dc:title>Hinton 等。 - 2015 - Distilling the Knowledge in a Neural Network.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1503.02531.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 11:31:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_589">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72813-293-8</dc:identifier>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00079</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Long Beach, CA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Yunhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Rongrong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Liujuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_586"/>
        <dc:title>Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation</dc:title>
        <dcterms:abstract>Weakly supervised learning has attracted growing research attention due to the signiﬁcant saving in annotation cost for tasks that require intra-image annotations, such as object detection and semantic segmentation. To this end, existing weakly supervised object detection and semantic segmentation approaches follow an iterative label mining and model training pipeline. However, such a self-enforcement pipeline makes both tasks easy to be trapped in local minimums. In this paper, we join weakly supervised object detection and segmentation tasks with a multi-task learning scheme for the ﬁrst time, which uses their respective failure patterns to complement each other’s learning. Such crosstask enforcement helps both tasks to leap out of their respective local minimums. In particular, we present an efﬁcient and effective framework termed Weakly Supervised Joint Detection and Segmentation (WS-JDS). WS-JDS has two branches for the above two tasks, which share the same backbone network. In the learning stage, it uses the same cyclic training paradigm but with a speciﬁc loss function such that the two branches beneﬁt each other. Extensive experiments have been conducted on the widely-used Pascal VOC and COCO benchmarks, which demonstrate that our model has achieved competitive performance with the state-of-the-art algorithms.</dcterms:abstract>
        <dc:date>6/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8953706/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 02:15:22</dcterms:dateSubmitted>
        <bib:pages>697-707</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_586">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/586/Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf"/>
        <dc:title>Shen 等。 - 2019 - Cyclic Guidance for Weakly Supervised Joint Detect.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://ieeexplore.ieee.org/document/7780677/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-8851-1</dc:identifier>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2016.308</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Las Vegas, NV, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szegedy</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanhoucke</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ioffe</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shlens</foaf:surname>
                        <foaf:givenName>Jon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojna</foaf:surname>
                        <foaf:givenName>Zbigniew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_587"/>
        <dc:title>Rethinking the Inception Architecture for Computer Vision</dc:title>
        <dcterms:abstract>Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error on the validation set and 3.6% top-5 error on the ofﬁcial test set.</dcterms:abstract>
        <dc:date>6/2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7780677/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 02:15:27</dcterms:dateSubmitted>
        <bib:pages>2818-2826</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_587">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/587/Szegedy 等。 - 2016 - Rethinking the Inception Architecture for Computer.pdf"/>
        <dc:title>Szegedy 等。 - 2016 - Rethinking the Inception Architecture for Computer.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9156610/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01070</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Qizhe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luong</foaf:surname>
                        <foaf:givenName>Minh-Thang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hovy</foaf:surname>
                        <foaf:givenName>Eduard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Quoc V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_588"/>
        <dc:title>Self-Training With Noisy Student Improves ImageNet Classification</dc:title>
        <dcterms:abstract>We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean ﬂip rate from 27.8 to 12.2.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9156610/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 02:15:30</dcterms:dateSubmitted>
        <bib:pages>10684-10695</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_588">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/588/Xie 等。 - 2020 - Self-Training With Noisy Student Improves ImageNet.pdf"/>
        <dc:title>Xie 等。 - 2020 - Self-Training With Noisy Student Improves ImageNet.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1511.04136">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1511.04136 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Longyin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>Dawei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Zhaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lei</foaf:surname>
                        <foaf:givenName>Zhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>Ming-Ching</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Honggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lim</foaf:surname>
                        <foaf:givenName>Jongwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Ming-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lyu</foaf:surname>
                        <foaf:givenName>Siwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_617"/>
        <dcterms:isReferencedBy rdf:resource="#item_594"/>
        <link:link rdf:resource="#item_577"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking</dc:title>
        <dcterms:abstract>Eﬀective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predeﬁned setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traﬃc scenes (over 140, 000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and tracking methods. Our analysis shows the complex eﬀects of detection accuracy on MOT system performance. Based on these observations, we propose eﬀective and informative evaluation metrics for MOT systems that consider the eﬀect of object detection for comprehensive performance analysis.</dcterms:abstract>
        <dc:date>2020-01-24</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>UA-DETRAC</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1511.04136</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 03:09:16</dcterms:dateSubmitted>
        <dc:description>arXiv: 1511.04136</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_617">
       <rdf:value>&lt;p&gt;20年更新后&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_594">
       <rdf:value>Comment: 18 pages, 11 figures, accepted by CVIU</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_577">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/577/Wen 等。 - 2020 - UA-DETRAC A New Benchmark and Protocol for Multi-.pdf"/>
        <dc:title>Wen 等。 - 2020 - UA-DETRAC A New Benchmark and Protocol for Multi-.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1511.04136.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-05 02:05:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_595">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/595/paper.pdf"/>
        <dc:title>paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 08:55:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_596">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/596/paper.pdf"/>
        <dc:title>paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 08:55:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_597">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/597/paper.pdf"/>
        <dc:title>paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 08:55:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_599">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-6964-0</dc:identifier>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2015.7298655</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Boston, MA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Shuran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lichtenberg</foaf:surname>
                        <foaf:givenName>Samuel P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Jianxiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_598"/>
        <dc:title>SUN RGB-D: A RGB-D scene understanding benchmark suite</dc:title>
        <dcterms:abstract>Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.</dcterms:abstract>
        <dc:date>6/2015</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SUN RGB-D</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7298655/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 09:07:36</dcterms:dateSubmitted>
        <bib:pages>567-576</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_598">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/598/Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf"/>
        <dc:title>Song 等。 - 2015 - SUN RGB-D A RGB-D scene understanding benchmark s.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_600">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/600/paper.pdf"/>
        <dc:title>paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 09:35:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_601">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/601/paper.pdf"/>
        <dc:title>paper.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 09:35:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1207.0580">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1207.0580 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoffrey E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srivastava</foaf:surname>
                        <foaf:givenName>Nitish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krizhevsky</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutskever</foaf:surname>
                        <foaf:givenName>Ilya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>Ruslan R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_602"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Improving neural networks by preventing co-adaptation of feature detectors</dc:title>
        <dcterms:abstract>When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This &quot;overfitting&quot; is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random &quot;dropout&quot; gives big improvements on many benchmark tasks and sets new records for speech and object recognition.</dcterms:abstract>
        <dc:date>2012-07-03</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1207.0580</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 13:49:16</dcterms:dateSubmitted>
        <dc:description>arXiv: 1207.0580</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_602">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/602/1207.0580.pdf"/>
        <dc:title>1207.0580.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://xxx.itp.ac.cn/pdf/1207.0580.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 13:49:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_605">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srivastava</foaf:surname>
                        <foaf:givenName>Nitish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoﬀrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krizhevsky</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutskever</foaf:surname>
                        <foaf:givenName>Ilya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>Ruslan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_604"/>
        <dc:title>Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting</dc:title>
        <dcterms:abstract>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>30</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_604">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/604/Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf"/>
        <dc:title>Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 13:53:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_607">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srivastava</foaf:surname>
                        <foaf:givenName>Nitish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoﬀrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krizhevsky</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutskever</foaf:surname>
                        <foaf:givenName>Ilya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>Ruslan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_606"/>
        <dc:title>Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting</dc:title>
        <dcterms:abstract>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>30</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_606">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/606/Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf"/>
        <dc:title>Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-06 13:53:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_610">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Mu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Yuqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smola</foaf:surname>
                        <foaf:givenName>Alexander J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_608"/>
        <dc:title>Efﬁcient Mini-batch Training for Stochastic Optimization</dc:title>
        <dcterms:abstract>Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
        <bib:pages>10</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_608">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/608/minibatch_sgd.pdf"/>
        <dc:title>minibatch_sgd.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-2956-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-2956-9</dc:identifier>
                <dc:title>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14</dc:title>
                <dc:identifier>DOI 10.1145/2623330.2623612</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, New York, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>ACM Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Mu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Yuqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smola</foaf:surname>
                        <foaf:givenName>Alexander J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_609"/>
        <dc:title>Efficient mini-batch training for stochastic optimization</dc:title>
        <dcterms:abstract>Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.</dcterms:abstract>
        <dc:date>2014</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://dl.acm.org/citation.cfm?doid=2623330.2623612</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-07 07:40:20</dcterms:dateSubmitted>
        <bib:pages>661-670</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>the 20th ACM SIGKDD international conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_609">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/609/Li 等。 - 2014 - Efficient mini-batch training for stochastic optim.pdf"/>
        <dc:title>Li 等。 - 2014 - Efficient mini-batch training for stochastic optim.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-07 07:39:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-2939-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-2939-0</dc:identifier>
                <dc:title>2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</dc:title>
                <dc:identifier>DOI 10.1109/AVSS.2017.8078520</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Lecce, Italy</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rujikietgumjorn</foaf:surname>
                        <foaf:givenName>Sitapa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watcharapinchai</foaf:surname>
                        <foaf:givenName>Nattachai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_612"/>
        <dc:title>Vehicle detection with sub-class training using R-CNN for the UA-DETRAC benchmark</dc:title>
        <dcterms:abstract>Different types of vehicles, such as buses and cars, can be quite different in shapes and details. This makes it more difﬁcult to try to learn a single feature vector that can detect all types of vehicles using a single object class. We proposed an approach to perform vehicle detection with Sub-Classes categories learning using R-CNN in order to improve the performance of vehicle detection. Instead of using a single object class, which is a vehicle in this experiment, to train on the R-CNN, we used multiple sub-classes of vehicles so that the network can better learn the features of each individual type. In the experiment, we also evaluated the result of using a transfer learning approach to use a pre-trained weights on a new dataset.</dcterms:abstract>
        <dc:date>8/2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/8078520/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-07 09:28:20</dcterms:dateSubmitted>
        <bib:pages>1-5</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_612">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/612/rujikietgumjorn2017.pdf"/>
        <dc:title>rujikietgumjorn2017.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/8737851/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>28</prism:volume>
                <dc:title>IEEE Transactions on Image Processing</dc:title>
                <dc:identifier>DOI 10.1109/TIP.2019.2922095</dc:identifier>
                <prism:number>12</prism:number>
                <dcterms:alternative>IEEE Trans. on Image Process.</dcterms:alternative>
                <dc:identifier>ISSN 1057-7149, 1941-0042</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fu</foaf:surname>
                        <foaf:givenName>Zhihang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Yaowu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yong</foaf:surname>
                        <foaf:givenName>Hongwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Rongxin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Lei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hua</foaf:surname>
                        <foaf:givenName>Xian-Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_614"/>
        <dc:title>Foreground Gating and Background Refining Network for Surveillance Object Detection</dc:title>
        <dcterms:abstract>Detecting objects in surveillance videos is an important problem due to its wide applications in trafﬁc control and public security. Existing methods tend to face performance degradation because of false positive or misalignment problems. We propose a novel framework, namely, Foreground Gating and Background Reﬁning Network (FG-BR Net), for surveillance object detection (SOD). To reduce false positives in background regions, which is a critical problem in SOD, we introduce a new module that ﬁrst subtracts the background of a video sequence and then generates high-quality region proposals. Unlike previous background subtraction methods that may wrongly remove the static foreground objects in a frame, a feedback connection from detection results to background subtraction process is proposed in our model to distill both static and moving objects in surveillance videos. Furthermore, we introduce another module, namely, the background reﬁning stage, to reﬁne the detection results with more accurate localizations. Pairwise non-local operations are adopted to cope with the misalignments between the features of original and background frames. Extensive experiments on real-world trafﬁc surveillance benchmarks demonstrate the competitive performance of the proposed FG-BR Net. In particular, FG-BR Net ranks on the top among all the methods on hard and sunny subsets of the UA-DETRAC detection dataset, without any bells and whistles.</dcterms:abstract>
        <dc:date>12/2019</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8737851/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-10 00:38:35</dcterms:dateSubmitted>
        <bib:pages>6077-6090</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_614">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/614/08737851.pdf"/>
        <dc:title>08737851.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_619">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>193</prism:volume>
                <dc:title>Computer Vision and Image Understanding</dc:title>
                <dc:identifier>DOI 10.1016/j.cviu.2020.102907</dc:identifier>
                <dcterms:alternative>Computer Vision and Image Understanding</dcterms:alternative>
                <dc:identifier>ISSN 10773142</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Longyin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>Dawei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Zhaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lei</foaf:surname>
                        <foaf:givenName>Zhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>Ming-Ching</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Honggang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lim</foaf:surname>
                        <foaf:givenName>Jongwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Ming-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lyu</foaf:surname>
                        <foaf:givenName>Siwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_620"/>
        <link:link rdf:resource="#item_616"/>
        <dc:title>UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking</dc:title>
        <dcterms:abstract>In recent years, most effective multi-object tracking (MOT) methods are based on the tracking-by-detection framework. Existing performance evaluations of MOT methods usually separate the target association step from the object detection step by using the same object detection results for comparisons. In this work, we perform a comprehensive quantitative study on the effect of object detection accuracy to the overall MOT performance. This is based on a new large-scale DETection and tRACking (DETRAC) benchmark dataset. The DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world trafﬁc scenes (over 140 thousand frames and 1.2 million labeled bounding boxes of objects) for both object detection and MOT. We evaluate complete MOT systems constructed from combinations of state-of-the-art target association methods and object detection schemes. Our analysis shows the complex effects of object detection accuracy on MOT performance. Based on these observations, we propose new evaluation tools and metrics for MOT systems that consider both object detection and target association for comprehensive analysis.</dcterms:abstract>
        <dc:date>04/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>UA-DETRAC</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S1077314220300035</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-10 01:12:11</dcterms:dateSubmitted>
        <bib:pages>102907</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_620">
        <rdf:value>&lt;p&gt;&lt;span style=&quot;font-size: 6.64495pt; font-family: SourceSansPro-Regular; color: #333333;&quot;&gt; 2015的。下面的年份出错了&lt;/span&gt;&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_616">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/616/Wen 等。 - 2020 - UA-DETRAC A new benchmark and protocol for multi-.pdf"/>
        <dc:title>Wen 等。 - 2020 - UA-DETRAC A new benchmark and protocol for multi-.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/2504-4990/1/3/44">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2504-4990"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>Haoran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kehtarnavaz</foaf:surname>
                        <foaf:givenName>Nasser</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_621"/>
        <dc:title>Semi-Supervised Faster RCNN-Based Person Detection and Load Classification for Far Field Video Surveillance</dc:title>
        <dcterms:abstract>This paper presents a semi-supervised faster region-based convolutional neural network (SF-RCNN) approach to detect persons and to classify the load carried by them in video data captured from distances several miles away via high-power lens video cameras. For detection, a set of computationally eﬃcient image processing steps are considered to identify moving areas that may contain a person. These areas are then passed onto a faster RCNN classiﬁer whose convolutional layers consist of ResNet50 transfer learning. Frame labels are obtained in a semi-supervised manner for the training of the faster RCNN classiﬁer. For load classiﬁcation, another convolutional neural network classiﬁer whose convolutional layers consist of GoogleNet transfer learning is used to distinguish a person carrying a bundle from a person carrying a long arm. Despite the challenges associated with the video dataset examined in terms of the low resolution of persons, the presence of heat haze, and the shaking of the camera, it is shown that the developed approach outperforms the faster RCNN approach.</dcterms:abstract>
        <dc:date>2019-06-27</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/2504-4990/1/3/44</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-10 02:46:40</dcterms:dateSubmitted>
        <bib:pages>756-767</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2504-4990">
        <prism:volume>1</prism:volume>
        <dc:title>Machine Learning and Knowledge Extraction</dc:title>
        <dc:identifier>DOI 10.3390/make1030044</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>MAKE</dcterms:alternative>
        <dc:identifier>ISSN 2504-4990</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_621">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/621/Wei 和 Kehtarnavaz - 2019 - Semi-Supervised Faster RCNN-Based Person Detection.pdf"/>
        <dc:title>Wei 和 Kehtarnavaz - 2019 - Semi-Supervised Faster RCNN-Based Person Detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdfs.semanticscholar.org/86b9/08eb60f62afcc929005354e3a21edc7272b4.pdf?_ga=2.153874616.1297580335.1607560066-662408763.1597203686</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-12-10 02:46:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Collection rdf:about="#collection_30">
        <dc:title>3D</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1612.00593"/>
        <dcterms:hasPart rdf:resource="#item_394"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9008567/"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9008127/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.09664"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-61284-386-5"/>
        <dcterms:hasPart rdf:resource="#item_595"/>
        <dcterms:hasPart rdf:resource="#item_597"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_10">
        <dc:title>半监督</dc:title>
        <dcterms:hasPart rdf:resource="#collection_26"/>
        <dcterms:hasPart rdf:resource="#collection_34"/>
        <dcterms:hasPart rdf:resource="#collection_28"/>
        <dcterms:hasPart rdf:resource="#collection_27"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.00068"/>
        <dcterms:hasPart rdf:resource="https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1912.02973"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.04252"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72813-293-8"/>
        <dcterms:hasPart rdf:resource="https://blog.csdn.net/weixin_42075898/article/details/104535684"/>
        <dcterms:hasPart rdf:resource="https://medium.com/@nainaakash012/self-training-with-noisy-student-f33640edbab2"/>
        <dcterms:hasPart rdf:resource="#item_264"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1912.05170"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.08199"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1406.2080"/>
        <dcterms:hasPart rdf:resource="#item_319"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8100179/"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9156477/"/>
        <dcterms:hasPart rdf:resource="#item_364"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1910.07153"/>
        <dcterms:hasPart rdf:resource="#item_495"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-030-61165-1%20978-3-030-61166-8"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.10029"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.11692"/>
        <dcterms:hasPart rdf:resource="#item_554"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.06882"/>
        <dcterms:hasPart rdf:resource="#item_560"/>
        <dcterms:hasPart rdf:resource="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7780677/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_26">
        <dc:title>超过了全监督</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.04757"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2011.10043"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3788-3"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.05278"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>超过了全监督</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.04757"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2011.10043"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3788-3"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.05278"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_34">
        <dc:title>分类</dc:title>
        <dcterms:hasPart rdf:resource="#item_578"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>分类</dc:title>
        <dcterms:hasPart rdf:resource="#item_578"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_28">
        <dc:title>complete</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.04757"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.09162"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>complete</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.04757"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.09162"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_27">
        <dc:title>weakly semi-</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3788-3"/>
        <dcterms:hasPart rdf:resource="#item_483"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>weakly semi-</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3788-3"/>
        <dcterms:hasPart rdf:resource="#item_483"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_18">
        <dc:title>带噪学习</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.00068"/>
        <dcterms:hasPart rdf:resource="https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise"/>
        <dcterms:hasPart rdf:resource="https://medium.com/@nainaakash012/self-training-with-noisy-student-f33640edbab2"/>
        <dcterms:hasPart rdf:resource="#item_264"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1912.05170"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.08199"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1406.2080"/>
        <dcterms:hasPart rdf:resource="#item_319"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8100179/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_21">
        <dc:title>多标签</dc:title>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8954216/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_7">
        <dc:title>分割</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.06667"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1512.04412"/>
        <dcterms:hasPart rdf:resource="#item_227"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1703.06870"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8950115/"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4673-6964-0"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_2">
        <dc:title>跟踪</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.01177"/>
        <dcterms:hasPart rdf:resource="#item_109"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1603.00831"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1504.01942"/>
        <dcterms:hasPart rdf:resource="#item_124"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s11263-012-0564-1"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1902.00749"/>
        <dcterms:hasPart rdf:resource="#item_141"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1703.07402"/>
        <dcterms:hasPart rdf:resource="https://linkinghub.elsevier.com/retrieve/pii/S1077314220300035"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.14619"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1506.01186"/>
        <dcterms:hasPart rdf:resource="#item_153"/>
        <dcterms:hasPart rdf:resource="#item_157"/>
        <dcterms:hasPart rdf:resource="#item_159"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.05540"/>
        <dcterms:hasPart rdf:resource="#item_176"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_1">
        <dc:title>检测</dc:title>
        <dcterms:hasPart rdf:resource="#collection_3"/>
        <dcterms:hasPart rdf:resource="#collection_9"/>
        <dcterms:hasPart rdf:resource="#collection_32"/>
        <dcterms:hasPart rdf:resource="#collection_11"/>
        <dcterms:hasPart rdf:resource="#collection_14"/>
        <dcterms:hasPart rdf:resource="#collection_13"/>
        <dcterms:hasPart rdf:resource="#item_4"/>
        <dcterms:hasPart rdf:resource="#item_11"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.08189"/>
        <dcterms:hasPart rdf:resource="https://zhuanlan.zhihu.com/p/85194783"/>
        <dcterms:hasPart rdf:resource="#item_60"/>
        <dcterms:hasPart rdf:resource="#item_68"/>
        <dcterms:hasPart rdf:resource="#item_70"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s00418-019-01831-2"/>
        <dcterms:hasPart rdf:resource="#item_75"/>
        <dcterms:hasPart rdf:resource="http://www.nature.com/articles/s41598-017-13565-z"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1603.07285"/>
        <dcterms:hasPart rdf:resource="https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.08955"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72814-803-8"/>
        <dcterms:hasPart rdf:resource="#item_88"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.09070"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.01177"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-0457-1"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1901.08043"/>
        <dcterms:hasPart rdf:resource="#item_122"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s00418-018-1759-5"/>
        <dcterms:hasPart rdf:resource="#item_128"/>
        <dcterms:hasPart rdf:resource="http://www.nature.com/articles/s41699-020-0137-z"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1903.09254"/>
        <dcterms:hasPart rdf:resource="#item_155"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.05540"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.10934"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1908.01998"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.09973"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.07850"/>
        <dcterms:hasPart rdf:resource="#item_182"/>
        <dcterms:hasPart rdf:resource="#item_183"/>
        <dcterms:hasPart rdf:resource="https://www.hindawi.com/journals/sp/2020/7845384/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2003.09119"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.01355"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1912.02424"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1506.02640"/>
        <dcterms:hasPart rdf:resource="#item_202"/>
        <dcterms:hasPart rdf:resource="#item_204"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.12099"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.11275"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2006.12671"/>
        <dcterms:hasPart rdf:resource="#item_213"/>
        <dcterms:hasPart rdf:resource="#item_219"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1708.02002"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-6889-6"/>
        <dcterms:hasPart rdf:resource="#item_233"/>
        <dcterms:hasPart rdf:resource="#item_234"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1512.02325"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1704.04503"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8099807/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_3">
        <dc:title>材料+深度学习</dc:title>
        <dcterms:hasPart rdf:resource="#item_132"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>材料+深度学习</dc:title>
        <dcterms:hasPart rdf:resource="#item_132"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_9">
        <dc:title>常用</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.05540"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.10934"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.07850"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1506.02640"/>
        <dcterms:hasPart rdf:resource="#item_223"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7485869/"/>
        <dcterms:hasPart rdf:resource="#item_298"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s11263-013-0620-5"/>
        <dcterms:hasPart rdf:resource="#item_411"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1504.08083"/>
        <dcterms:hasPart rdf:resource="#item_486"/>
        <dcterms:hasPart rdf:resource="#item_487"/>
        <dcterms:hasPart rdf:resource="#item_560"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72819-360-1"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>常用</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.05540"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.10934"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.07850"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1506.02640"/>
        <dcterms:hasPart rdf:resource="#item_223"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7485869/"/>
        <dcterms:hasPart rdf:resource="#item_298"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s11263-013-0620-5"/>
        <dcterms:hasPart rdf:resource="#item_411"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1504.08083"/>
        <dcterms:hasPart rdf:resource="#item_486"/>
        <dcterms:hasPart rdf:resource="#item_487"/>
        <dcterms:hasPart rdf:resource="#item_560"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72819-360-1"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_32">
        <dc:title>车</dc:title>
        <dcterms:hasPart rdf:resource="http://www.mdpi.com/1424-8220/16/8/1325"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-9471-8"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7726065/"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8737851/"/>
        <dcterms:hasPart rdf:resource="https://www.mdpi.com/2504-4990/1/3/44"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>车</dc:title>
        <dcterms:hasPart rdf:resource="http://www.mdpi.com/1424-8220/16/8/1325"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-9471-8"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7726065/"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8737851/"/>
        <dcterms:hasPart rdf:resource="https://www.mdpi.com/2504-4990/1/3/44"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_11">
        <dc:title>数据集</dc:title>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8099957/"/>
        <dcterms:hasPart rdf:resource="#item_286"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-319-10601-4%20978-3-319-10602-1"/>
        <dcterms:hasPart rdf:resource="#item_304"/>
        <dcterms:hasPart rdf:resource="#item_306"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1811.00982"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7298655/"/>
        <dcterms:hasPart rdf:resource="#item_398"/>
        <dcterms:hasPart rdf:resource="https://zhuanlan.zhihu.com/p/39578493"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>数据集</dc:title>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8099957/"/>
        <dcterms:hasPart rdf:resource="#item_286"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-319-10601-4%20978-3-319-10602-1"/>
        <dcterms:hasPart rdf:resource="#item_304"/>
        <dcterms:hasPart rdf:resource="#item_306"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1811.00982"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7298655/"/>
        <dcterms:hasPart rdf:resource="#item_398"/>
        <dcterms:hasPart rdf:resource="https://zhuanlan.zhihu.com/p/39578493"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_14">
        <dc:title>小物体</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1902.07296"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>小物体</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1902.07296"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_13">
        <dc:title>指标</dc:title>
        <dcterms:hasPart rdf:resource="#item_275"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>指标</dc:title>
        <dcterms:hasPart rdf:resource="#item_275"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_20">
        <dc:title>解释性</dc:title>
        <dcterms:hasPart rdf:resource="#item_407"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/6909475/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_24">
        <dc:title>经典分类网络</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1512.03385"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_35">
        <dc:title>模型压缩</dc:title>
        <dcterms:hasPart rdf:resource="#item_582"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1503.02531"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9156610/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_15">
        <dc:title>弱监督</dc:title>
        <dcterms:hasPart rdf:resource="#collection_23"/>
        <dcterms:hasPart rdf:resource="#collection_16"/>
        <dcterms:hasPart rdf:resource="#collection_17"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9156477/"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4673-8851-1"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4799-5118-5"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1802.09129"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1704.00138"/>
        <dcterms:hasPart rdf:resource="#item_355"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/8099510/"/>
        <dcterms:hasPart rdf:resource="#item_372"/>
        <dcterms:hasPart rdf:resource="#item_375"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-030-01251-9%20978-3-030-01252-6"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8489885/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.01087"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1612.00593"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7477688/"/>
        <dcterms:hasPart rdf:resource="#item_448"/>
        <dcterms:hasPart rdf:resource="#item_449"/>
        <dcterms:hasPart rdf:resource="#item_546"/>
        <dcterms:hasPart rdf:resource="#item_589"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_23">
        <dc:title>沈云航</dc:title>
        <dcterms:hasPart rdf:resource="#item_425"/>
        <dcterms:hasPart rdf:resource="#item_427"/>
        <dcterms:hasPart rdf:resource="#item_429"/>
        <dcterms:hasPart rdf:resource="#item_433"/>
        <dcterms:hasPart rdf:resource="#item_437"/>
        <dcterms:hasPart rdf:resource="#item_441"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1702.08591"/>
        <dcterms:hasPart rdf:resource="#item_455"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8554285/"/>
        <dcterms:hasPart rdf:resource="#item_589"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>沈云航</dc:title>
        <dcterms:hasPart rdf:resource="#item_425"/>
        <dcterms:hasPart rdf:resource="#item_427"/>
        <dcterms:hasPart rdf:resource="#item_429"/>
        <dcterms:hasPart rdf:resource="#item_433"/>
        <dcterms:hasPart rdf:resource="#item_437"/>
        <dcterms:hasPart rdf:resource="#item_441"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1702.08591"/>
        <dcterms:hasPart rdf:resource="#item_455"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8554285/"/>
        <dcterms:hasPart rdf:resource="#item_589"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_16">
        <dc:title>定位</dc:title>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9157038/"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7780688/"/>
        <dcterms:hasPart rdf:resource="#item_351"/>
        <dcterms:hasPart rdf:resource="#item_357"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5090-0641-0"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>定位</dc:title>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9157038/"/>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/7780688/"/>
        <dcterms:hasPart rdf:resource="#item_351"/>
        <dcterms:hasPart rdf:resource="#item_357"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5090-0641-0"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_17">
        <dc:title>分类</dc:title>
        <dcterms:hasPart rdf:resource="#item_353"/>
        <rdf:type rdf:resource="http://www.zotero.org/namespaces/export#Collection"/>
        <dc:title>分类</dc:title>
        <dcterms:hasPart rdf:resource="#item_353"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_31">
        <dc:title>视频</dc:title>
        <dcterms:hasPart rdf:resource="#item_497"/>
        <dcterms:hasPart rdf:resource="#item_499"/>
        <dcterms:hasPart rdf:resource="#item_516"/>
        <dcterms:hasPart rdf:resource="#item_517"/>
        <dcterms:hasPart rdf:resource="#item_518"/>
        <dcterms:hasPart rdf:resource="#item_519"/>
        <dcterms:hasPart rdf:resource="#item_520"/>
        <dcterms:hasPart rdf:resource="#item_521"/>
        <dcterms:hasPart rdf:resource="#item_522"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_33">
        <dc:title>数据集</dc:title>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s11263-009-0275-4"/>
        <dcterms:hasPart rdf:resource="https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html"/>
        <dcterms:hasPart rdf:resource="#item_565"/>
        <dcterms:hasPart rdf:resource="#item_569"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9009553/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1511.04136"/>
        <dcterms:hasPart rdf:resource="#item_599"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-2956-9"/>
        <dcterms:hasPart rdf:resource="#item_619"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_8">
        <dc:title>数据增强</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1906.11172"/>
        <dcterms:hasPart rdf:resource="#item_218"/>
        <dcterms:hasPart rdf:resource="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0"/>
        <dcterms:hasPart rdf:resource="#item_237"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1708.04552"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2002.08709"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1805.09501"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8954244/"/>
        <dcterms:hasPart rdf:resource="#item_300"/>
        <dcterms:hasPart rdf:resource="#item_302"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1710.09412"/>
        <dcterms:hasPart rdf:resource="#item_529"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2011.08036"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_22">
       <dc:title>数学基础</dc:title><dcterms:hasPart rdf:resource="#item_421"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_37">
        <dc:title>应用型论文（水文）</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-2939-0"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_12">
        <dc:title>优化-loss</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1506.01186"/>
        <dcterms:hasPart rdf:resource="#item_605"/>
        <dcterms:hasPart rdf:resource="#item_610"/>
    </z:Collection>
   <z:Collection rdf:about="#collection_5"><dc:title>预处理</dc:title></z:Collection>
    <z:Collection rdf:about="#collection_36">
        <dc:title>正则化</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1207.0580"/>
        <dcterms:hasPart rdf:resource="#item_607"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_29">
        <dc:title>unsupervised</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1911.05722"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_25">
        <dc:title>Weakly- and Semi-Supervised Object Detection</dc:title>
        <dcterms:hasPart rdf:resource="#item_460"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/8554285/"/>
    </z:Collection>
</rdf:RDF>
